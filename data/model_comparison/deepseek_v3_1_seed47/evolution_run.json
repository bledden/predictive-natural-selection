{
  "model": "deepseek-ai/DeepSeek-V3.1",
  "slug": "deepseek_v3_1",
  "seed": 47,
  "elapsed_seconds": 141.9419801235199,
  "generation_stats": [
    {
      "generation": 0,
      "population_size": 10,
      "avg_fitness": 0.7111269333333333,
      "best_fitness": 0.7743613333333333,
      "worst_fitness": 0.6683293333333334,
      "avg_raw_calibration": 0.8051633333333333,
      "avg_prediction_accuracy": 0.8069893333333333,
      "avg_task_accuracy": 0.6933333333333334,
      "dominant_reasoning": "debate-self",
      "dominant_memory": "relevance",
      "elapsed_seconds": 8.617677927017212
    },
    {
      "generation": 1,
      "population_size": 10,
      "avg_fitness": 0.8206117333333334,
      "best_fitness": 0.8510333333333333,
      "worst_fitness": 0.7747826666666666,
      "avg_raw_calibration": 0.9043993333333333,
      "avg_prediction_accuracy": 0.8927973333333333,
      "avg_task_accuracy": 0.88,
      "dominant_reasoning": "debate-self",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 7.635627031326294
    },
    {
      "generation": 2,
      "population_size": 10,
      "avg_fitness": 0.8463596000000001,
      "best_fitness": 0.8988466666666667,
      "worst_fitness": 0.80762,
      "avg_raw_calibration": 0.9179033333333333,
      "avg_prediction_accuracy": 0.9112659999999999,
      "avg_task_accuracy": 0.8866666666666667,
      "dominant_reasoning": "debate-self",
      "dominant_memory": "relevance",
      "elapsed_seconds": 8.119926929473877
    },
    {
      "generation": 3,
      "population_size": 10,
      "avg_fitness": 0.7599266666666666,
      "best_fitness": 0.8108133333333334,
      "worst_fitness": 0.7068733333333334,
      "avg_raw_calibration": 0.8536099999999999,
      "avg_prediction_accuracy": 0.8574333333333334,
      "avg_task_accuracy": 0.7666666666666667,
      "dominant_reasoning": "analogical",
      "dominant_memory": "relevance",
      "elapsed_seconds": 12.371535062789917
    },
    {
      "generation": 4,
      "population_size": 10,
      "avg_fitness": 0.7397552000000001,
      "best_fitness": 0.8174333333333333,
      "worst_fitness": 0.6568280000000001,
      "avg_raw_calibration": 0.819078,
      "avg_prediction_accuracy": 0.8202586666666666,
      "avg_task_accuracy": 0.7533333333333333,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 8.851157188415527
    },
    {
      "generation": 5,
      "population_size": 10,
      "avg_fitness": 0.6950424,
      "best_fitness": 0.7346133333333333,
      "worst_fitness": 0.6448226666666667,
      "avg_raw_calibration": 0.7719646666666666,
      "avg_prediction_accuracy": 0.7810706666666667,
      "avg_task_accuracy": 0.6866666666666666,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 8.331852674484253
    },
    {
      "generation": 6,
      "population_size": 10,
      "avg_fitness": 0.7167152,
      "best_fitness": 0.7560666666666667,
      "worst_fitness": 0.6840533333333333,
      "avg_raw_calibration": 0.82595,
      "avg_prediction_accuracy": 0.8305253333333333,
      "avg_task_accuracy": 0.68,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 9.124781131744385
    },
    {
      "generation": 7,
      "population_size": 10,
      "avg_fitness": 0.7192614666666667,
      "best_fitness": 0.7434786666666666,
      "worst_fitness": 0.6987333333333333,
      "avg_raw_calibration": 0.81918,
      "avg_prediction_accuracy": 0.823658,
      "avg_task_accuracy": 0.7,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 7.608312129974365
    },
    {
      "generation": 8,
      "population_size": 10,
      "avg_fitness": 0.7212382666666667,
      "best_fitness": 0.7910266666666668,
      "worst_fitness": 0.6843333333333333,
      "avg_raw_calibration": 0.81055,
      "avg_prediction_accuracy": 0.8229526666666667,
      "avg_task_accuracy": 0.68,
      "dominant_reasoning": "analogical",
      "dominant_memory": "relevance",
      "elapsed_seconds": 8.142844915390015
    },
    {
      "generation": 9,
      "population_size": 10,
      "avg_fitness": 0.76517,
      "best_fitness": 0.8154666666666668,
      "worst_fitness": 0.6947066666666667,
      "avg_raw_calibration": 0.8530646666666667,
      "avg_prediction_accuracy": 0.8586166666666666,
      "avg_task_accuracy": 0.7533333333333333,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 9.148252964019775
    },
    {
      "generation": 10,
      "population_size": 10,
      "avg_fitness": 0.7683005333333333,
      "best_fitness": 0.8482133333333334,
      "worst_fitness": 0.7008533333333333,
      "avg_raw_calibration": 0.8723659999999999,
      "avg_prediction_accuracy": 0.8762786666666667,
      "avg_task_accuracy": 0.7666666666666667,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 9.531512022018433
    },
    {
      "generation": 11,
      "population_size": 10,
      "avg_fitness": 0.6769173333333333,
      "best_fitness": 0.7449333333333333,
      "worst_fitness": 0.60458,
      "avg_raw_calibration": 0.7819633333333333,
      "avg_prediction_accuracy": 0.80864,
      "avg_task_accuracy": 0.6,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 8.435533046722412
    },
    {
      "generation": 12,
      "population_size": 10,
      "avg_fitness": 0.7488968,
      "best_fitness": 0.81972,
      "worst_fitness": 0.6931666666666666,
      "avg_raw_calibration": 0.8408126666666667,
      "avg_prediction_accuracy": 0.8394946666666666,
      "avg_task_accuracy": 0.7466666666666667,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 11.354768991470337
    },
    {
      "generation": 13,
      "population_size": 10,
      "avg_fitness": 0.7349712,
      "best_fitness": 0.8420333333333333,
      "worst_fitness": 0.6368653333333333,
      "avg_raw_calibration": 0.8255653333333334,
      "avg_prediction_accuracy": 0.8316186666666667,
      "avg_task_accuracy": 0.74,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "relevance",
      "elapsed_seconds": 7.95073390007019
    },
    {
      "generation": 14,
      "population_size": 10,
      "avg_fitness": 0.8167069333333334,
      "best_fitness": 0.8495186666666666,
      "worst_fitness": 0.7594146666666667,
      "avg_raw_calibration": 0.8985920000000001,
      "avg_prediction_accuracy": 0.8882893333333334,
      "avg_task_accuracy": 0.8666666666666667,
      "dominant_reasoning": "elimination",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 7.831406116485596
    }
  ],
  "all_genomes": [
    {
      "genome_id": "ce109155",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.65,
      "temperature": 1.15,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "6973cfc0",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.42,
      "temperature": 1.1,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "dac37cdb",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.56,
      "temperature": 0.5,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "3b0f8e05",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.72,
      "temperature": 0.91,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "e5c99c80",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.71,
      "temperature": 0.79,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "589028ca",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.48,
      "temperature": 0.41,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "66801cf1",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "relevance",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.26,
      "temperature": 0.76,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "50fe7f9b",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 0.94,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "0df380a2",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.05,
      "risk_tolerance": 0.76,
      "temperature": 0.68,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "a0f7ef53",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.37,
      "temperature": 0.63,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "199f955d",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.42,
      "temperature": 1.1,
      "generation": 1,
      "parent_ids": [
        "6973cfc0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b838ecbb",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 0.94,
      "generation": 1,
      "parent_ids": [
        "50fe7f9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "46d4359f",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 1,
      "parent_ids": [
        "ce109155",
        "50fe7f9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d38be129",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.42,
      "temperature": 1.17,
      "generation": 1,
      "parent_ids": [
        "ce109155",
        "6973cfc0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7b3b2954",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.5,
      "temperature": 0.94,
      "generation": 1,
      "parent_ids": [
        "50fe7f9b",
        "6973cfc0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a0be4a07",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.62,
      "temperature": 1.1,
      "generation": 1,
      "parent_ids": [
        "6973cfc0",
        "50fe7f9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0fc5cb7c",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.42,
      "temperature": 1.03,
      "generation": 1,
      "parent_ids": [
        "ce109155",
        "6973cfc0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8ab38be2",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.65,
      "temperature": 1.1,
      "generation": 1,
      "parent_ids": [
        "ce109155",
        "6973cfc0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8fa9489e",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "debate-self",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.68,
      "temperature": 0.83,
      "generation": 1,
      "parent_ids": [
        "ce109155",
        "50fe7f9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "82d425da",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.65,
      "temperature": 0.94,
      "generation": 1,
      "parent_ids": [
        "ce109155",
        "50fe7f9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b5c7f1df",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 0.94,
      "generation": 2,
      "parent_ids": [
        "b838ecbb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5b2ab7f6",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.65,
      "temperature": 1.1,
      "generation": 2,
      "parent_ids": [
        "8ab38be2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2f20a812",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.65,
      "temperature": 1.1,
      "generation": 2,
      "parent_ids": [
        "46d4359f",
        "8ab38be2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0cfc4a01",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": 0.0,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 2,
      "parent_ids": [
        "8ab38be2",
        "46d4359f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "89cab9d3",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 0.97,
      "generation": 2,
      "parent_ids": [
        "b838ecbb",
        "46d4359f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "822c3831",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.65,
      "temperature": 1.25,
      "generation": 2,
      "parent_ids": [
        "46d4359f",
        "8ab38be2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e7ae097d",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.39,
      "temperature": 1.07,
      "generation": 2,
      "parent_ids": [
        "8ab38be2",
        "46d4359f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a21fcd0e",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 1.1,
      "generation": 2,
      "parent_ids": [
        "8ab38be2",
        "b838ecbb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "525767ec",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.65,
      "temperature": 1.09,
      "generation": 2,
      "parent_ids": [
        "8ab38be2",
        "46d4359f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bc1c411a",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 2,
      "parent_ids": [
        "46d4359f",
        "b838ecbb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "15c00c7f",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 3,
      "parent_ids": [
        "bc1c411a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "de72e2c9",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 0.94,
      "generation": 3,
      "parent_ids": [
        "b5c7f1df"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7253b364",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.03,
      "risk_tolerance": 0.55,
      "temperature": 1.07,
      "generation": 3,
      "parent_ids": [
        "0cfc4a01",
        "bc1c411a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2c63b1d0",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 1.14,
      "generation": 3,
      "parent_ids": [
        "bc1c411a",
        "b5c7f1df"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5b35344c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.48,
      "temperature": 1.08,
      "generation": 3,
      "parent_ids": [
        "0cfc4a01",
        "bc1c411a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3cabbf83",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.48,
      "temperature": 0.96,
      "generation": 3,
      "parent_ids": [
        "bc1c411a",
        "b5c7f1df"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ef020b88",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.57,
      "temperature": 0.94,
      "generation": 3,
      "parent_ids": [
        "bc1c411a",
        "b5c7f1df"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e81ddc11",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.0,
      "risk_tolerance": 0.48,
      "temperature": 1.24,
      "generation": 3,
      "parent_ids": [
        "0cfc4a01",
        "b5c7f1df"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3801cc5c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 3,
      "parent_ids": [
        "bc1c411a",
        "b5c7f1df"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b18b3c2d",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 3,
      "parent_ids": [
        "b5c7f1df",
        "bc1c411a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "03757261",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "15c00c7f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "aa88c9d3",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "b18b3c2d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "58dbf517",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "b18b3c2d",
        "15c00c7f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b5a0d02f",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "15c00c7f",
        "b18b3c2d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8be8a4ff",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.62,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "de72e2c9",
        "b18b3c2d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0fd6b287",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "de72e2c9",
        "b18b3c2d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d8ecd7b7",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.64,
      "temperature": 0.97,
      "generation": 4,
      "parent_ids": [
        "de72e2c9",
        "b18b3c2d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8e21daf3",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.63,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "de72e2c9",
        "b18b3c2d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "87b12550",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "de72e2c9",
        "b18b3c2d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e2195cf2",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.62,
      "temperature": 1.07,
      "generation": 4,
      "parent_ids": [
        "de72e2c9",
        "15c00c7f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4b0aea2b",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 1.07,
      "generation": 5,
      "parent_ids": [
        "0fd6b287"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e7ea0fdc",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 5,
      "parent_ids": [
        "58dbf517"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9e375a08",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 5,
      "parent_ids": [
        "0fd6b287",
        "87b12550"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3d0617d1",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 5,
      "parent_ids": [
        "58dbf517",
        "87b12550"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "44d3c48d",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 1.05,
      "generation": 5,
      "parent_ids": [
        "0fd6b287",
        "87b12550"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "372cbf9d",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.4,
      "temperature": 1.25,
      "generation": 5,
      "parent_ids": [
        "0fd6b287",
        "87b12550"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0961dda3",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 4,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.62,
      "temperature": 1.07,
      "generation": 5,
      "parent_ids": [
        "87b12550",
        "0fd6b287"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a27ecc26",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.48,
      "temperature": 1.07,
      "generation": 5,
      "parent_ids": [
        "58dbf517",
        "0fd6b287"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "38a9bb7a",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.62,
      "temperature": 0.92,
      "generation": 5,
      "parent_ids": [
        "87b12550",
        "0fd6b287"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "22442304",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.7,
      "temperature": 1.23,
      "generation": 5,
      "parent_ids": [
        "87b12550",
        "0fd6b287"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "771c3460",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "3d0617d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e681d5b2",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "9e375a08"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3b929ad6",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "3d0617d1",
        "9e375a08"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "48db5994",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.56,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "4b0aea2b",
        "9e375a08"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e0afd48f",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "4b0aea2b",
        "3d0617d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d726d133",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.33,
      "temperature": 1.18,
      "generation": 6,
      "parent_ids": [
        "9e375a08",
        "3d0617d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fbfb6953",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "4b0aea2b",
        "3d0617d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6f5dd6fa",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.4,
      "temperature": 1.18,
      "generation": 6,
      "parent_ids": [
        "4b0aea2b",
        "9e375a08"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ebd51ace",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "3d0617d1",
        "4b0aea2b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b81138e1",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.62,
      "temperature": 1.07,
      "generation": 6,
      "parent_ids": [
        "4b0aea2b",
        "3d0617d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0f17ce83",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 7,
      "parent_ids": [
        "ebd51ace"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "47cf0f60",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 7,
      "parent_ids": [
        "771c3460"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "40a9704d",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.33,
      "temperature": 1.18,
      "generation": 7,
      "parent_ids": [
        "ebd51ace",
        "6f5dd6fa"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "60c049a7",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 7,
      "parent_ids": [
        "ebd51ace",
        "771c3460"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e610ded7",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.54,
      "temperature": 1.07,
      "generation": 7,
      "parent_ids": [
        "6f5dd6fa",
        "771c3460"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fa3a7518",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.38,
      "temperature": 1.07,
      "generation": 7,
      "parent_ids": [
        "771c3460",
        "6f5dd6fa"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "057de4c9",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": 0.02,
      "risk_tolerance": 0.34,
      "temperature": 1.07,
      "generation": 7,
      "parent_ids": [
        "6f5dd6fa",
        "771c3460"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2f3c3939",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.3,
      "temperature": 1.02,
      "generation": 7,
      "parent_ids": [
        "ebd51ace",
        "771c3460"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5f55cb84",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 7,
      "parent_ids": [
        "771c3460",
        "ebd51ace"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a99f6249",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": 0.0,
      "risk_tolerance": 0.4,
      "temperature": 1.18,
      "generation": 7,
      "parent_ids": [
        "771c3460",
        "6f5dd6fa"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e84f3098",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 8,
      "parent_ids": [
        "60c049a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "45336ddf",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.54,
      "temperature": 1.07,
      "generation": 8,
      "parent_ids": [
        "e610ded7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6495b94a",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.33,
      "temperature": 0.89,
      "generation": 8,
      "parent_ids": [
        "fa3a7518",
        "60c049a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "733ff896",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.33,
      "temperature": 1.07,
      "generation": 8,
      "parent_ids": [
        "e610ded7",
        "60c049a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6971b128",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.38,
      "temperature": 1.07,
      "generation": 8,
      "parent_ids": [
        "60c049a7",
        "fa3a7518"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6abf59fd",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.38,
      "temperature": 1.07,
      "generation": 8,
      "parent_ids": [
        "e610ded7",
        "fa3a7518"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "231edfd2",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "debate-self",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.33,
      "temperature": 1.02,
      "generation": 8,
      "parent_ids": [
        "60c049a7",
        "fa3a7518"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "625fe576",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.38,
      "temperature": 1.25,
      "generation": 8,
      "parent_ids": [
        "fa3a7518",
        "60c049a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "527adb50",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.37,
      "temperature": 0.97,
      "generation": 8,
      "parent_ids": [
        "60c049a7",
        "fa3a7518"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7d23162d",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.38,
      "temperature": 1.07,
      "generation": 8,
      "parent_ids": [
        "fa3a7518",
        "60c049a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6d0b56c6",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.38,
      "temperature": 1.25,
      "generation": 9,
      "parent_ids": [
        "625fe576"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "adf33a8d",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.38,
      "temperature": 1.07,
      "generation": 9,
      "parent_ids": [
        "6abf59fd"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2672ed9f",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.42,
      "temperature": 0.89,
      "generation": 9,
      "parent_ids": [
        "6abf59fd",
        "45336ddf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "30669090",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.38,
      "temperature": 1.07,
      "generation": 9,
      "parent_ids": [
        "625fe576",
        "45336ddf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "515649ec",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.51,
      "temperature": 0.97,
      "generation": 9,
      "parent_ids": [
        "6abf59fd",
        "45336ddf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4413a21c",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 9,
      "parent_ids": [
        "6abf59fd",
        "625fe576"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d5ec4330",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.45,
      "temperature": 1.07,
      "generation": 9,
      "parent_ids": [
        "45336ddf",
        "625fe576"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2980f653",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.38,
      "temperature": 1.25,
      "generation": 9,
      "parent_ids": [
        "625fe576",
        "6abf59fd"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "27cb3440",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.38,
      "temperature": 1.1,
      "generation": 9,
      "parent_ids": [
        "6abf59fd",
        "625fe576"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "73c75e94",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.29,
      "temperature": 1.15,
      "generation": 9,
      "parent_ids": [
        "6abf59fd",
        "625fe576"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e8d0dfa8",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 10,
      "parent_ids": [
        "4413a21c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5861092d",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.38,
      "temperature": 1.25,
      "generation": 10,
      "parent_ids": [
        "6d0b56c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "83c29e3a",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 10,
      "parent_ids": [
        "6d0b56c6",
        "4413a21c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "df5e1424",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.29,
      "temperature": 1.07,
      "generation": 10,
      "parent_ids": [
        "6d0b56c6",
        "adf33a8d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e78e1ff4",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 10,
      "parent_ids": [
        "6d0b56c6",
        "4413a21c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5d0e7cc6",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.07,
      "generation": 10,
      "parent_ids": [
        "6d0b56c6",
        "4413a21c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7f61836f",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 10,
      "parent_ids": [
        "4413a21c",
        "6d0b56c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fa0605f7",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 10,
      "parent_ids": [
        "4413a21c",
        "6d0b56c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "86c3f657",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 10,
      "parent_ids": [
        "6d0b56c6",
        "adf33a8d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "120b3c2f",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.38,
      "temperature": 1.07,
      "generation": 10,
      "parent_ids": [
        "adf33a8d",
        "6d0b56c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "86f8a917",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.07,
      "generation": 11,
      "parent_ids": [
        "5d0e7cc6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4d4cbfac",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 11,
      "parent_ids": [
        "86c3f657"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7ba95019",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 11,
      "parent_ids": [
        "86c3f657",
        "5d0e7cc6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eb62a896",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 11,
      "parent_ids": [
        "5d0e7cc6",
        "e8d0dfa8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fd58fab4",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.24,
      "generation": 11,
      "parent_ids": [
        "e8d0dfa8",
        "86c3f657"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5f0876d0",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 11,
      "parent_ids": [
        "e8d0dfa8",
        "5d0e7cc6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e39a439a",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.64,
      "temperature": 1.07,
      "generation": 11,
      "parent_ids": [
        "5d0e7cc6",
        "86c3f657"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "74b8489e",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 11,
      "parent_ids": [
        "5d0e7cc6",
        "86c3f657"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "24b80f04",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.4,
      "temperature": 0.92,
      "generation": 11,
      "parent_ids": [
        "5d0e7cc6",
        "86c3f657"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eb5f0fe3",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.42,
      "temperature": 1.07,
      "generation": 11,
      "parent_ids": [
        "86c3f657",
        "5d0e7cc6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "810df0f2",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.4,
      "temperature": 1.07,
      "generation": 12,
      "parent_ids": [
        "7ba95019"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1760c6be",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 12,
      "parent_ids": [
        "eb62a896"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "df271dc4",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.4,
      "temperature": 1.19,
      "generation": 12,
      "parent_ids": [
        "7ba95019",
        "eb62a896"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4f3ea2a8",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.58,
      "temperature": 1.01,
      "generation": 12,
      "parent_ids": [
        "7ba95019",
        "eb62a896"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dfcfb768",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.64,
      "temperature": 1.07,
      "generation": 12,
      "parent_ids": [
        "eb62a896",
        "e39a439a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a9bb2279",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.4,
      "temperature": 1.25,
      "generation": 12,
      "parent_ids": [
        "eb62a896",
        "7ba95019"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1e99e195",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 12,
      "parent_ids": [
        "e39a439a",
        "7ba95019"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a8f09611",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.64,
      "temperature": 1.07,
      "generation": 12,
      "parent_ids": [
        "7ba95019",
        "e39a439a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "61fa5009",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 12,
      "parent_ids": [
        "e39a439a",
        "eb62a896"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "26b1b86a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.4,
      "temperature": 1.25,
      "generation": 12,
      "parent_ids": [
        "e39a439a",
        "7ba95019"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "04102026",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 13,
      "parent_ids": [
        "1e99e195"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e1815924",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 13,
      "parent_ids": [
        "61fa5009"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ef57d3e0",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.48,
      "temperature": 1.1,
      "generation": 13,
      "parent_ids": [
        "1e99e195",
        "4f3ea2a8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "03d1df7e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.39,
      "temperature": 1.07,
      "generation": 13,
      "parent_ids": [
        "61fa5009",
        "1e99e195"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a492d963",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 13,
      "parent_ids": [
        "61fa5009",
        "1e99e195"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "81aa614b",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.69,
      "temperature": 1.07,
      "generation": 13,
      "parent_ids": [
        "1e99e195",
        "4f3ea2a8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5897ece8",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 9,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.51,
      "temperature": 1.07,
      "generation": 13,
      "parent_ids": [
        "1e99e195",
        "61fa5009"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7399b167",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 9,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.58,
      "temperature": 1.01,
      "generation": 13,
      "parent_ids": [
        "61fa5009",
        "4f3ea2a8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b6506fb3",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.58,
      "temperature": 1.09,
      "generation": 13,
      "parent_ids": [
        "4f3ea2a8",
        "61fa5009"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1be6c7a9",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 13,
      "parent_ids": [
        "61fa5009",
        "1e99e195"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "060fe64a",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 14,
      "parent_ids": [
        "1be6c7a9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3b92af7a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 14,
      "parent_ids": [
        "a492d963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f0a63272",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.47,
      "temperature": 1.25,
      "generation": 14,
      "parent_ids": [
        "04102026",
        "a492d963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "04646a8e",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 14,
      "parent_ids": [
        "1be6c7a9",
        "04102026"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dc488370",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 14,
      "parent_ids": [
        "04102026",
        "a492d963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8e830e0d",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.07,
      "generation": 14,
      "parent_ids": [
        "a492d963",
        "04102026"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b075c43b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.25,
      "generation": 14,
      "parent_ids": [
        "a492d963",
        "04102026"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e32ab164",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.36,
      "temperature": 1.4,
      "generation": 14,
      "parent_ids": [
        "a492d963",
        "1be6c7a9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2b6cf61f",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.51,
      "temperature": 1.07,
      "generation": 14,
      "parent_ids": [
        "04102026",
        "a492d963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0d2e07a7",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.36,
      "temperature": 1.07,
      "generation": 14,
      "parent_ids": [
        "1be6c7a9",
        "04102026"
      ],
      "fitness_history": []
    }
  ],
  "all_results": [
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "t04",
      "predicted_confidence": 0.75,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9216,
      "fitness": 0.89296
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "r02",
      "predicted_confidence": 0.98,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "t05",
      "predicted_confidence": 0.8,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 0,
      "genome_id": "ce109155",
      "task_id": "e04",
      "predicted_confidence": 0.65,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8333600000000001
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "t14",
      "predicted_confidence": 0.75,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "r11",
      "predicted_confidence": 0.05,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.9975,
      "prediction_accuracy": 1.0,
      "fitness": 0.6
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "e10",
      "predicted_confidence": 0.45,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.6602600000000001
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.6602600000000001
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "r07",
      "predicted_confidence": 0.85,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.89296
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.89296
    },
    {
      "generation": 0,
      "genome_id": "6973cfc0",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.80906
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "e09",
      "predicted_confidence": 0.2,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 0,
      "genome_id": "dac37cdb",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9991,
      "fitness": 0.91946
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.87466
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "r11",
      "predicted_confidence": 0.75,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "e10",
      "predicted_confidence": 0.45,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.8417600000000001
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "r07",
      "predicted_confidence": 0.3,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.52666
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 0,
      "genome_id": "3b0f8e05",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9216,
      "fitness": 0.55296
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9676,
      "fitness": 0.58056
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8709600000000001
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.70936
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 0,
      "genome_id": "e5c99c80",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 0,
      "genome_id": "589028ca",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.89296
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.7025600000000001
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.7018599999999999
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 0,
      "genome_id": "66801cf1",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.9065
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 0,
      "genome_id": "50fe7f9b",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.8585
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9185000000000001
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 0,
      "genome_id": "0df380a2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.52586
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 0,
      "genome_id": "a0f7ef53",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "275",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.91296
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.6602600000000001
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.80986
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "r07",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8290599999999999
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 1,
      "genome_id": "199f955d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9185
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 1,
      "genome_id": "b838ecbb",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "t15",
      "predicted_confidence": 0.7,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 1,
      "genome_id": "46d4359f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "225",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.75,
      "fitness": 0.44999999999999996
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.8859999999999999
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "e04",
      "predicted_confidence": 0.65,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.75,
      "fitness": 0.77
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "e07",
      "predicted_confidence": 0.75,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 1,
      "genome_id": "d38be129",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.86826
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.89296
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.91296
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.89296
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.5666599999999999
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.80986
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.79466
    },
    {
      "generation": 1,
      "genome_id": "7b3b2954",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "e06",
      "predicted_confidence": 0.15,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.59976
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.91296
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "e04",
      "predicted_confidence": 0.5,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.68186
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "e10",
      "predicted_confidence": 0.45,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.80986
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 1,
      "genome_id": "a0be4a07",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "e06",
      "predicted_confidence": 0.8,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "e03",
      "predicted_confidence": 0.6,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.79304
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "r02",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.87354
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "t14",
      "predicted_confidence": 0.75,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.8823399999999999
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9023399999999999
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5904000000000001,
      "fitness": 0.35424000000000005
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.87914
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8178399999999999
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 1,
      "genome_id": "0fc5cb7c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.71864
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "t15",
      "predicted_confidence": 0.6,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "t04",
      "predicted_confidence": 0.6,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.83914
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.97994
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "12000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "e07",
      "predicted_confidence": 0.75,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8199400000000001
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8578399999999999
    },
    {
      "generation": 1,
      "genome_id": "8ab38be2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.87914
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8178399999999999
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 1,
      "genome_id": "8fa9489e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "450",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.5765600000000001
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.81976
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8570599999999999
    },
    {
      "generation": 1,
      "genome_id": "82d425da",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8465
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 2,
      "genome_id": "b5c7f1df",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.5863399999999999
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "r07",
      "predicted_confidence": 0.25,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.4224,
      "fitness": 0.59344
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "e12",
      "predicted_confidence": 0.75,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.97994
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 2,
      "genome_id": "5b2ab7f6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4959,
      "fitness": 0.67754
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "r07",
      "predicted_confidence": 0.97,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "t15",
      "predicted_confidence": 0.4,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6279,
      "fitness": 0.7367400000000001
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "e12",
      "predicted_confidence": 0.55,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "r13",
      "predicted_confidence": 0.8,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 2,
      "genome_id": "2f20a812",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "e12",
      "predicted_confidence": 0.55,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "r05",
      "predicted_confidence": 0.85,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 2,
      "genome_id": "0cfc4a01",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "e12",
      "predicted_confidence": 0.55,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.75,
      "fitness": 0.44999999999999996
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "r14",
      "predicted_confidence": 0.8,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 2,
      "genome_id": "89cab9d3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.86234
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "e07",
      "predicted_confidence": 0.55,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.7975000000000001,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "r07",
      "predicted_confidence": 0.02,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.35194000000000003
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9159,
      "fitness": 0.54954
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "r14",
      "predicted_confidence": 0.7,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9023399999999999
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 2,
      "genome_id": "822c3831",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8465
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.51,
      "fitness": 0.6859999999999999
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 2,
      "genome_id": "e7ae097d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.9065
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 2,
      "genome_id": "a21fcd0e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.83786
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "175",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9216,
      "fitness": 0.55296
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "t05",
      "predicted_confidence": 0.75,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.85856
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 2,
      "genome_id": "525767ec",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.84224
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.9022399999999999
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.75184
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.87914
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.87914
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "bc1c411a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "r11",
      "predicted_confidence": 0.75,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9424,
      "fitness": 0.5654399999999999
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "t13",
      "predicted_confidence": 1.0,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8578399999999999
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.7918399999999999
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "15c00c7f",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9185
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 3,
      "genome_id": "de72e2c9",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "r11",
      "predicted_confidence": 0.2,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.85976
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "2.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.91976
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.91976
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "t04",
      "predicted_confidence": 0.98,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "7253b364",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "275",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "8 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "r03",
      "predicted_confidence": 0.8,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8425
    },
    {
      "generation": 3,
      "genome_id": "2c63b1d0",
      "task_id": "t11",
      "predicted_confidence": 0.75,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9059999999999999
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9446400000000001
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "e06",
      "predicted_confidence": 0.55,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6518999999999999,
      "fitness": 0.39113999999999993
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "e03",
      "predicted_confidence": 0.6,
      "predicted_answer": "230",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.79304
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "e07",
      "predicted_confidence": 0.25,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.5863399999999999
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "5b35344c",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "450",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5904000000000001,
      "fitness": 0.35424000000000005
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.87914
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "3cabbf83",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "3.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.51,
      "fitness": 0.6859999999999999
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 3,
      "genome_id": "ef020b88",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "r11",
      "predicted_confidence": 0.05,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.9975,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "e06",
      "predicted_confidence": 0.15,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "e04",
      "predicted_confidence": 0.5,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.75,
      "fitness": 0.77
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "t04",
      "predicted_confidence": 0.75,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.51,
      "fitness": 0.6859999999999999
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "e81ddc11",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "China officially uses one time zone, which is China Standard Time (UTC+8)",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3599999999999999,
      "fitness": 0.21599999999999991
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "r11",
      "predicted_confidence": 0.7,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5904000000000001,
      "fitness": 0.35424000000000005
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9424,
      "fitness": 0.5654399999999999
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "solidly in the 4 million range",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.7918399999999999
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "3801cc5c",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "China officially uses one time zone, which is China Standard Time (UTC+8)",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.9065
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 3,
      "genome_id": "b18b3c2d",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.84224
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9424,
      "fitness": 0.5654399999999999
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "e07",
      "predicted_confidence": 0.55,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "r07",
      "predicted_confidence": 0.2,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.49624000000000007
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9446400000000001
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "03757261",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "r07",
      "predicted_confidence": 0.2,
      "predicted_answer": "Yes",
      "ground_truth": "No",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 4,
      "genome_id": "aa88c9d3",
      "task_id": "t04",
      "predicted_confidence": 0.98,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "r03",
      "predicted_confidence": 0.8,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8425
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "His son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "One (1)",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "58dbf517",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "r03",
      "predicted_confidence": 0.9,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8646400000000001
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "e12",
      "predicted_confidence": 0.25,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9639,
      "fitness": 0.57834
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9023399999999999
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.87914
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "e06",
      "predicted_confidence": 0.25,
      "predicted_answer": "500",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9639,
      "fitness": 0.57834
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "e03",
      "predicted_confidence": 0.5,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.73184
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "b5a0d02f",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8578399999999999
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "r12",
      "predicted_confidence": 0.85,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "t03",
      "predicted_confidence": 0.98,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "e12",
      "predicted_confidence": 0.55,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "t15",
      "predicted_confidence": 0.65,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.8822400000000001
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.7839400000000001
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "t11",
      "predicted_confidence": 1.0,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 4,
      "genome_id": "8be8a4ff",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "r12",
      "predicted_confidence": 0.85,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "r11",
      "predicted_confidence": 0.7,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9059999999999999
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 4,
      "genome_id": "0fd6b287",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9185
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.51,
      "fitness": 0.6859999999999999
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 4,
      "genome_id": "d8ecd7b7",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "Human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 4,
      "genome_id": "8e21daf3",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "t13",
      "predicted_confidence": 1.0,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.85946
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "e04",
      "predicted_confidence": 0.6,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.80906
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "r11",
      "predicted_confidence": 0.2,
      "predicted_answer": "second",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6031,
      "fitness": 0.74186
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "e07",
      "predicted_confidence": 0.55,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.7975000000000001,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.8417600000000001
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.4210600000000001
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "e03",
      "predicted_confidence": 0.6,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.80906
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 4,
      "genome_id": "87b12550",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "t15",
      "predicted_confidence": 0.6,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.83304
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.84224
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 4,
      "genome_id": "e2195cf2",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "r11",
      "predicted_confidence": 0.7,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9185
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "4b0aea2b",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.91,
      "fitness": 0.546
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "China officially uses one time zone, which is China Standard Time (UTC+8)",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 5,
      "genome_id": "e7ea0fdc",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "e12",
      "predicted_confidence": 0.35,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.53856
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "e09",
      "predicted_confidence": 0.3,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 5,
      "genome_id": "9e375a08",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "e09",
      "predicted_confidence": 0.35,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.53856
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.6602600000000001
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "t04",
      "predicted_confidence": 0.3,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.62026
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 5,
      "genome_id": "3d0617d1",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8911,
      "fitness": 0.91466
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "e12",
      "predicted_confidence": 0.35,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.51,
      "fitness": 0.6859999999999999
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "e09",
      "predicted_confidence": 0.55,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.75,
      "fitness": 0.44999999999999996
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "e03",
      "predicted_confidence": 0.15,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.43400000000000005
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "44d3c48d",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.91,
      "fitness": 0.546
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.82544
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "t11",
      "predicted_confidence": 1.0,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "5 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9559,
      "fitness": 0.5735399999999999
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "e03",
      "predicted_confidence": 0.2,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.44474
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man is looking at a picture of his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 5,
      "genome_id": "372cbf9d",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "One (UTC+8)",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "1.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "t04",
      "predicted_confidence": 0.2,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.5065000000000001
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 5,
      "genome_id": "0961dda3",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9185
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "e03",
      "predicted_confidence": 0.3,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.5825
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "a27ecc26",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "t13",
      "predicted_confidence": 1.0,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.85946
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6031,
      "fitness": 0.74186
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 5,
      "genome_id": "38a9bb7a",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.85136
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "e07",
      "predicted_confidence": 0.75,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "t05",
      "predicted_confidence": 0.75,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.97976
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 5,
      "genome_id": "22442304",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "t12",
      "predicted_confidence": 0.6,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "r11",
      "predicted_confidence": 0.2,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "t14",
      "predicted_confidence": 0.7,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8911,
      "fitness": 0.87466
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "t13",
      "predicted_confidence": 0.75,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9216,
      "fitness": 0.81296
    },
    {
      "generation": 6,
      "genome_id": "771c3460",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "e12",
      "predicted_confidence": 0.35,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.53856
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 6,
      "genome_id": "e681d5b2",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "r05",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8911,
      "fitness": 0.87466
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "t14",
      "predicted_confidence": 0.6,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8290599999999999
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 6,
      "genome_id": "3b929ad6",
      "task_id": "r14",
      "predicted_confidence": 0.8,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "e12",
      "predicted_confidence": 0.75,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "e03",
      "predicted_confidence": 0.4,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6031,
      "fitness": 0.68186
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 6,
      "genome_id": "48db5994",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "t04",
      "predicted_confidence": 0.4,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.70936
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.46176
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "e03",
      "predicted_confidence": 0.25,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.4071,
      "fitness": 0.5642600000000001
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.85136
    },
    {
      "generation": 6,
      "genome_id": "e0afd48f",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "r11",
      "predicted_confidence": 0.65,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "t14",
      "predicted_confidence": 0.7,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.88954
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.9295399999999999
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.86954
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8590399999999999
    },
    {
      "generation": 6,
      "genome_id": "d726d133",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8844,
      "fitness": 0.8706400000000001
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "e06",
      "predicted_confidence": 0.75,
      "predicted_answer": "225",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "t14",
      "predicted_confidence": 0.7,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8844,
      "fitness": 0.8706400000000001
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.5366400000000001
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8844,
      "fitness": 0.8506400000000001
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "e09",
      "predicted_confidence": 0.35,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9039,
      "fitness": 0.54234
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 6,
      "genome_id": "fbfb6953",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "r11",
      "predicted_confidence": 0.2,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "450",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "e03",
      "predicted_confidence": 0.3,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.60026
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 6,
      "genome_id": "6f5dd6fa",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "r02",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8625
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "t12",
      "predicted_confidence": 0.4,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "r11",
      "predicted_confidence": 0.6,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "e09",
      "predicted_confidence": 0.15,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.99,
      "fitness": 0.594
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 6,
      "genome_id": "ebd51ace",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "r05",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "r10",
      "predicted_confidence": 0.9,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "e10",
      "predicted_confidence": 0.35,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9039,
      "fitness": 0.54234
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8844,
      "fitness": 0.91064
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 6,
      "genome_id": "b81138e1",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "t15",
      "predicted_confidence": 0.3,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6225
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "e04",
      "predicted_confidence": 0.65,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.8240000000000001
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "r13",
      "predicted_confidence": 0.85,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 7,
      "genome_id": "0f17ce83",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "e09",
      "predicted_confidence": 0.25,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8194600000000001
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "r07",
      "predicted_confidence": 0.15,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.47536
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "t12",
      "predicted_confidence": 0.4,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6031,
      "fitness": 0.7018599999999999
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 7,
      "genome_id": "47cf0f60",
      "task_id": "e03",
      "predicted_confidence": 0.65,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8333600000000001
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "e09",
      "predicted_confidence": 0.45,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "225",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "t12",
      "predicted_confidence": 0.45,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "t04",
      "predicted_confidence": 0.75,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.8859999999999999
    },
    {
      "generation": 7,
      "genome_id": "40a9704d",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.59136
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.8417600000000001
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "r13",
      "predicted_confidence": 0.85,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 7,
      "genome_id": "60c049a7",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "t15",
      "predicted_confidence": 0.65,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8194600000000001
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "t12",
      "predicted_confidence": 0.75,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.91946
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "t04",
      "predicted_confidence": 0.5,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.77146
    },
    {
      "generation": 7,
      "genome_id": "e610ded7",
      "task_id": "e03",
      "predicted_confidence": 0.5,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.75146
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "e09",
      "predicted_confidence": 0.2,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.59136
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "r07",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 7,
      "genome_id": "fa3a7518",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "t15",
      "predicted_confidence": 0.1,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.49536
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "r11",
      "predicted_confidence": 0.5,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.80176
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "t12",
      "predicted_confidence": 0.5,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "e04",
      "predicted_confidence": 0.6,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8333600000000001
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 7,
      "genome_id": "057de4c9",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "t15",
      "predicted_confidence": 0.2,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.54666
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8194600000000001
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "e04",
      "predicted_confidence": 0.6,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.80906
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 7,
      "genome_id": "2f3c3939",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "t15",
      "predicted_confidence": 0.25,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.58554
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.81904
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "r07",
      "predicted_confidence": 0.6,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.82384
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9790399999999999
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 7,
      "genome_id": "5f55cb84",
      "task_id": "e03",
      "predicted_confidence": 0.3,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4524,
      "fitness": 0.5914400000000001
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "t15",
      "predicted_confidence": 0.2,
      "predicted_answer": "Finland",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.96,
      "fitness": 0.576
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "a99f6249",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8333600000000001
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "t15",
      "predicted_confidence": 0.5,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.75816
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "e84f3098",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.77146
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "r05",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "e09",
      "predicted_confidence": 0.35,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.53856
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "t15",
      "predicted_confidence": 0.6,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8490599999999999
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "r11",
      "predicted_confidence": 0.7,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 8,
      "genome_id": "45336ddf",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "r07",
      "predicted_confidence": 0.2,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.47536
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "e03",
      "predicted_confidence": 0.5,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.71816
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "e09",
      "predicted_confidence": 0.45,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "r11",
      "predicted_confidence": 0.05,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.9975,
      "prediction_accuracy": 1.0,
      "fitness": 0.6
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "6495b94a",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.4210600000000001
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "r05",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9216,
      "fitness": 0.91296
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "r11",
      "predicted_confidence": 0.05,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.9975,
      "prediction_accuracy": 0.9996,
      "fitness": 0.59976
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 8,
      "genome_id": "733ff896",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "China officially uses one time zone, which is China Standard Time (UTC+8)",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "t12",
      "predicted_confidence": 0.6,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 8,
      "genome_id": "6971b128",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "r07",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8479,
      "fitness": 0.84874
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8479,
      "fitness": 0.82874
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.5423399999999999
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "t12",
      "predicted_confidence": 0.75,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8706400000000001
    },
    {
      "generation": 8,
      "genome_id": "6abf59fd",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.75,
      "fitness": 0.44999999999999996
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "r07",
      "predicted_confidence": 0.15,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.3985
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.84,
      "fitness": 0.8240000000000001
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "China officially uses one time zone, which is China Standard Time (CST), UTC+8",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "t15",
      "predicted_confidence": 0.7,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.84,
      "fitness": 0.8640000000000001
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 8,
      "genome_id": "231edfd2",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "e06",
      "predicted_confidence": 0.75,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.91466
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "e03",
      "predicted_confidence": 0.65,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8090600000000001
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "e10",
      "predicted_confidence": 0.55,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7790999999999999,
      "fitness": 0.46745999999999993
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "e09",
      "predicted_confidence": 0.55,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7790999999999999,
      "fitness": 0.46745999999999993
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1 time zone",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "t15",
      "predicted_confidence": 0.7,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "625fe576",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "Yes",
      "ground_truth": "No",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "t11",
      "predicted_confidence": 0.75,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 8,
      "genome_id": "527adb50",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8333600000000001
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "t12",
      "predicted_confidence": 0.6,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 8,
      "genome_id": "7d23162d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "2.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.8417600000000001
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 9,
      "genome_id": "6d0b56c6",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.5423399999999999
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "8",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9559,
      "fitness": 0.5735399999999999
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "e12",
      "predicted_confidence": 0.2,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 9,
      "genome_id": "adf33a8d",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "e09",
      "predicted_confidence": 0.55,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7790999999999999,
      "fitness": 0.46745999999999993
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "e07",
      "predicted_confidence": 0.25,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.5666599999999999
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 9,
      "genome_id": "2672ed9f",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 9,
      "genome_id": "30669090",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.6602600000000001
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 9,
      "genome_id": "515649ec",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "3.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 9,
      "genome_id": "4413a21c",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.61496
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "r12",
      "predicted_confidence": 0.8,
      "predicted_answer": "His son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "One time zone (China Standard Time, UTC+8)",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 9,
      "genome_id": "d5ec4330",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,500 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 9,
      "genome_id": "2980f653",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 9,
      "genome_id": "27cb3440",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "e09",
      "predicted_confidence": 0.55,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.50474
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "r12",
      "predicted_confidence": 0.85,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 9,
      "genome_id": "73c75e94",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.81514
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "r10",
      "predicted_confidence": 0.9,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "e12",
      "predicted_confidence": 0.45,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 10,
      "genome_id": "e8d0dfa8",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "100",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "e04",
      "predicted_confidence": 0.4,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.64256
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 10,
      "genome_id": "5861092d",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.77816
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "That man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "83c29e3a",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "450",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "e10",
      "predicted_confidence": 0.35,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "t13",
      "predicted_confidence": 0.75,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.79466
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "df5e1424",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "r13",
      "predicted_confidence": 0.8,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.93296
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "e04",
      "predicted_confidence": 0.5,
      "predicted_answer": "10,500 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.71816
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 10,
      "genome_id": "e78e1ff4",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "e06",
      "predicted_confidence": 0.75,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.91466
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.82176
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "e12",
      "predicted_confidence": 0.75,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.91466
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "5d0e7cc6",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.8417600000000001
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "r14",
      "predicted_confidence": 0.85,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.6825600000000001
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "e12",
      "predicted_confidence": 0.75,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 10,
      "genome_id": "7f61836f",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.61496
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "e10",
      "predicted_confidence": 0.25,
      "predicted_answer": "8500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "e12",
      "predicted_confidence": 0.75,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "t04",
      "predicted_confidence": 0.35,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.62026
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 10,
      "genome_id": "fa0605f7",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.7711399999999999
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.81514
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man is looking at a picture of his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 10,
      "genome_id": "86c3f657",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6031,
      "fitness": 0.74186
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.61496
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.59136
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "21",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "120b3c2f",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "t12",
      "predicted_confidence": 0.75,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "t11",
      "predicted_confidence": 0.7,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 11,
      "genome_id": "86f8a917",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11,000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "the man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 11,
      "genome_id": "4d4cbfac",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man is looking at a picture of his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "t12",
      "predicted_confidence": 0.4,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.81296
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "e12",
      "predicted_confidence": 0.8,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.93296
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 11,
      "genome_id": "7ba95019",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "12,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "His son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "r10",
      "predicted_confidence": 0.9,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "2.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 11,
      "genome_id": "eb62a896",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "175",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "r02",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.86826
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1.6",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 11,
      "genome_id": "fd58fab4",
      "task_id": "t15",
      "predicted_confidence": 0.7,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9500",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8844,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "e04",
      "predicted_confidence": 0.3,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4524,
      "fitness": 0.5914400000000001
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "His son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.33864000000000005
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "t13",
      "predicted_confidence": 1.0,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8590399999999999
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "r02",
      "predicted_confidence": 0.7,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8844,
      "fitness": 0.83064
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "t11",
      "predicted_confidence": 0.75,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.90954
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 11,
      "genome_id": "5f0876d0",
      "task_id": "t15",
      "predicted_confidence": 0.4,
      "predicted_answer": "Finland",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "e10",
      "predicted_confidence": 0.15,
      "predicted_answer": "8500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9991,
      "fitness": 0.59946
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "e04",
      "predicted_confidence": 0.6,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7296,
      "fitness": 0.75776
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "His son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "t12",
      "predicted_confidence": 0.75,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.87416
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "t11",
      "predicted_confidence": 0.6,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7296,
      "fitness": 0.79776
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "0.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 11,
      "genome_id": "e39a439a",
      "task_id": "t15",
      "predicted_confidence": 0.65,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.7791,
      "fitness": 0.8274600000000001
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.82544
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "One",
      "ground_truth": "1",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.88954
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 11,
      "genome_id": "74b8489e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "t13",
      "predicted_confidence": 1.0,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 11,
      "genome_id": "24b80f04",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.5970599999999999
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "e04",
      "predicted_confidence": 0.4,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.60026
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "That man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9216,
      "fitness": 0.89296
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "r11",
      "predicted_confidence": 0.75,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "r02",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8346600000000001
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "e12",
      "predicted_confidence": 0.15,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.59976
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 11,
      "genome_id": "eb5f0fe3",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "t12",
      "predicted_confidence": 0.5,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "e06",
      "predicted_confidence": 0.25,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "e04",
      "predicted_confidence": 0.15,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.40105999999999997
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "t05",
      "predicted_confidence": 0.98,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.81296
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "e07",
      "predicted_confidence": 0.55,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7790999999999999,
      "fitness": 0.46745999999999993
    },
    {
      "generation": 12,
      "genome_id": "810df0f2",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 12,
      "genome_id": "1760c6be",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "e10",
      "predicted_confidence": 0.45,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.8660000000000001
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.8860000000000001
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "r11",
      "predicted_confidence": 0.75,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,500",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.84,
      "fitness": 0.8240000000000001
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "t15",
      "predicted_confidence": 0.65,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.8385
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.75,
      "fitness": 0.83
    },
    {
      "generation": 12,
      "genome_id": "df271dc4",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "e10",
      "predicted_confidence": 0.35,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "r07",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "e06",
      "predicted_confidence": 0.55,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7790999999999999,
      "fitness": 0.46745999999999993
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "e04",
      "predicted_confidence": 0.65,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8090600000000001
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "t15",
      "predicted_confidence": 0.4,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.6825600000000001
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 12,
      "genome_id": "4f3ea2a8",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "e03",
      "predicted_confidence": 0.4,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.64256
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "r07",
      "predicted_confidence": 0.4,
      "predicted_answer": "Yes",
      "ground_truth": "No",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "t05",
      "predicted_confidence": 0.7,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8333600000000001
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.81296
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.77816
    },
    {
      "generation": 12,
      "genome_id": "dfcfb768",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "e03",
      "predicted_confidence": 0.5,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.72506
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "r14",
      "predicted_confidence": 0.8,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "e06",
      "predicted_confidence": 0.25,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.57656
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "e04",
      "predicted_confidence": 0.4,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.6506600000000001
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 12,
      "genome_id": "a9bb2279",
      "task_id": "r05",
      "predicted_confidence": 0.85,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.84906
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "r07",
      "predicted_confidence": 0.4,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.66256
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 12,
      "genome_id": "1e99e195",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "e03",
      "predicted_confidence": 0.3,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.55496
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "175",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "t05",
      "predicted_confidence": 0.75,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 12,
      "genome_id": "a8f09611",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "t05",
      "predicted_confidence": 0.75,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "t15",
      "predicted_confidence": 0.6,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.82176
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "e07",
      "predicted_confidence": 0.55,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.7975000000000001,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 12,
      "genome_id": "61fa5009",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "e10",
      "predicted_confidence": 0.25,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9831,
      "fitness": 0.5898599999999999
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.8585600000000001
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.87856
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.46176
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "t04",
      "predicted_confidence": 0.6,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7296,
      "fitness": 0.77776
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "t15",
      "predicted_confidence": 0.4,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.64896
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9676,
      "fitness": 0.58056
    },
    {
      "generation": 12,
      "genome_id": "26b1b86a",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "e06",
      "predicted_confidence": 0.25,
      "predicted_answer": "100",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "r05",
      "predicted_confidence": 0.85,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man is looking at a photograph of his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 13,
      "genome_id": "04102026",
      "task_id": "t15",
      "predicted_confidence": 0.4,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.6825600000000001
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.61496
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.3637600000000001
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "e03",
      "predicted_confidence": 0.75,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 13,
      "genome_id": "e1815924",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "r07",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 13,
      "genome_id": "ef57d3e0",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.81296
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "e03",
      "predicted_confidence": 0.6,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.78176
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 13,
      "genome_id": "03d1df7e",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "e12",
      "predicted_confidence": 0.8,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.93296
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "t13",
      "predicted_confidence": 0.7,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.77336
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "r10",
      "predicted_confidence": 0.8,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.85296
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "His son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.8729600000000001
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "1.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 13,
      "genome_id": "a492d963",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "e06",
      "predicted_confidence": 0.25,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 13,
      "genome_id": "81aa614b",
      "task_id": "t15",
      "predicted_confidence": 0.75,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9559,
      "fitness": 0.5735399999999999
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9324,
      "fitness": 0.5594399999999999
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "r05",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "t14",
      "predicted_confidence": 0.7,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8479,
      "fitness": 0.84874
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 13,
      "genome_id": "5897ece8",
      "task_id": "t15",
      "predicted_confidence": 0.4,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.67434
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "21",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "r05",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8333600000000001
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 13,
      "genome_id": "7399b167",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "20",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "t13",
      "predicted_confidence": 1.0,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "r05",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man is looking at a picture of his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 13,
      "genome_id": "b6506fb3",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "e12",
      "predicted_confidence": 0.3,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "e09",
      "predicted_confidence": 0.25,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.96,
      "fitness": 0.576
    },
    {
      "generation": 13,
      "genome_id": "1be6c7a9",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "t01",
      "predicted_confidence": 0.95,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8140000000000001
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "r13",
      "predicted_confidence": 0.8,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "e03",
      "predicted_confidence": 0.3,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.5825
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "e10",
      "predicted_confidence": 0.35,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.51,
      "fitness": 0.666
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 14,
      "genome_id": "060fe64a",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "t03",
      "predicted_confidence": 0.98,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "e03",
      "predicted_confidence": 0.6,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.78176
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.7025600000000001
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.89296
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "e10",
      "predicted_confidence": 0.25,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9711,
      "fitness": 0.58266
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "t11",
      "predicted_confidence": 1.0,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 14,
      "genome_id": "3b92af7a",
      "task_id": "t14",
      "predicted_confidence": 0.5,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "r07",
      "predicted_confidence": 0.05,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "f0a63272",
      "task_id": "t14",
      "predicted_confidence": 0.7,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "r02",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "e10",
      "predicted_confidence": 0.45,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "e06",
      "predicted_confidence": 0.35,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.91,
      "fitness": 0.546
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 14,
      "genome_id": "04646a8e",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8170599999999999
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "e03",
      "predicted_confidence": 0.6,
      "predicted_answer": "210",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.78746
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "r02",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.85626
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "t11",
      "predicted_confidence": 0.95,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 14,
      "genome_id": "dc488370",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 14,
      "genome_id": "8e830e0d",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.81514
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "r13",
      "predicted_confidence": 0.85,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "8",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "r02",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "b075c43b",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8194600000000001
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "t03",
      "predicted_confidence": 0.98,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "e04",
      "predicted_confidence": 0.15,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.4553600000000001
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "r07",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8911,
      "fitness": 0.87466
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 14,
      "genome_id": "e32ab164",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.61496
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "t11",
      "predicted_confidence": 0.85,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "2b6cf61f",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.3985
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 14,
      "genome_id": "0d2e07a7",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    }
  ],
  "test_results": {
    "n_tasks": 9,
    "n_agents": 10,
    "avg_raw_calibration": 0.9342411111111111,
    "avg_prediction_accuracy": 0.9285666666666667,
    "avg_task_accuracy": 0.8888888888888888,
    "best_fitness": 0.8886711111111112,
    "avg_fitness": 0.83514
  }
}