{
  "model": "openai/gpt-oss-20b",
  "slug": "gpt_oss_20b",
  "seed": 42,
  "elapsed_seconds": 344.5585548877716,
  "generation_stats": [
    {
      "generation": 0,
      "population_size": 10,
      "avg_fitness": 0.7091864,
      "best_fitness": 0.7464666666666666,
      "worst_fitness": 0.6719,
      "avg_raw_calibration": 0.7857833333333333,
      "avg_prediction_accuracy": 0.788644,
      "avg_task_accuracy": 0.68,
      "dominant_reasoning": "analogical",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 22.694797039031982
    },
    {
      "generation": 1,
      "population_size": 10,
      "avg_fitness": 0.773456,
      "best_fitness": 0.8261186666666666,
      "worst_fitness": 0.68082,
      "avg_raw_calibration": 0.8745160000000001,
      "avg_prediction_accuracy": 0.8690933333333334,
      "avg_task_accuracy": 0.74,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "recency",
      "elapsed_seconds": 29.24858808517456
    },
    {
      "generation": 2,
      "population_size": 10,
      "avg_fitness": 0.7942530666666666,
      "best_fitness": 0.845476,
      "worst_fitness": 0.7361466666666666,
      "avg_raw_calibration": 0.8580873333333333,
      "avg_prediction_accuracy": 0.8539773333333333,
      "avg_task_accuracy": 0.82,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "recency",
      "elapsed_seconds": 21.83705496788025
    },
    {
      "generation": 3,
      "population_size": 10,
      "avg_fitness": 0.6787257333333334,
      "best_fitness": 0.7384666666666667,
      "worst_fitness": 0.6179093333333333,
      "avg_raw_calibration": 0.7192879999999999,
      "avg_prediction_accuracy": 0.767654,
      "avg_task_accuracy": 0.6333333333333333,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 21.088199853897095
    },
    {
      "generation": 4,
      "population_size": 10,
      "avg_fitness": 0.6898679999999999,
      "best_fitness": 0.7522760000000001,
      "worst_fitness": 0.6578986666666666,
      "avg_raw_calibration": 0.7421833333333333,
      "avg_prediction_accuracy": 0.7824466666666667,
      "avg_task_accuracy": 0.6266666666666667,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "recency",
      "elapsed_seconds": 24.495834827423096
    },
    {
      "generation": 5,
      "population_size": 10,
      "avg_fitness": 0.6994504,
      "best_fitness": 0.7417426666666667,
      "worst_fitness": 0.6469066666666666,
      "avg_raw_calibration": 0.752506,
      "avg_prediction_accuracy": 0.779084,
      "avg_task_accuracy": 0.68,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "recency",
      "elapsed_seconds": 22.88764476776123
    },
    {
      "generation": 6,
      "population_size": 10,
      "avg_fitness": 0.6947894666666666,
      "best_fitness": 0.7496946666666666,
      "worst_fitness": 0.654612,
      "avg_raw_calibration": 0.7543226666666666,
      "avg_prediction_accuracy": 0.8055380000000001,
      "avg_task_accuracy": 0.6066666666666667,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "recency",
      "elapsed_seconds": 21.794766902923584
    },
    {
      "generation": 7,
      "population_size": 10,
      "avg_fitness": 0.7074857333333333,
      "best_fitness": 0.810844,
      "worst_fitness": 0.5912093333333334,
      "avg_raw_calibration": 0.7639573333333333,
      "avg_prediction_accuracy": 0.8029206666666667,
      "avg_task_accuracy": 0.64,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "recency",
      "elapsed_seconds": 24.255422115325928
    },
    {
      "generation": 8,
      "population_size": 10,
      "avg_fitness": 0.7111636,
      "best_fitness": 0.748988,
      "worst_fitness": 0.6662199999999999,
      "avg_raw_calibration": 0.7779426666666667,
      "avg_prediction_accuracy": 0.8099393333333333,
      "avg_task_accuracy": 0.66,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "recency",
      "elapsed_seconds": 23.183232069015503
    },
    {
      "generation": 9,
      "population_size": 10,
      "avg_fitness": 0.7087008,
      "best_fitness": 0.805952,
      "worst_fitness": 0.6654426666666667,
      "avg_raw_calibration": 0.7827693333333333,
      "avg_prediction_accuracy": 0.8218346666666666,
      "avg_task_accuracy": 0.6466666666666666,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "relevance",
      "elapsed_seconds": 21.787440061569214
    },
    {
      "generation": 10,
      "population_size": 10,
      "avg_fitness": 0.6620277333333333,
      "best_fitness": 0.726452,
      "worst_fitness": 0.596352,
      "avg_raw_calibration": 0.69097,
      "avg_prediction_accuracy": 0.7391573333333333,
      "avg_task_accuracy": 0.64,
      "dominant_reasoning": "debate-self",
      "dominant_memory": "relevance",
      "elapsed_seconds": 18.781163930892944
    },
    {
      "generation": 11,
      "population_size": 10,
      "avg_fitness": 0.6834512,
      "best_fitness": 0.7184106666666668,
      "worst_fitness": 0.6498053333333333,
      "avg_raw_calibration": 0.7256573333333334,
      "avg_prediction_accuracy": 0.7784186666666667,
      "avg_task_accuracy": 0.6066666666666667,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "relevance",
      "elapsed_seconds": 21.488433361053467
    },
    {
      "generation": 12,
      "population_size": 10,
      "avg_fitness": 0.7216465333333333,
      "best_fitness": 0.7769426666666667,
      "worst_fitness": 0.6586199999999999,
      "avg_raw_calibration": 0.7658333333333334,
      "avg_prediction_accuracy": 0.7871886666666666,
      "avg_task_accuracy": 0.7266666666666667,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "relevance",
      "elapsed_seconds": 20.614015102386475
    },
    {
      "generation": 13,
      "population_size": 10,
      "avg_fitness": 0.8038420000000001,
      "best_fitness": 0.8729426666666666,
      "worst_fitness": 0.7505853333333333,
      "avg_raw_calibration": 0.8855386666666667,
      "avg_prediction_accuracy": 0.8770699999999999,
      "avg_task_accuracy": 0.82,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "relevance",
      "elapsed_seconds": 22.320719957351685
    },
    {
      "generation": 14,
      "population_size": 10,
      "avg_fitness": 0.8172048000000001,
      "best_fitness": 0.850976,
      "worst_fitness": 0.7875173333333334,
      "avg_raw_calibration": 0.8983239999999999,
      "avg_prediction_accuracy": 0.888008,
      "avg_task_accuracy": 0.8333333333333334,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "relevance",
      "elapsed_seconds": 18.874937772750854
    }
  ],
  "all_genomes": [
    {
      "genome_id": "28f27c58",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.76,
      "temperature": 0.93,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "10de5433",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.47,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "7ba2a2a9",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 4,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.07,
      "risk_tolerance": 0.22,
      "temperature": 0.46,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "bb3bac81",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.24,
      "temperature": 0.53,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "02134daf",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.88,
      "temperature": 1.0,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "259b254e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.34,
      "temperature": 0.46,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "c97b3b7a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.14,
      "risk_tolerance": 0.38,
      "temperature": 0.4,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "7ae6c5d2",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.1,
      "temperature": 0.63,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "b64c449e",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.18,
      "temperature": 1.19,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "7d978319",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "debate-self",
      "memory_window": 9,
      "memory_weighting": "relevance",
      "confidence_bias": 0.07,
      "risk_tolerance": 0.42,
      "temperature": 0.51,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "baba1aea",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.34,
      "temperature": 0.46,
      "generation": 1,
      "parent_ids": [
        "259b254e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7ec0e5e2",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.76,
      "temperature": 0.93,
      "generation": 1,
      "parent_ids": [
        "28f27c58"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "29920b4c",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.63,
      "temperature": 0.93,
      "generation": 1,
      "parent_ids": [
        "10de5433",
        "28f27c58"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a65a1b69",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.34,
      "temperature": 1.1,
      "generation": 1,
      "parent_ids": [
        "28f27c58",
        "259b254e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "63c76057",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.34,
      "temperature": 0.32,
      "generation": 1,
      "parent_ids": [
        "259b254e",
        "28f27c58"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "464027f8",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.47,
      "generation": 1,
      "parent_ids": [
        "10de5433",
        "28f27c58"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a9352b9e",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.47,
      "generation": 1,
      "parent_ids": [
        "28f27c58",
        "10de5433"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eb021dc0",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.76,
      "temperature": 0.93,
      "generation": 1,
      "parent_ids": [
        "28f27c58",
        "10de5433"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e9d35bec",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.47,
      "generation": 1,
      "parent_ids": [
        "259b254e",
        "10de5433"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5408d001",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.14,
      "temperature": 0.46,
      "generation": 1,
      "parent_ids": [
        "259b254e",
        "10de5433"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "16cce7fc",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.47,
      "generation": 2,
      "parent_ids": [
        "a9352b9e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "77f18e86",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.14,
      "temperature": 0.46,
      "generation": 2,
      "parent_ids": [
        "5408d001"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "599b4090",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.46,
      "generation": 2,
      "parent_ids": [
        "5408d001",
        "a9352b9e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fb031903",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.34,
      "temperature": 0.46,
      "generation": 2,
      "parent_ids": [
        "5408d001",
        "baba1aea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f0fa4123",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.39,
      "generation": 2,
      "parent_ids": [
        "baba1aea",
        "a9352b9e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "24725c5d",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.27,
      "temperature": 0.65,
      "generation": 2,
      "parent_ids": [
        "baba1aea",
        "a9352b9e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c2e87f82",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.34,
      "temperature": 0.46,
      "generation": 2,
      "parent_ids": [
        "5408d001",
        "baba1aea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8d19f1f7",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.14,
      "temperature": 0.34,
      "generation": 2,
      "parent_ids": [
        "5408d001",
        "a9352b9e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9867b532",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.52,
      "generation": 2,
      "parent_ids": [
        "baba1aea",
        "a9352b9e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4fe6ad29",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.34,
      "temperature": 0.47,
      "generation": 2,
      "parent_ids": [
        "a9352b9e",
        "baba1aea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0ed5e469",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "9867b532"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9aa34000",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.47,
      "generation": 3,
      "parent_ids": [
        "16cce7fc"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "11dedcfb",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.14,
      "temperature": 0.42,
      "generation": 3,
      "parent_ids": [
        "9867b532",
        "77f18e86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "75110567",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 9,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.14,
      "temperature": 0.58,
      "generation": 3,
      "parent_ids": [
        "16cce7fc",
        "77f18e86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f564bab2",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.44,
      "generation": 3,
      "parent_ids": [
        "16cce7fc",
        "9867b532"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a170c1b1",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.07,
      "temperature": 0.61,
      "generation": 3,
      "parent_ids": [
        "77f18e86",
        "9867b532"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0ea80bf4",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.14,
      "temperature": 0.47,
      "generation": 3,
      "parent_ids": [
        "77f18e86",
        "16cce7fc"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6b0d5265",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 3,
      "parent_ids": [
        "77f18e86",
        "16cce7fc"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5a735290",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.46,
      "generation": 3,
      "parent_ids": [
        "9867b532",
        "77f18e86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2157cf17",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.47,
      "generation": 3,
      "parent_ids": [
        "16cce7fc",
        "77f18e86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b8a44517",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 4,
      "parent_ids": [
        "6b0d5265"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "835074d0",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.47,
      "generation": 4,
      "parent_ids": [
        "9aa34000"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a35de7a3",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.09,
      "temperature": 0.4,
      "generation": 4,
      "parent_ids": [
        "11dedcfb",
        "6b0d5265"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "32c2c532",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.42,
      "generation": 4,
      "parent_ids": [
        "9aa34000",
        "11dedcfb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7d91637b",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.27,
      "temperature": 0.39,
      "generation": 4,
      "parent_ids": [
        "9aa34000",
        "11dedcfb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d928a5e6",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 4,
      "parent_ids": [
        "6b0d5265",
        "11dedcfb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f404d023",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.09,
      "temperature": 0.42,
      "generation": 4,
      "parent_ids": [
        "11dedcfb",
        "6b0d5265"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "36d81198",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.47,
      "generation": 4,
      "parent_ids": [
        "9aa34000",
        "11dedcfb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eedf9cba",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.43,
      "generation": 4,
      "parent_ids": [
        "9aa34000",
        "6b0d5265"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d98f865b",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.49,
      "generation": 4,
      "parent_ids": [
        "9aa34000",
        "6b0d5265"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e5f2f151",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 5,
      "parent_ids": [
        "b8a44517"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ced6c813",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.49,
      "generation": 5,
      "parent_ids": [
        "d98f865b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c7cde991",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 5,
      "parent_ids": [
        "b8a44517",
        "d98f865b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2676bc32",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.42,
      "generation": 5,
      "parent_ids": [
        "32c2c532",
        "b8a44517"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "96accc41",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 5,
      "parent_ids": [
        "b8a44517",
        "32c2c532"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cf795bdf",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.42,
      "generation": 5,
      "parent_ids": [
        "d98f865b",
        "32c2c532"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3bf79bde",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.3,
      "temperature": 0.42,
      "generation": 5,
      "parent_ids": [
        "d98f865b",
        "32c2c532"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ea271d40",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.49,
      "generation": 5,
      "parent_ids": [
        "d98f865b",
        "b8a44517"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cbae18f9",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.27,
      "temperature": 0.42,
      "generation": 5,
      "parent_ids": [
        "d98f865b",
        "32c2c532"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ab80845f",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.63,
      "generation": 5,
      "parent_ids": [
        "d98f865b",
        "b8a44517"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "08c23126",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 6,
      "parent_ids": [
        "96accc41"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e705d4a8",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.49,
      "generation": 6,
      "parent_ids": [
        "ea271d40"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "76230350",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 6,
      "parent_ids": [
        "c7cde991",
        "96accc41"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6472b95a",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.49,
      "generation": 6,
      "parent_ids": [
        "ea271d40",
        "96accc41"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7e0bfff8",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.36,
      "generation": 6,
      "parent_ids": [
        "ea271d40",
        "c7cde991"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f667effb",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "debate-self",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.14,
      "temperature": 0.47,
      "generation": 6,
      "parent_ids": [
        "c7cde991",
        "96accc41"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9aa311d0",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.15,
      "temperature": 0.49,
      "generation": 6,
      "parent_ids": [
        "c7cde991",
        "ea271d40"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2a5b72ce",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.49,
      "generation": 6,
      "parent_ids": [
        "96accc41",
        "ea271d40"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9e0e1a25",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.26,
      "temperature": 0.51,
      "generation": 6,
      "parent_ids": [
        "96accc41",
        "c7cde991"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fd957fc2",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.41,
      "generation": 6,
      "parent_ids": [
        "ea271d40",
        "96accc41"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "970b1dd1",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.15,
      "temperature": 0.49,
      "generation": 7,
      "parent_ids": [
        "9aa311d0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3ae31dd9",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.49,
      "generation": 7,
      "parent_ids": [
        "2a5b72ce"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6ff457a7",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.49,
      "generation": 7,
      "parent_ids": [
        "08c23126",
        "2a5b72ce"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b22bc1fc",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 9,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.64,
      "generation": 7,
      "parent_ids": [
        "08c23126",
        "2a5b72ce"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c7ec6241",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.03,
      "temperature": 0.49,
      "generation": 7,
      "parent_ids": [
        "9aa311d0",
        "08c23126"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4afe9748",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.58,
      "generation": 7,
      "parent_ids": [
        "08c23126",
        "9aa311d0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d2a5dbf3",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.15,
      "temperature": 0.49,
      "generation": 7,
      "parent_ids": [
        "2a5b72ce",
        "9aa311d0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "32cee1d1",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.49,
      "generation": 7,
      "parent_ids": [
        "9aa311d0",
        "08c23126"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "924890a9",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 7,
      "parent_ids": [
        "9aa311d0",
        "08c23126"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "375dde0f",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.32,
      "generation": 7,
      "parent_ids": [
        "9aa311d0",
        "2a5b72ce"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "062f4340",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.49,
      "generation": 8,
      "parent_ids": [
        "3ae31dd9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d2441963",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 8,
      "parent_ids": [
        "924890a9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0b00c8f0",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.29,
      "temperature": 0.49,
      "generation": 8,
      "parent_ids": [
        "924890a9",
        "6ff457a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "430f489f",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.68,
      "generation": 8,
      "parent_ids": [
        "6ff457a7",
        "3ae31dd9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eecb7160",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.14,
      "temperature": 0.49,
      "generation": 8,
      "parent_ids": [
        "924890a9",
        "6ff457a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b2325398",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.46,
      "generation": 8,
      "parent_ids": [
        "6ff457a7",
        "3ae31dd9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "81aacefe",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 8,
      "parent_ids": [
        "6ff457a7",
        "924890a9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e89e5658",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.49,
      "generation": 8,
      "parent_ids": [
        "924890a9",
        "3ae31dd9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8f3f828e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.43,
      "generation": 8,
      "parent_ids": [
        "924890a9",
        "3ae31dd9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8718a3b9",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.49,
      "generation": 8,
      "parent_ids": [
        "924890a9",
        "6ff457a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "35e9f79d",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.68,
      "generation": 9,
      "parent_ids": [
        "430f489f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b4b744a1",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 9,
      "parent_ids": [
        "d2441963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cf2bf1dd",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.4,
      "generation": 9,
      "parent_ids": [
        "81aacefe",
        "430f489f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "99222bec",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 9,
      "parent_ids": [
        "d2441963",
        "430f489f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "24a8c2f3",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 9,
      "parent_ids": [
        "81aacefe",
        "d2441963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dd715e3e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 9,
      "parent_ids": [
        "430f489f",
        "d2441963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7c225ed9",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.68,
      "generation": 9,
      "parent_ids": [
        "430f489f",
        "d2441963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "626c69e5",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.14,
      "temperature": 0.4,
      "generation": 9,
      "parent_ids": [
        "81aacefe",
        "d2441963"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5114be9b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 9,
      "parent_ids": [
        "d2441963",
        "81aacefe"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ccc5c06b",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.25,
      "generation": 9,
      "parent_ids": [
        "d2441963",
        "81aacefe"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a1716eb4",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.4,
      "generation": 10,
      "parent_ids": [
        "cf2bf1dd"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "28aee44b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 10,
      "parent_ids": [
        "99222bec"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a950a8ea",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.0,
      "temperature": 0.49,
      "generation": 10,
      "parent_ids": [
        "cf2bf1dd",
        "5114be9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "af6716ea",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.03,
      "temperature": 0.29,
      "generation": 10,
      "parent_ids": [
        "cf2bf1dd",
        "5114be9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "886c3479",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.55,
      "generation": 10,
      "parent_ids": [
        "99222bec",
        "5114be9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b7fb9109",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 10,
      "parent_ids": [
        "99222bec",
        "5114be9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bfd7a052",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.26,
      "temperature": 0.4,
      "generation": 10,
      "parent_ids": [
        "99222bec",
        "cf2bf1dd"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "22afa4ca",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 10,
      "parent_ids": [
        "99222bec",
        "5114be9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bbe0bdf5",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.0,
      "temperature": 0.4,
      "generation": 10,
      "parent_ids": [
        "cf2bf1dd",
        "5114be9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cb42d94e",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.4,
      "generation": 10,
      "parent_ids": [
        "cf2bf1dd",
        "5114be9b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0c112002",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.26,
      "temperature": 0.4,
      "generation": 11,
      "parent_ids": [
        "bfd7a052"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7e73eed7",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.03,
      "temperature": 0.29,
      "generation": 11,
      "parent_ids": [
        "af6716ea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6060acc6",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.4,
      "generation": 11,
      "parent_ids": [
        "bfd7a052",
        "af6716ea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1158b536",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.26,
      "temperature": 0.29,
      "generation": 11,
      "parent_ids": [
        "bfd7a052",
        "af6716ea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "75be59e4",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 11,
      "parent_ids": [
        "af6716ea",
        "bfd7a052"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0996e1cc",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.4,
      "generation": 11,
      "parent_ids": [
        "28aee44b",
        "bfd7a052"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bf14f888",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.03,
      "temperature": 0.29,
      "generation": 11,
      "parent_ids": [
        "bfd7a052",
        "af6716ea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9596f52c",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.0,
      "temperature": 0.54,
      "generation": 11,
      "parent_ids": [
        "af6716ea",
        "bfd7a052"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "108ad7d3",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.26,
      "temperature": 0.4,
      "generation": 11,
      "parent_ids": [
        "28aee44b",
        "bfd7a052"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c5b13951",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.3,
      "generation": 11,
      "parent_ids": [
        "28aee44b",
        "af6716ea"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6ddfa208",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.0,
      "temperature": 0.54,
      "generation": 12,
      "parent_ids": [
        "9596f52c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1e3b32b6",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 12,
      "parent_ids": [
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f7a24cf3",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.04,
      "temperature": 0.2,
      "generation": 12,
      "parent_ids": [
        "75be59e4",
        "9596f52c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "527531a5",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.0,
      "temperature": 0.25,
      "generation": 12,
      "parent_ids": [
        "9596f52c",
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "59393179",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.0,
      "temperature": 0.25,
      "generation": 12,
      "parent_ids": [
        "9596f52c",
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e0d52aee",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 12,
      "parent_ids": [
        "108ad7d3",
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "de6d9631",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 12,
      "parent_ids": [
        "9596f52c",
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f14ed33e",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 12,
      "parent_ids": [
        "108ad7d3",
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f202b950",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 12,
      "parent_ids": [
        "108ad7d3",
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bb1c9ece",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.3,
      "generation": 12,
      "parent_ids": [
        "9596f52c",
        "75be59e4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5ee8a626",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "f14ed33e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "77ec8ab2",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "f202b950"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "20472180",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "de6d9631",
        "f202b950"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7bf3590b",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "de6d9631",
        "f14ed33e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6bca1849",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "f202b950",
        "de6d9631"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a171fd29",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "f202b950",
        "de6d9631"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "65571fac",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.0,
      "temperature": 0.23,
      "generation": 13,
      "parent_ids": [
        "f14ed33e",
        "de6d9631"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "737b7325",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "f14ed33e",
        "f202b950"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9c4c5329",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.0,
      "temperature": 0.25,
      "generation": 13,
      "parent_ids": [
        "de6d9631",
        "f14ed33e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d95b3033",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.17,
      "temperature": 0.27,
      "generation": 13,
      "parent_ids": [
        "f14ed33e",
        "f202b950"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "80d5c2ad",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "737b7325"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0c260fca",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.0,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "9c4c5329"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d84741c7",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.0,
      "temperature": 0.16,
      "generation": 14,
      "parent_ids": [
        "77ec8ab2",
        "737b7325"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "42bbc434",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "737b7325",
        "77ec8ab2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "69857ddb",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "737b7325",
        "9c4c5329"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2a67af74",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.04,
      "temperature": 0.38,
      "generation": 14,
      "parent_ids": [
        "9c4c5329",
        "737b7325"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "49628338",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.0,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "77ec8ab2",
        "9c4c5329"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f9687d4e",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.0,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "9c4c5329",
        "77ec8ab2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "801ba2fc",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "737b7325",
        "77ec8ab2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "38e656ba",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.03,
      "temperature": 0.25,
      "generation": 14,
      "parent_ids": [
        "77ec8ab2",
        "737b7325"
      ],
      "fitness_history": []
    }
  ],
  "all_results": [
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.75114
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (the skin of a polar bear is black)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "About 4,000,000 miles of roads in the U.S",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "e11",
      "predicted_confidence": 0.85,
      "predicted_answer": "6",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 0,
      "genome_id": "28f27c58",
      "task_id": "e10",
      "predicted_confidence": 0.68,
      "predicted_answer": "Approximately 10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.8976000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.85914
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "e11",
      "predicted_confidence": 0.93,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9951,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 0,
      "genome_id": "10de5433",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River, about 6,300\u202fkm long, is the longest river that lies entirely within a single country (China)",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.24309999999999976,
      "fitness": 0.14585999999999985
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "7ba2a2a9",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "t15",
      "predicted_confidence": 0.98,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "e11",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 0,
      "genome_id": "bb3bac81",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7785
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Canada",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "~235\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.79146
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, which is about 6,300\u202fkm long and lies entirely within one country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 0,
      "genome_id": "02134daf",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.72936
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.72936
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 0,
      "genome_id": "259b254e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.11639999999999984,
      "fitness": 0.0698399999999999
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "c97b3b7a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 0,
      "genome_id": "7ae6c5d2",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "3000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.81416
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "e05",
      "predicted_confidence": 0.6,
      "predicted_answer": "19,000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7296,
      "fitness": 0.79776
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River \u2013 about 6,300\u202fkm long \u2013 is the longest river that lies entirely within a single sovereign state (China)",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black (polarbear skin is black beneath its fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00PM",
      "ground_truth": "5:00 PM",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "e11",
      "predicted_confidence": 0.75,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "t09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 0,
      "genome_id": "b64c449e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "About 27,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "t15",
      "predicted_confidence": 1.0,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9991,
      "fitness": 0.91946
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River, which flows entirely within China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9991,
      "fitness": 0.97946
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "7d978319",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.88266
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 1,
      "genome_id": "baba1aea",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.73114
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain contains far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that flows entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.7711399999999999
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "7ec0e5e2",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "13",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.92954
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.73114
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog\u2019s brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.75114
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.7711399999999999
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "r06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yes",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "29920b4c",
      "task_id": "e11",
      "predicted_confidence": 0.75,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.91856
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.70936
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.72936
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "r02",
      "predicted_confidence": 0.98,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "r06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 1,
      "genome_id": "a65a1b69",
      "task_id": "e11",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 light-years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Giovanni\u202fDomenico\u202fCassini was the first to observe Saturn\u2019s rings, though he did not understand their true nature",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "63c76057",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "e07",
      "predicted_confidence": 0.95,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 1,
      "genome_id": "464027f8",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain contains far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "a9352b9e",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4,500",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "t13",
      "predicted_confidence": 0.97,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9856,
      "fitness": 0.85136
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.75114
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.7711399999999999
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "eb021dc0",
      "task_id": "e11",
      "predicted_confidence": 0.85,
      "predicted_answer": "6",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "760,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 1,
      "genome_id": "e9d35bec",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.87416
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first person to observe Saturn's rings, though he did not understand their nature",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest thousand)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 1,
      "genome_id": "5408d001",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (rounded to the nearest thousand)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first person to observe Saturn\u2019s rings, although he did not understand their nature",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 2,
      "genome_id": "16cce7fc",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 2,
      "genome_id": "77f18e86",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.91856
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 2,
      "genome_id": "599b4090",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "100",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first person to observe Saturn\u2019s rings, though he did not know what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 2,
      "genome_id": "fb031903",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.91856
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "20,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 2,
      "genome_id": "f0fa4123",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands (over 17,000)",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.70936
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 2,
      "genome_id": "24725c5d",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.91856
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "t15",
      "predicted_confidence": 1.0,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.88266
    },
    {
      "generation": 2,
      "genome_id": "c2e87f82",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden has the most islands",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Christiaan Huygens was the first person to observe and correctly identify Saturn\u2019s rings",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 2,
      "genome_id": "8d19f1f7",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 2,
      "genome_id": "9867b532",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "30,000 airports (approximately)",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "210 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings (though he did not realize they were rings)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 2,
      "genome_id": "4fe6ad29",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The picture shows the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 3,
      "genome_id": "0ed5e469",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the narrator\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 3,
      "genome_id": "9aa34000",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.8660000000000001
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 3,
      "genome_id": "11dedcfb",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "760,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 3,
      "genome_id": "75110567",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "10",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 3,
      "genome_id": "f564bab2",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles (about 451\u202fkm\u00b2)",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 3,
      "genome_id": "a170c1b1",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles (approximately 459\u202fkm\u00b2) is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.91856
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The picture shows the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 3,
      "genome_id": "0ea80bf4",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 3,
      "genome_id": "6b0d5265",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "He is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "5a735290",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 3,
      "genome_id": "2157cf17",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The picture is of the man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "50",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not understand their nature",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "b8a44517",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black (polar bears have black skin beneath their fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "835074d0",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "6",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "a35de7a3",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "r06",
      "predicted_confidence": 0.97,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (in 1610) was the first to observe Saturn\u2019s rings, though he did not understand what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "32c2c532",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.9060000000000001
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "7d91637b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "13 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.8660000000000001
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (he first observed Saturn\u2019s rings in 1610, though he did not realize they were rings)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 4,
      "genome_id": "d928a5e6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.9060000000000001
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "f404d023",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "36d81198",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "eedf9cba",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "560,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "e05",
      "predicted_confidence": 0.95,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not recognize them as such",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "d98f865b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "e5f2f151",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (approximately)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "ced6c813",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "216 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "t15",
      "predicted_confidence": 1.0,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "c7cde991",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River, which flows entirely within China, is the longest river that stays within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "2676bc32",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 5,
      "genome_id": "96accc41",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "cf795bdf",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.72936
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports (rounded to the nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 5,
      "genome_id": "3bf79bde",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "220\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 5,
      "genome_id": "ea271d40",
      "task_id": "t13",
      "predicted_confidence": 0.99,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "cbae18f9",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, at about 6,300\u202fkm, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9059999999999999
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "ab80845f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn's rings, though he did not understand what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 6,
      "genome_id": "08c23126",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but described them as \u201cears\u201d or \u201chandles\u201d rather than recognizing them as rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "12,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 6,
      "genome_id": "e705d4a8",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "76230350",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,300",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "6472b95a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not yet understand their nature",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "r06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yes",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "7e0bfff8",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "e10",
      "predicted_confidence": 0.9,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "500 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden has the most islands of any country",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 6,
      "genome_id": "f667effb",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he mistook them for moons",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black (polar bears have black skin underneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 6,
      "genome_id": "9aa311d0",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "t07",
      "predicted_confidence": 0.99,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 6,
      "genome_id": "2a5b72ce",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The picture is of the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "t09",
      "predicted_confidence": 0.92,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.15359999999999996,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "9e0e1a25",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "t14",
      "predicted_confidence": 0.98,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,500 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "220\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.7985
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "fd957fc2",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "450",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "t07",
      "predicted_confidence": 1.0,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "250 million",
      "ground_truth": "220",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "970b1dd1",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "3ae31dd9",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "6ff457a7",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo\u202fGalilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "He is looking at his own son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "b22bc1fc",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.9060000000000001
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands, with over 17,000 islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "r11",
      "predicted_confidence": 0.98,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 7,
      "genome_id": "c7ec6241",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands, with over 17,000 islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "4afe9748",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "t15",
      "predicted_confidence": 0.97,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.05910000000000004,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "t14",
      "predicted_confidence": 0.98,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "d2a5dbf3",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden has the most islands",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "560,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "r12",
      "predicted_confidence": 0.98,
      "predicted_answer": "The picture is of his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 7,
      "genome_id": "32cee1d1",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "924890a9",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "375dde0f",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not know they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "r07",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "062f4340",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "d2441963",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "S\u00e3o Tom\u00e9 and Pr\u00edncipe is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "r14",
      "predicted_confidence": 0.98,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.7985
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "0b00c8f0",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not recognize them as rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "210\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.8623400000000001
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "430f489f",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (he first observed Saturn\u2019s rings in 1610, though he did not recognize them as rings)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 5,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 8,
      "genome_id": "eecb7160",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "b2325398",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "81aacefe",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "210\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.7985
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "e89e5658",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "r14",
      "predicted_confidence": 0.98,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "8f3f828e",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "100",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "8718a3b9",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "t14",
      "predicted_confidence": 0.98,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei, who first noted Saturn's \"ears\" with his early telescope in 1610 but did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 9,
      "genome_id": "35e9f79d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, although he mistakenly thought they were moons",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "b4b744a1",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "cf2bf1dd",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9059999999999999
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "99222bec",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "24a8c2f3",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.70936
    },
    {
      "generation": 9,
      "genome_id": "dd715e3e",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "7c225ed9",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,300 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "626c69e5",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not know what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 9,
      "genome_id": "5114be9b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 9,
      "genome_id": "ccc5c06b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "700,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Christiaan Huygens",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "a1716eb4",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "e05",
      "predicted_confidence": 0.75,
      "predicted_answer": "20,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.84,
      "fitness": 0.8640000000000001
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first person to observe Saturn\u2019s rings, though he did not understand what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "28aee44b",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "20,000 airports (approximately)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.87416
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 10,
      "genome_id": "a950a8ea",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "e11",
      "predicted_confidence": 0.95,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "100 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "e03",
      "predicted_confidence": 0.7,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.81914
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "af6716ea",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "886c3479",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "e11",
      "predicted_confidence": 0.95,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "40,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "450",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "b7fb9109",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "6",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The picture is of his son",
      "ground_truth": "His son",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 10,
      "genome_id": "bfd7a052",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "r12",
      "predicted_confidence": 0.99,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 10,
      "genome_id": "22afa4ca",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "r12",
      "predicted_confidence": 1.0,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 10,
      "genome_id": "bbe0bdf5",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "t15",
      "predicted_confidence": 1.0,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s \u201cears\u201d in 1610 but did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man in the picture is the speaker\u2019s son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "cb42d94e",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9059999999999999
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "r06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 11,
      "genome_id": "0c112002",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.9023400000000001
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 11,
      "genome_id": "7e73eed7",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "r14",
      "predicted_confidence": 0.97,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9059999999999999
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 11,
      "genome_id": "6060acc6",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 11,
      "genome_id": "1158b536",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.9023400000000001
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.9023400000000001
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 11,
      "genome_id": "75be59e4",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first person to observe Saturn\u2019s rings, though he did not know what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 11,
      "genome_id": "0996e1cc",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.9023400000000001
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "e11",
      "predicted_confidence": 0.95,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 11,
      "genome_id": "bf14f888",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "30,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "r11",
      "predicted_confidence": 0.98,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (in 1610)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 11,
      "genome_id": "9596f52c",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s \u201cears\u201d in 1610 but did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 11,
      "genome_id": "108ad7d3",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Christiaan Huygens",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 11,
      "genome_id": "c5b13951",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "6ddfa208",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Christiaan Huygens, who first observed Saturn\u2019s rings in 1655",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 12,
      "genome_id": "1e3b32b6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles (approximately 451\u202fkm\u00b2)",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 12,
      "genome_id": "f7a24cf3",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Christiaan Huygens",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "527531a5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "6",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 12,
      "genome_id": "59393179",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "e11",
      "predicted_confidence": 0.85,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "e0d52aee",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.69424
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.91064
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 12,
      "genome_id": "de6d9631",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "e03",
      "predicted_confidence": 0.95,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "e11",
      "predicted_confidence": 0.95,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "f14ed33e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 12,
      "genome_id": "f202b950",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Christiaan Huygens was the first astronomer to observe Saturn\u2019s rings, though he did not yet understand their true nature",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "r06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "13",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "bb1c9ece",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "e03",
      "predicted_confidence": 0.85,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 light-years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "5ee8a626",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "77ec8ab2",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "20472180",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "7bf3590b",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "r06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yes",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.87834
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "6bca1849",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.74186
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 13,
      "genome_id": "a171fd29",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "650,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "230 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "e11",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 light\u2011years",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "65571fac",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "737b7325",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles (approximately 452\u202fkm\u00b2)",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "9c4c5329",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "100",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "d95b3033",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "80d5c2ad",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Christiaan Huygens",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 light\u2011years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 14,
      "genome_id": "0c260fca",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (he first observed Saturn\u2019s rings in 1610, describing them as \u201cears\u201d or \u201chandles\u201d but not realizing they were rings)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "d84741c7",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "r06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 14,
      "genome_id": "42bbc434",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "69857ddb",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240\u202fmillion",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but mistakenly described them as \u201cears\u201d or \u201chandles.\u201d",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 14,
      "genome_id": "2a67af74",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black (polar bears have black skin beneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles (approximately 459\u202fkm\u00b2)",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "49628338",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "r02",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.90954
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "e11",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "5",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 14,
      "genome_id": "f9687d4e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "220 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "r06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.6864999999999999
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "e11",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 light-years",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 14,
      "genome_id": "801ba2fc",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "3,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "e03",
      "predicted_confidence": 0.9,
      "predicted_answer": "240 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "r06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Giovanni\u202fBattista\u202fHodierna, who first noted Saturn\u2019s rings in 1654 but did not understand their nature",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "e11",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 14,
      "genome_id": "38e656ba",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    }
  ],
  "test_results": {
    "n_tasks": 9,
    "n_agents": 10,
    "avg_raw_calibration": 0.9715699999999999,
    "avg_prediction_accuracy": 0.9619255555555556,
    "avg_task_accuracy": 0.8888888888888888,
    "best_fitness": 0.8296244444444444,
    "avg_fitness": 0.8282664444444444
  }
}