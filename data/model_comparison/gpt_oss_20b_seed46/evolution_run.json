{
  "model": "openai/gpt-oss-20b",
  "slug": "gpt_oss_20b",
  "seed": 46,
  "elapsed_seconds": 358.9905800819397,
  "generation_stats": [
    {
      "generation": 0,
      "population_size": 10,
      "avg_fitness": 0.6926289333333333,
      "best_fitness": 0.7267466666666667,
      "worst_fitness": 0.6613706666666667,
      "avg_raw_calibration": 0.7993426666666666,
      "avg_prediction_accuracy": 0.7894926666666667,
      "avg_task_accuracy": 0.68,
      "dominant_reasoning": "analogical",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 23.938138008117676
    },
    {
      "generation": 1,
      "population_size": 10,
      "avg_fitness": 0.7803373333333333,
      "best_fitness": 0.8022639999999999,
      "worst_fitness": 0.7579666666666667,
      "avg_raw_calibration": 0.8775199999999999,
      "avg_prediction_accuracy": 0.8816733333333334,
      "avg_task_accuracy": 0.7666666666666667,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 23.474778652191162
    },
    {
      "generation": 2,
      "population_size": 10,
      "avg_fitness": 0.7451789333333333,
      "best_fitness": 0.7899626666666666,
      "worst_fitness": 0.7085893333333332,
      "avg_raw_calibration": 0.8537680000000001,
      "avg_prediction_accuracy": 0.8684093333333334,
      "avg_task_accuracy": 0.7066666666666667,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 20.10906481742859
    },
    {
      "generation": 3,
      "population_size": 10,
      "avg_fitness": 0.7128237333333333,
      "best_fitness": 0.7429,
      "worst_fitness": 0.6795853333333334,
      "avg_raw_calibration": 0.8313266666666667,
      "avg_prediction_accuracy": 0.8618173333333334,
      "avg_task_accuracy": 0.6133333333333333,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 27.7704598903656
    },
    {
      "generation": 4,
      "population_size": 10,
      "avg_fitness": 0.7388172,
      "best_fitness": 0.7824466666666666,
      "worst_fitness": 0.7110133333333333,
      "avg_raw_calibration": 0.8317326666666667,
      "avg_prediction_accuracy": 0.8513620000000001,
      "avg_task_accuracy": 0.7066666666666667,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 23.02808904647827
    },
    {
      "generation": 5,
      "population_size": 10,
      "avg_fitness": 0.7382529333333333,
      "best_fitness": 0.7975066666666667,
      "worst_fitness": 0.7124400000000001,
      "avg_raw_calibration": 0.8076706666666666,
      "avg_prediction_accuracy": 0.8321993333333333,
      "avg_task_accuracy": 0.7066666666666667,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 21.301756858825684
    },
    {
      "generation": 6,
      "population_size": 10,
      "avg_fitness": 0.6917827999999999,
      "best_fitness": 0.7292093333333333,
      "worst_fitness": 0.6540959999999999,
      "avg_raw_calibration": 0.7870086666666667,
      "avg_prediction_accuracy": 0.831638,
      "avg_task_accuracy": 0.6,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 22.56076407432556
    },
    {
      "generation": 7,
      "population_size": 10,
      "avg_fitness": 0.7417674666666667,
      "best_fitness": 0.7735413333333333,
      "worst_fitness": 0.7169439999999999,
      "avg_raw_calibration": 0.8531673333333333,
      "avg_prediction_accuracy": 0.8798346666666667,
      "avg_task_accuracy": 0.68,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 23.26878309249878
    },
    {
      "generation": 8,
      "population_size": 10,
      "avg_fitness": 0.7317170666666667,
      "best_fitness": 0.7506826666666666,
      "worst_fitness": 0.6949013333333334,
      "avg_raw_calibration": 0.835008,
      "avg_prediction_accuracy": 0.8684173333333333,
      "avg_task_accuracy": 0.6466666666666666,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 23.799784183502197
    },
    {
      "generation": 9,
      "population_size": 10,
      "avg_fitness": 0.7792966666666666,
      "best_fitness": 0.8235666666666667,
      "worst_fitness": 0.75808,
      "avg_raw_calibration": 0.8738633333333333,
      "avg_prediction_accuracy": 0.8843833333333333,
      "avg_task_accuracy": 0.7666666666666667,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "relevance",
      "elapsed_seconds": 22.903635025024414
    },
    {
      "generation": 10,
      "population_size": 10,
      "avg_fitness": 0.7862625333333333,
      "best_fitness": 0.848004,
      "worst_fitness": 0.71302,
      "avg_raw_calibration": 0.9013166666666667,
      "avg_prediction_accuracy": 0.9155486666666667,
      "avg_task_accuracy": 0.74,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 21.775314807891846
    },
    {
      "generation": 11,
      "population_size": 10,
      "avg_fitness": 0.7629126666666666,
      "best_fitness": 0.81938,
      "worst_fitness": 0.7112466666666666,
      "avg_raw_calibration": 0.8589973333333334,
      "avg_prediction_accuracy": 0.87241,
      "avg_task_accuracy": 0.7466666666666667,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "relevance",
      "elapsed_seconds": 22.82560896873474
    },
    {
      "generation": 12,
      "population_size": 10,
      "avg_fitness": 0.6731152,
      "best_fitness": 0.7122733333333334,
      "worst_fitness": 0.6380560000000001,
      "avg_raw_calibration": 0.7761286666666666,
      "avg_prediction_accuracy": 0.8298586666666667,
      "avg_task_accuracy": 0.54,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 24.52253794670105
    },
    {
      "generation": 13,
      "population_size": 10,
      "avg_fitness": 0.7566398666666666,
      "best_fitness": 0.8213866666666667,
      "worst_fitness": 0.7206613333333334,
      "avg_raw_calibration": 0.8553126666666666,
      "avg_prediction_accuracy": 0.8712886666666666,
      "avg_task_accuracy": 0.7333333333333333,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "relevance",
      "elapsed_seconds": 22.037103176116943
    },
    {
      "generation": 14,
      "population_size": 10,
      "avg_fitness": 0.7058761333333333,
      "best_fitness": 0.761752,
      "worst_fitness": 0.6621693333333333,
      "avg_raw_calibration": 0.7933833333333333,
      "avg_prediction_accuracy": 0.828238,
      "avg_task_accuracy": 0.6533333333333333,
      "dominant_reasoning": "analogical",
      "dominant_memory": "relevance",
      "elapsed_seconds": 21.849859714508057
    }
  ],
  "all_genomes": [
    {
      "genome_id": "d7e2e580",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.87,
      "temperature": 0.77,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "0620e38d",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.0,
      "risk_tolerance": 0.53,
      "temperature": 0.65,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "b03e666b",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": 0.07,
      "risk_tolerance": 0.5,
      "temperature": 0.73,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "ab22b67c",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.68,
      "temperature": 1.01,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "b45bc97d",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.14,
      "risk_tolerance": 0.51,
      "temperature": 0.46,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "fc4f9414",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 3,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.25,
      "temperature": 0.44,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "de60ae4f",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.43,
      "temperature": 0.92,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "bcdcc0f3",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.84,
      "temperature": 0.87,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "6bba167d",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.89,
      "temperature": 1.0,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "b3784113",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.17,
      "temperature": 0.63,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "7a3a0364",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.68,
      "temperature": 1.01,
      "generation": 1,
      "parent_ids": [
        "ab22b67c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "286751b3",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.89,
      "temperature": 1.0,
      "generation": 1,
      "parent_ids": [
        "6bba167d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8a7d1975",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 1,
      "parent_ids": [
        "6bba167d",
        "de60ae4f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4d12ac3a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.43,
      "temperature": 1.08,
      "generation": 1,
      "parent_ids": [
        "ab22b67c",
        "de60ae4f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "998910c6",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.89,
      "temperature": 0.97,
      "generation": 1,
      "parent_ids": [
        "6bba167d",
        "ab22b67c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "023484ab",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "elimination",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.43,
      "temperature": 0.99,
      "generation": 1,
      "parent_ids": [
        "6bba167d",
        "de60ae4f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "db714276",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.89,
      "temperature": 1.0,
      "generation": 1,
      "parent_ids": [
        "6bba167d",
        "de60ae4f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8bbaa5d3",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 3,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.43,
      "temperature": 1.0,
      "generation": 1,
      "parent_ids": [
        "6bba167d",
        "de60ae4f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "26efc443",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.89,
      "temperature": 1.01,
      "generation": 1,
      "parent_ids": [
        "6bba167d",
        "ab22b67c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2b098c55",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 9,
      "memory_weighting": "recency",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.43,
      "temperature": 1.01,
      "generation": 1,
      "parent_ids": [
        "de60ae4f",
        "ab22b67c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "400fc8ba",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.89,
      "temperature": 0.97,
      "generation": 2,
      "parent_ids": [
        "998910c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "67bce988",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 2,
      "parent_ids": [
        "8a7d1975"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9788276c",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.89,
      "temperature": 1.0,
      "generation": 2,
      "parent_ids": [
        "998910c6",
        "286751b3"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3c80aadd",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.89,
      "temperature": 1.0,
      "generation": 2,
      "parent_ids": [
        "286751b3",
        "998910c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "163e4b1b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.89,
      "temperature": 0.97,
      "generation": 2,
      "parent_ids": [
        "998910c6",
        "286751b3"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "14b09d97",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 2,
      "parent_ids": [
        "998910c6",
        "8a7d1975"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "550c35af",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.44,
      "temperature": 0.97,
      "generation": 2,
      "parent_ids": [
        "998910c6",
        "8a7d1975"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0b00db3d",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.89,
      "temperature": 0.92,
      "generation": 2,
      "parent_ids": [
        "8a7d1975",
        "998910c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8ae543b9",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.89,
      "temperature": 0.92,
      "generation": 2,
      "parent_ids": [
        "8a7d1975",
        "286751b3"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "51a6ca86",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.89,
      "temperature": 0.92,
      "generation": 2,
      "parent_ids": [
        "998910c6",
        "8a7d1975"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8a4cf810",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 3,
      "parent_ids": [
        "14b09d97"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "85540ad1",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.44,
      "temperature": 0.97,
      "generation": 3,
      "parent_ids": [
        "550c35af"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "adfa2661",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 3,
      "parent_ids": [
        "550c35af",
        "14b09d97"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dbd0449f",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 3,
      "parent_ids": [
        "67bce988",
        "14b09d97"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "97c3628e",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.97,
      "generation": 3,
      "parent_ids": [
        "14b09d97",
        "550c35af"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4f7308a2",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.44,
      "temperature": 0.91,
      "generation": 3,
      "parent_ids": [
        "550c35af",
        "67bce988"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "52a2c69c",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.79,
      "generation": 3,
      "parent_ids": [
        "67bce988",
        "14b09d97"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c3e1daf4",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.45,
      "temperature": 1.1,
      "generation": 3,
      "parent_ids": [
        "14b09d97",
        "67bce988"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "484b8f67",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 3,
      "parent_ids": [
        "14b09d97",
        "67bce988"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "beab94d7",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.44,
      "temperature": 0.97,
      "generation": 3,
      "parent_ids": [
        "550c35af",
        "67bce988"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1fde2aa5",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.44,
      "temperature": 0.97,
      "generation": 4,
      "parent_ids": [
        "85540ad1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b8adb714",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.34,
      "temperature": 0.92,
      "generation": 4,
      "parent_ids": [
        "8a4cf810"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7662bbf9",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.79,
      "generation": 4,
      "parent_ids": [
        "52a2c69c",
        "8a4cf810"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "25a62caf",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.45,
      "temperature": 0.97,
      "generation": 4,
      "parent_ids": [
        "52a2c69c",
        "85540ad1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d2744711",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.44,
      "temperature": 1.12,
      "generation": 4,
      "parent_ids": [
        "85540ad1",
        "8a4cf810"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a31ecc45",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.44,
      "temperature": 0.97,
      "generation": 4,
      "parent_ids": [
        "85540ad1",
        "52a2c69c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dd8005af",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.44,
      "temperature": 0.97,
      "generation": 4,
      "parent_ids": [
        "85540ad1",
        "8a4cf810"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7b713060",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.23,
      "temperature": 0.92,
      "generation": 4,
      "parent_ids": [
        "85540ad1",
        "8a4cf810"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "59d6a684",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.24,
      "temperature": 1.04,
      "generation": 4,
      "parent_ids": [
        "52a2c69c",
        "8a4cf810"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b3ee0fa1",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.27,
      "temperature": 0.79,
      "generation": 4,
      "parent_ids": [
        "8a4cf810",
        "52a2c69c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dc4693f1",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.44,
      "temperature": 1.12,
      "generation": 5,
      "parent_ids": [
        "d2744711"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4b45b7fa",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.79,
      "generation": 5,
      "parent_ids": [
        "7662bbf9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "161ebcde",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 5,
      "parent_ids": [
        "7662bbf9",
        "25a62caf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "67f12066",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.44,
      "temperature": 1.12,
      "generation": 5,
      "parent_ids": [
        "25a62caf",
        "d2744711"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c1f6c2a1",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.79,
      "generation": 5,
      "parent_ids": [
        "25a62caf",
        "7662bbf9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "31bb0948",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.28,
      "temperature": 1.12,
      "generation": 5,
      "parent_ids": [
        "7662bbf9",
        "d2744711"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ce0b927c",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.45,
      "temperature": 0.97,
      "generation": 5,
      "parent_ids": [
        "25a62caf",
        "7662bbf9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6d954829",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 5,
      "parent_ids": [
        "25a62caf",
        "7662bbf9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9ce0e231",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 5,
      "parent_ids": [
        "7662bbf9",
        "25a62caf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "531731d5",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.79,
      "generation": 5,
      "parent_ids": [
        "7662bbf9",
        "25a62caf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a5c7f150",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 6,
      "parent_ids": [
        "161ebcde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d7634f2a",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.45,
      "temperature": 0.97,
      "generation": 6,
      "parent_ids": [
        "ce0b927c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "89e59323",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 6,
      "parent_ids": [
        "ce0b927c",
        "161ebcde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "275c294b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 6,
      "parent_ids": [
        "ce0b927c",
        "161ebcde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fe3e2780",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 6,
      "parent_ids": [
        "c1f6c2a1",
        "ce0b927c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4a552fa0",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 6,
      "parent_ids": [
        "161ebcde",
        "c1f6c2a1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9b0eaa43",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 3,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 6,
      "parent_ids": [
        "c1f6c2a1",
        "161ebcde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d195a53f",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.52,
      "temperature": 1.16,
      "generation": 6,
      "parent_ids": [
        "161ebcde",
        "ce0b927c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f171a613",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.34,
      "temperature": 0.79,
      "generation": 6,
      "parent_ids": [
        "c1f6c2a1",
        "161ebcde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "993f6409",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.3,
      "temperature": 0.79,
      "generation": 6,
      "parent_ids": [
        "c1f6c2a1",
        "161ebcde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dd4db3e0",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 7,
      "parent_ids": [
        "275c294b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "607a3e48",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 7,
      "parent_ids": [
        "a5c7f150"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9746530c",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.9,
      "generation": 7,
      "parent_ids": [
        "275c294b",
        "4a552fa0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2530a3eb",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.4,
      "temperature": 0.95,
      "generation": 7,
      "parent_ids": [
        "4a552fa0",
        "275c294b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "73d7078e",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.81,
      "generation": 7,
      "parent_ids": [
        "275c294b",
        "a5c7f150"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "96690487",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 7,
      "parent_ids": [
        "4a552fa0",
        "275c294b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d0ecc28a",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.3,
      "temperature": 0.97,
      "generation": 7,
      "parent_ids": [
        "a5c7f150",
        "4a552fa0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2cfe1c7f",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.97,
      "generation": 7,
      "parent_ids": [
        "a5c7f150",
        "4a552fa0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a66c07ca",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.37,
      "temperature": 0.82,
      "generation": 7,
      "parent_ids": [
        "275c294b",
        "a5c7f150"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "155365af",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.25,
      "temperature": 1.06,
      "generation": 7,
      "parent_ids": [
        "a5c7f150",
        "275c294b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "40d0a76f",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.9,
      "generation": 8,
      "parent_ids": [
        "9746530c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d959892d",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 8,
      "parent_ids": [
        "96690487"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c6d2e15b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.4,
      "temperature": 0.95,
      "generation": 8,
      "parent_ids": [
        "9746530c",
        "2530a3eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7ea31d2c",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.4,
      "temperature": 0.83,
      "generation": 8,
      "parent_ids": [
        "2530a3eb",
        "96690487"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "774d5f7f",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.75,
      "generation": 8,
      "parent_ids": [
        "96690487",
        "2530a3eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "132171e8",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.1,
      "temperature": 0.9,
      "generation": 8,
      "parent_ids": [
        "9746530c",
        "2530a3eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9e5f4d1b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 8,
      "parent_ids": [
        "96690487",
        "2530a3eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8bc0973a",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.21,
      "temperature": 0.9,
      "generation": 8,
      "parent_ids": [
        "9746530c",
        "96690487"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "484ac2cd",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 8,
      "parent_ids": [
        "96690487",
        "2530a3eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9e76fc83",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.25,
      "temperature": 0.77,
      "generation": 8,
      "parent_ids": [
        "9746530c",
        "2530a3eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4352fad2",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 9,
      "parent_ids": [
        "9e5f4d1b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b2787617",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.4,
      "temperature": 0.95,
      "generation": 9,
      "parent_ids": [
        "c6d2e15b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d810e03a",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 9,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.21,
      "temperature": 0.83,
      "generation": 9,
      "parent_ids": [
        "9e5f4d1b",
        "8bc0973a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0e142b89",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.21,
      "temperature": 0.9,
      "generation": 9,
      "parent_ids": [
        "8bc0973a",
        "c6d2e15b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e3845f9b",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.39,
      "temperature": 0.95,
      "generation": 9,
      "parent_ids": [
        "c6d2e15b",
        "9e5f4d1b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e876d3e8",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.4,
      "temperature": 0.9,
      "generation": 9,
      "parent_ids": [
        "c6d2e15b",
        "8bc0973a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fafbe360",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.21,
      "temperature": 0.88,
      "generation": 9,
      "parent_ids": [
        "8bc0973a",
        "9e5f4d1b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d2b81895",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.4,
      "temperature": 0.95,
      "generation": 9,
      "parent_ids": [
        "8bc0973a",
        "c6d2e15b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "40186367",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 9,
      "parent_ids": [
        "9e5f4d1b",
        "c6d2e15b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "608d12f1",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.21,
      "temperature": 0.95,
      "generation": 9,
      "parent_ids": [
        "c6d2e15b",
        "8bc0973a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bdeefcd7",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.4,
      "temperature": 0.95,
      "generation": 10,
      "parent_ids": [
        "d2b81895"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2f937dcb",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 10,
      "parent_ids": [
        "40186367"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5e3fddaf",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 10,
      "parent_ids": [
        "fafbe360",
        "40186367"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "90469096",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.39,
      "temperature": 0.88,
      "generation": 10,
      "parent_ids": [
        "fafbe360",
        "40186367"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9fecd29a",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 10,
      "parent_ids": [
        "40186367",
        "fafbe360"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1036ed10",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.4,
      "temperature": 0.88,
      "generation": 10,
      "parent_ids": [
        "d2b81895",
        "fafbe360"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ab2a49ba",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.21,
      "temperature": 0.93,
      "generation": 10,
      "parent_ids": [
        "d2b81895",
        "fafbe360"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1b22b202",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.33,
      "temperature": 1.14,
      "generation": 10,
      "parent_ids": [
        "40186367",
        "d2b81895"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a6bdb55e",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.26,
      "temperature": 0.98,
      "generation": 10,
      "parent_ids": [
        "40186367",
        "d2b81895"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c7b75f31",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.21,
      "temperature": 0.95,
      "generation": 10,
      "parent_ids": [
        "fafbe360",
        "d2b81895"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "18dce262",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 11,
      "parent_ids": [
        "5e3fddaf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e255b13f",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 11,
      "parent_ids": [
        "9fecd29a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d914e3e4",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 11,
      "parent_ids": [
        "9fecd29a",
        "5e3fddaf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3ebe8b4e",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.32,
      "temperature": 0.83,
      "generation": 11,
      "parent_ids": [
        "1036ed10",
        "5e3fddaf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "961fb172",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.46,
      "temperature": 0.88,
      "generation": 11,
      "parent_ids": [
        "5e3fddaf",
        "1036ed10"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "426db6f8",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.36,
      "temperature": 0.76,
      "generation": 11,
      "parent_ids": [
        "1036ed10",
        "9fecd29a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0d86d225",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.4,
      "temperature": 0.9,
      "generation": 11,
      "parent_ids": [
        "5e3fddaf",
        "1036ed10"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "587d4fa5",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.4,
      "temperature": 0.83,
      "generation": 11,
      "parent_ids": [
        "1036ed10",
        "9fecd29a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "aae71c24",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.33,
      "temperature": 0.78,
      "generation": 11,
      "parent_ids": [
        "9fecd29a",
        "5e3fddaf"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e799cfd3",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.33,
      "temperature": 0.74,
      "generation": 11,
      "parent_ids": [
        "5e3fddaf",
        "1036ed10"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "085bd51e",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.4,
      "temperature": 0.83,
      "generation": 12,
      "parent_ids": [
        "587d4fa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "42a4ca05",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.36,
      "temperature": 0.76,
      "generation": 12,
      "parent_ids": [
        "426db6f8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a943904a",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.4,
      "temperature": 0.99,
      "generation": 12,
      "parent_ids": [
        "18dce262",
        "587d4fa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9b9e59f2",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.49,
      "temperature": 0.83,
      "generation": 12,
      "parent_ids": [
        "587d4fa5",
        "426db6f8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0e48a956",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.36,
      "temperature": 0.76,
      "generation": 12,
      "parent_ids": [
        "426db6f8",
        "18dce262"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "00192755",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.4,
      "temperature": 0.83,
      "generation": 12,
      "parent_ids": [
        "18dce262",
        "587d4fa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eb11a7e6",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.43,
      "temperature": 0.76,
      "generation": 12,
      "parent_ids": [
        "426db6f8",
        "587d4fa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9203c039",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.33,
      "temperature": 0.83,
      "generation": 12,
      "parent_ids": [
        "18dce262",
        "587d4fa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ba8d29a7",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.36,
      "temperature": 0.76,
      "generation": 12,
      "parent_ids": [
        "426db6f8",
        "587d4fa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8bc6721b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 12,
      "parent_ids": [
        "587d4fa5",
        "18dce262"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2c7cd4f4",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.36,
      "temperature": 0.76,
      "generation": 13,
      "parent_ids": [
        "ba8d29a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "634015fc",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 13,
      "parent_ids": [
        "8bc6721b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "12d2a3aa",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.25,
      "temperature": 0.76,
      "generation": 13,
      "parent_ids": [
        "8bc6721b",
        "0e48a956"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dc4751ca",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 9,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.22,
      "temperature": 0.83,
      "generation": 13,
      "parent_ids": [
        "ba8d29a7",
        "8bc6721b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4a197662",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.36,
      "temperature": 0.83,
      "generation": 13,
      "parent_ids": [
        "ba8d29a7",
        "8bc6721b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2ff6561a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.25,
      "temperature": 0.76,
      "generation": 13,
      "parent_ids": [
        "8bc6721b",
        "ba8d29a7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2828ab6a",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.76,
      "generation": 13,
      "parent_ids": [
        "ba8d29a7",
        "8bc6721b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5c242739",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.36,
      "temperature": 0.76,
      "generation": 13,
      "parent_ids": [
        "0e48a956",
        "8bc6721b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "da4cd12b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.36,
      "temperature": 0.76,
      "generation": 13,
      "parent_ids": [
        "ba8d29a7",
        "0e48a956"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "66d50722",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 13,
      "parent_ids": [
        "8bc6721b",
        "0e48a956"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cd3951b2",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.25,
      "temperature": 0.76,
      "generation": 14,
      "parent_ids": [
        "12d2a3aa"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "81ecd1bc",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 14,
      "parent_ids": [
        "66d50722"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eb68515b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 14,
      "parent_ids": [
        "66d50722",
        "634015fc"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "932f53b2",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.25,
      "temperature": 0.69,
      "generation": 14,
      "parent_ids": [
        "12d2a3aa",
        "66d50722"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "71fdecfd",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.96,
      "generation": 14,
      "parent_ids": [
        "12d2a3aa",
        "634015fc"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0c84391a",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.28,
      "temperature": 0.76,
      "generation": 14,
      "parent_ids": [
        "66d50722",
        "12d2a3aa"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "aa6e5a82",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.25,
      "temperature": 0.76,
      "generation": 14,
      "parent_ids": [
        "12d2a3aa",
        "66d50722"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6f854799",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 14,
      "parent_ids": [
        "66d50722",
        "634015fc"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ccfc4433",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.25,
      "temperature": 0.94,
      "generation": 14,
      "parent_ids": [
        "66d50722",
        "634015fc"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5d54c48e",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.25,
      "temperature": 0.83,
      "generation": 14,
      "parent_ids": [
        "66d50722",
        "634015fc"
      ],
      "fitness_history": []
    }
  ],
  "all_results": [
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9785
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "d7e2e580",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.75,
      "fitness": 0.83
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.75,
      "fitness": 0.44999999999999996
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9999,
      "fitness": 0.83994
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.75,
      "fitness": 0.44999999999999996
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "0620e38d",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "r11",
      "predicted_confidence": 0.97,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.05910000000000004,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9991,
      "fitness": 0.91946
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "b03e666b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8578399999999999
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9023399999999999
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4\u200b",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.83994
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 0,
      "genome_id": "ab22b67c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.90224
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.11639999999999984,
      "fitness": 0.0698399999999999
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "b45bc97d",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975,
      "fitness": 0.4785
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 0,
      "genome_id": "fc4f9414",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog\u2019s brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "10\u202f000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "9",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 0,
      "genome_id": "de60ae4f",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog\u2019s brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8590399999999999
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.8159400000000001
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "bcdcc0f3",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5\u202fcents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "6bba167d",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "t12",
      "predicted_confidence": 0.93,
      "predicted_answer": "Bananas (Musa spp.) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.1350999999999999,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "e04",
      "predicted_confidence": 0.96,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9984,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "b3784113",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.83976
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana/plantain is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 1,
      "genome_id": "7a3a0364",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "t12",
      "predicted_confidence": 0.92,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.15359999999999996,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "t05",
      "predicted_confidence": 0.98,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9790399999999999
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks (you need to pull out three socks to guarantee a matching pair)",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "e04",
      "predicted_confidence": 0.92,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 1,
      "genome_id": "286751b3",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas are the most produced fruit in the world when measured by total weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "e04",
      "predicted_confidence": 0.97,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "8a7d1975",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "e09",
      "predicted_confidence": 0.9,
      "predicted_answer": "12.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "t05",
      "predicted_confidence": 0.97,
      "predicted_answer": "silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "t06",
      "predicted_confidence": 0.93,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9951,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "4d12ac3a",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8390399999999999
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "t05",
      "predicted_confidence": 0.97,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.80504
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4.2 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 1,
      "genome_id": "998910c6",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles (about 452 km\u00b2)",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks (because with two colors, drawing three socks guarantees at least two of the same color)",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "Approximately 10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "023484ab",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.83514
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "t12",
      "predicted_confidence": 0.87,
      "predicted_answer": "Banana is the fruit with the highest global production by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.24309999999999998,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.7711399999999999
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "t14",
      "predicted_confidence": 0.98,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "e04",
      "predicted_confidence": 0.92,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 1,
      "genome_id": "db714276",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.91904
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "t06",
      "predicted_confidence": 0.97,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks (pull two socks; if they are different colors, the third sock will match one of them)",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 1,
      "genome_id": "8bbaa5d3",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.83976
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including banana/plantain species) are the most produced fruit worldwide by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 1,
      "genome_id": "26efc443",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.83976
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "They have dark, usually black skin underneath the fur",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "7 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks are needed",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters (nearest 500)",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 1,
      "genome_id": "2b098c55",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "r07",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.78504
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "t13",
      "predicted_confidence": 0.92,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9856,
      "fitness": 0.85136
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "400fc8ba",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon (Si)",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "r14",
      "predicted_confidence": 0.98,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 2,
      "genome_id": "67bce988",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "e06",
      "predicted_confidence": 0.78,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.39159999999999995,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "t13",
      "predicted_confidence": 0.97,
      "predicted_answer": "The human brain contains more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 2,
      "genome_id": "9788276c",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "7,000 satellites orbiting Earth (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9185
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "r04",
      "predicted_confidence": 0.96,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9984,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area, covering about 459\u202fkm\u00b2",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "Approximately 8 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "3c80aadd",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "Approximately 6,000 satellites (nearest 1,000) are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.91904
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8590399999999999
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "163e4b1b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "100 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles (approximately 451 km\u00b2)",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 2,
      "genome_id": "14b09d97",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7399,
      "fitness": 0.8039400000000001
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "t13",
      "predicted_confidence": 0.97,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 2,
      "genome_id": "550c35af",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "t13",
      "predicted_confidence": 0.97,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 2,
      "genome_id": "0b00db3d",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "t05",
      "predicted_confidence": 0.92,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "e02",
      "predicted_confidence": 0.97,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "8ae543b9",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites orbiting Earth (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.33864000000000005
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9185
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles (the archipelago off the coast of Eastern Africa) is the smallest country on the African continent by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "51a6ca86",
      "task_id": "r10",
      "predicted_confidence": 0.98,
      "predicted_answer": "3 socks (you must pull out at least three socks to guarantee a matching pair)",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "t04",
      "predicted_confidence": 0.98,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in 2nd place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "8a4cf810",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "e04",
      "predicted_confidence": 0.97,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You would be in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "85540ad1",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "adfa2661",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "t03",
      "predicted_confidence": 0.98,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "900",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "t13",
      "predicted_confidence": 0.92,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "0.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 3,
      "genome_id": "dbd0449f",
      "task_id": "t14",
      "predicted_confidence": 0.98,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has significantly more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "2nd place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "97c3628e",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog\u2019s brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You\u2019re in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "4f7308a2",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (polar bears have black skin under their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "52a2c69c",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "r10",
      "predicted_confidence": 0.98,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "15.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "c3e1daf4",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.82544
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "800,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "484b8f67",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "e04",
      "predicted_confidence": 0.97,
      "predicted_answer": "11,000\u00a0meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 3,
      "genome_id": "beab94d7",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "30,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "1fde2aa5",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.7367400000000001
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "70",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "150,000 golf balls (nearest 10,000)",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 4,
      "genome_id": "b8adb714",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black (polar bears have black skin beneath their fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "7662bbf9",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "3000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "560",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 4,
      "genome_id": "25a62caf",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "e07",
      "predicted_confidence": 0.75,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons\u2014about 100\u202fbillion\u2014than a dog's brain, which has roughly 530\u202fmillion neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.72936
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 4,
      "genome_id": "d2744711",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas\u00a0(plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "a31ecc45",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4,200 miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the fruit produced in the world in the greatest quantity by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.834
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "12,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 4,
      "genome_id": "dd8005af",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "8 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "t13",
      "predicted_confidence": 0.98,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "28,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "7b713060",
      "task_id": "r05",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles (approximately 463\u202fkm\u00b2) is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The banana (mostly cultivated from the banana plant, Cavendish variety) is the most produced fruit worldwide by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "e02",
      "predicted_confidence": 0.98,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8298599999999999
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "3000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks   (pull 3 socks to guarantee at least one matching pair)",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "r01",
      "predicted_confidence": 0.98,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "59d6a684",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "7 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 7,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 4,
      "genome_id": "b3ee0fa1",
      "task_id": "r05",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "8",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000 mdop",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "4000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.86266
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 5,
      "genome_id": "dc4693f1",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "4b45b7fa",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 6,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the dessert banana/plantain) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "161ebcde",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "e04",
      "predicted_confidence": 0.94,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9964,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the Cavendish banana is the world\u2019s most produced fruit by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 5,
      "genome_id": "67f12066",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "r09",
      "predicted_confidence": 0.97,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8385
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the banana/plantain is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 5,
      "genome_id": "c1f6c2a1",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.77816
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.93296
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana \u540d\u7121\u3057\u3055\u3093",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 5,
      "genome_id": "31bb0948",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks (you must pick 3 to guarantee a matching pair)",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "t14",
      "predicted_confidence": 0.97,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "ce0b927c",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (the banana/plantain is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 5,
      "genome_id": "6d954829",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks are sufficient to guarantee a matching pair",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "r09",
      "predicted_confidence": 0.98,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "9ce0e231",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "t06",
      "predicted_confidence": 0.97,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "r09",
      "predicted_confidence": 0.98,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "531731d5",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "e10",
      "predicted_confidence": 0.72,
      "predicted_answer": "10\u202f000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.48160000000000003,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "a5c7f150",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "r07",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "r04",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d7634f2a",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "89e59323",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "275c294b",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites orbiting Earth (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 6,
      "genome_id": "fe3e2780",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 6,
      "genome_id": "4a552fa0",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon (Si)",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "12,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "t04",
      "predicted_confidence": 0.98,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "9b0eaa43",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "e10",
      "predicted_confidence": 0.78,
      "predicted_answer": "Approximately 8,000 satellites orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.39159999999999995,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "t03",
      "predicted_confidence": 0.98,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11000 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "d195a53f",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.76506
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "0.7",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 6,
      "genome_id": "f171a613",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "t08",
      "predicted_confidence": 0.98,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "t04",
      "predicted_confidence": 0.98,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 6,
      "genome_id": "993f6409",
      "task_id": "r11",
      "predicted_confidence": 0.98,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "The human brain has about 86\u202fbillion neurons, whereas a dog\u2019s brain has only roughly 530\u202fmillion, so humans have far more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The banana is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "2,800 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "dd4db3e0",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons \u2013 on the order of 86 billion \u2013 whereas a dog's brain contains roughly 530 million to 700 million neurons, depending on the breed",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Polar bears have black skin",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon (Si)",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "607a3e48",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks are needed to guarantee a matching pair",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (polar bears have black skin underneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "9746530c",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "e02",
      "predicted_confidence": 0.98,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8298599999999999
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "e10",
      "predicted_confidence": 0.9,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 7,
      "genome_id": "2530a3eb",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons \u2013 roughly 86\u202fbillion \u2013 compared to a dog\u2019s brain, which has around 500\u202fmillion to 600\u202fmillion neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000 satellites (rounded to the nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "73d7078e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "t13",
      "predicted_confidence": 0.98,
      "predicted_answer": "A human brain has far more neurons\u2014about 86\u202fbillion\u2014than a dog's brain, which has roughly 530\u202fmillion",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the banana fruit is the world\u2019s most produced fruit by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "96690487",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "7\u202f000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "d0ecc28a",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "t13",
      "predicted_confidence": 0.98,
      "predicted_answer": "Humans, with about 86\u202fbillion neurons, have far more neurons in the brain than dogs, which have roughly 530\u202fmillion neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "e02",
      "predicted_confidence": 0.98,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8298599999999999
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "t12",
      "predicted_confidence": 0.97,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.05910000000000004,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "11000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "2cfe1c7f",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.834
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.75114
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 7,
      "genome_id": "a66c07ca",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.9295399999999999
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "You must pull out 3 socks to guarantee that at least two of them are the same color",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.82824
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black (polar bears have black skin underneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "t12",
      "predicted_confidence": 0.92,
      "predicted_answer": "banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.15359999999999996,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "26,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 7,
      "genome_id": "155365af",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons\u2014about 86\u202fbillion versus roughly 2\u20133\u202fbillion in a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "40d0a76f",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "The human brain contains far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "d959892d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "c6d2e15b",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "8 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "7ea31d2c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "The letter **Q** is the only letter that does not appear in any U.S. state name",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 8,
      "genome_id": "774d5f7f",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "e10",
      "predicted_confidence": 0.88,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.22560000000000002,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "t05",
      "predicted_confidence": 0.98,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles, with a land area of about 459\u202fkm\u00b2, is the smallest country in Africa",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 8,
      "genome_id": "132171e8",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4.2 millions of miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5\u202fcents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon (Si)",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "r14",
      "predicted_confidence": 0.98,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "9e5f4d1b",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "Approximately 10,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.7884,
      "fitness": 0.83304
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks\u200e",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 8,
      "genome_id": "8bc0973a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "12,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "484ac2cd",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 10,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "S\u00e3o Tom\u00e9 and Pr\u00edncipe",
      "ground_truth": "Seychelles",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "50 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks \u2013 you may pick one black and one white, but the third sock will certainly match one of them",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "9e76fc83",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "t06",
      "predicted_confidence": 0.97,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "e02",
      "predicted_confidence": 0.98,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.82266
    },
    {
      "generation": 9,
      "genome_id": "4352fad2",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons \u2013 roughly 86\u202fbillion compared to a typical dog\u2019s about 530\u202fmillion",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 9,
      "genome_id": "b2787617",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50 piano tuners in Chicago (nearest 50)",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area, covering about 451\u202fkm\u00b2",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 9,
      "genome_id": "d810e03a",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 9,
      "genome_id": "0e142b89",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain contains far more neurons than a dog's \u2013 roughly 86\u202fbillion to 100\u202fbillion neurons versus a dog's brain, which has on the order of 500\u2011600\u202fmillion neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 9,
      "genome_id": "e3845f9b",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "r14",
      "predicted_confidence": 0.98,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "t13",
      "predicted_confidence": 0.98,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 9,
      "genome_id": "e876d3e8",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "t09",
      "predicted_confidence": 0.97,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.05910000000000004,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "e08",
      "predicted_confidence": 0.8,
      "predicted_answer": "650,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4\u00a0million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 9,
      "genome_id": "fafbe360",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain contains far more neurons than a dog's brain (roughly 86\u202fbillion versus a few hundred million)",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 9,
      "genome_id": "d2b81895",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 9,
      "genome_id": "40186367",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (dark brown) skin",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00a0\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 9,
      "genome_id": "608d12f1",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "9 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks must be pulled to guarantee a matching pair",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 10,
      "genome_id": "bdeefcd7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "t04",
      "predicted_confidence": 0.98,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "t05",
      "predicted_confidence": 0.98,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 10,
      "genome_id": "2f937dcb",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.816
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 10,
      "genome_id": "5e3fddaf",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles (approximately 452 square kilometers)",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8298599999999999
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon (Si)",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "90469096",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4,000,000 miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.7367400000000001
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon (Si) is the second most abundant element in Earth's crust by mass",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 10,
      "genome_id": "9fecd29a",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has far more neurons than a dog\u2019s brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "The Treaty of Westphalia was signed in 1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "e04",
      "predicted_confidence": 0.97,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks (two of the same color will be guaranteed)",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 10,
      "genome_id": "1036ed10",
      "task_id": "t13",
      "predicted_confidence": 0.98,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area, covering about 459 square kilometers",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "t05",
      "predicted_confidence": 0.98,
      "predicted_answer": "Silicon\u00a0(Si)",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "ab2a49ba",
      "task_id": "t13",
      "predicted_confidence": 0.98,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4000",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8298599999999999
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.72186
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.51786
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 10,
      "genome_id": "1b22b202",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "a6bdb55e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "t04",
      "predicted_confidence": 0.98,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "Approximately 11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 10,
      "genome_id": "c7b75f31",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog\u2019s brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black (polar bears have black skin underneath their white fur)",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "18dce262",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "S\u00e3o Tom\u00e9 and Pr\u00edncipe is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Human brains contain more neurons than dog brains",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 11,
      "genome_id": "e255b13f",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u00a0meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "t06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "80,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "d914e3e4",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u00a0meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "50",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "r05",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "3ebe8b4e",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "t06",
      "predicted_confidence": 0.99,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.87834
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "e06",
      "predicted_confidence": 0.8,
      "predicted_answer": "100",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Human brains have far more neurons than dog brains",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "961fb172",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "r05",
      "predicted_confidence": 0.97,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8298599999999999
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.77816
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 11,
      "genome_id": "426db6f8",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "t06",
      "predicted_confidence": 0.98,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "0d86d225",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "t06",
      "predicted_confidence": 1.0,
      "predicted_answer": "Seychelles\uff08approximately 459\u202fkm\u00b2, the smallest country in Africa by land area\uff09",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "t14",
      "predicted_confidence": 0.99,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.87416
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "587d4fa5",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "You are in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "aae71c24",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles is the smallest country in Africa by land area",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "100",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 11,
      "genome_id": "e799cfd3",
      "task_id": "r11",
      "predicted_confidence": 0.99,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "r01",
      "predicted_confidence": 0.98,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks are needed to guarantee that you have at least one matching pair",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "085bd51e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana fruit)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks\u200b",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "r14",
      "predicted_confidence": 0.98,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 12,
      "genome_id": "42a4ca05",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 6,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "a943904a",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.8822399999999999
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The banana (Cavendish and related varieties) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 12,
      "genome_id": "9b9e59f2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "0e48a956",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "00192755",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.75816
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "r10",
      "predicted_confidence": 0.98,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.43776
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 12,
      "genome_id": "eb11a7e6",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "80",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4,000,000 miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 12,
      "genome_id": "9203c039",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "t05",
      "predicted_confidence": 0.98,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "r14",
      "predicted_confidence": 0.98,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 12,
      "genome_id": "ba8d29a7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (the banana fruit)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "Approximately 10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "r14",
      "predicted_confidence": 0.99,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "e07",
      "predicted_confidence": 0.95,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 12,
      "genome_id": "8bc6721b",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "The human brain has many more neurons than a dog\u2019s brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "2c7cd4f4",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (the banana fruit) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "r05",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "20,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.46176
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 13,
      "genome_id": "634015fc",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) \u2013 they are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "12d2a3aa",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (the banana fruit is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "e04",
      "predicted_confidence": 0.93,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9951,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "r04",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 13,
      "genome_id": "dc4751ca",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has many more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "4a197662",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "t13",
      "predicted_confidence": 0.99,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "r05",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "2ff6561a",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "2,900,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the most produced fruit worldwide by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "t13",
      "predicted_confidence": 0.93,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9951,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.82266
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 13,
      "genome_id": "2828ab6a",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has many more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.816
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "r07",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "5c242739",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "200.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain\u2014humans have on the order of 80\u2013100\u202fbillion neurons, whereas dogs have roughly 500\u202fmillion to a few billion",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.82266
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "t04",
      "predicted_confidence": 0.99,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 13,
      "genome_id": "da4cd12b",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "600,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (Musa species) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "t08",
      "predicted_confidence": 0.99,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "e04",
      "predicted_confidence": 0.92,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 13,
      "genome_id": "66d50722",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the banana is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 14,
      "genome_id": "cd3951b2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "r01",
      "predicted_confidence": 0.98,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.82824
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 14,
      "genome_id": "81ecd1bc",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7264999999999999
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 14,
      "genome_id": "eb68515b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (the banana/plantain fruit is the most produced fruit worldwide by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 14,
      "genome_id": "932f53b2",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "r04",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "22,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.88266
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 14,
      "genome_id": "71fdecfd",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit worldwide by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "r01",
      "predicted_confidence": 0.97,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 0.9711,
      "fitness": 0.88266
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 14,
      "genome_id": "0c84391a",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "50 piano tuners in Chicago",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.82824
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 14,
      "genome_id": "aa6e5a82",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana fruit is the most produced fruit worldwide by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 14,
      "genome_id": "6f854799",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u202f\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8246399999999999
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "50,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 14,
      "genome_id": "ccfc4433",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.5265
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "r10",
      "predicted_confidence": 0.99,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "t08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Q",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "t05",
      "predicted_confidence": 0.99,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 14,
      "genome_id": "5d54c48e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    }
  ],
  "test_results": {
    "n_tasks": 9,
    "n_agents": 10,
    "avg_raw_calibration": 0.8501433333333334,
    "avg_prediction_accuracy": 0.8741344444444444,
    "avg_task_accuracy": 0.7222222222222222,
    "best_fitness": 0.8467711111111111,
    "avg_fitness": 0.7422584444444444
  }
}