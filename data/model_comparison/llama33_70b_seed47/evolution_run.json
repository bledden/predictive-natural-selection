{
  "model": "meta-llama/Llama-3.3-70B-Instruct",
  "slug": "llama33_70b",
  "seed": 47,
  "elapsed_seconds": 49.267117977142334,
  "generation_stats": [
    {
      "generation": 0,
      "population_size": 10,
      "avg_fitness": 0.6280284,
      "best_fitness": 0.64372,
      "worst_fitness": 0.59468,
      "avg_raw_calibration": 0.71785,
      "avg_prediction_accuracy": 0.7200473333333333,
      "avg_task_accuracy": 0.6,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 3.3402740955352783
    },
    {
      "generation": 1,
      "population_size": 10,
      "avg_fitness": 0.8320758666666667,
      "best_fitness": 0.8442000000000001,
      "worst_fitness": 0.7939066666666666,
      "avg_raw_calibration": 0.9259333333333333,
      "avg_prediction_accuracy": 0.9243486666666666,
      "avg_task_accuracy": 0.86,
      "dominant_reasoning": "debate-self",
      "dominant_memory": "relevance",
      "elapsed_seconds": 3.1113178730010986
    },
    {
      "generation": 2,
      "population_size": 10,
      "avg_fitness": 0.8113954666666666,
      "best_fitness": 0.8180786666666666,
      "worst_fitness": 0.8052786666666666,
      "avg_raw_calibration": 0.9003333333333332,
      "avg_prediction_accuracy": 0.9012146666666666,
      "avg_task_accuracy": 0.8,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 3.072117805480957
    },
    {
      "generation": 3,
      "population_size": 10,
      "avg_fitness": 0.7317672,
      "best_fitness": 0.7373,
      "worst_fitness": 0.72146,
      "avg_raw_calibration": 0.8188166666666666,
      "avg_prediction_accuracy": 0.8262786666666667,
      "avg_task_accuracy": 0.7333333333333333,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 3.143723726272583
    },
    {
      "generation": 4,
      "population_size": 10,
      "avg_fitness": 0.740968,
      "best_fitness": 0.7501,
      "worst_fitness": 0.72824,
      "avg_raw_calibration": 0.8275666666666666,
      "avg_prediction_accuracy": 0.8349466666666667,
      "avg_task_accuracy": 0.7333333333333333,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 3.3527932167053223
    },
    {
      "generation": 5,
      "population_size": 10,
      "avg_fitness": 0.6523953333333333,
      "best_fitness": 0.6621333333333334,
      "worst_fitness": 0.6427733333333333,
      "avg_raw_calibration": 0.7377833333333332,
      "avg_prediction_accuracy": 0.7584366666666666,
      "avg_task_accuracy": 0.6,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 3.3452579975128174
    },
    {
      "generation": 6,
      "population_size": 10,
      "avg_fitness": 0.6641993333333333,
      "best_fitness": 0.6757133333333333,
      "worst_fitness": 0.6513733333333334,
      "avg_raw_calibration": 0.7690166666666666,
      "avg_prediction_accuracy": 0.7914433333333333,
      "avg_task_accuracy": 0.6,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.2286458015441895
    },
    {
      "generation": 7,
      "population_size": 10,
      "avg_fitness": 0.6982533333333334,
      "best_fitness": 0.7093133333333334,
      "worst_fitness": 0.6895333333333333,
      "avg_raw_calibration": 0.7874666666666666,
      "avg_prediction_accuracy": 0.8082,
      "avg_task_accuracy": 0.6666666666666666,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.042912006378174
    },
    {
      "generation": 8,
      "population_size": 10,
      "avg_fitness": 0.7100386666666666,
      "best_fitness": 0.7155666666666666,
      "worst_fitness": 0.7026066666666666,
      "avg_raw_calibration": 0.7864,
      "avg_prediction_accuracy": 0.8122866666666667,
      "avg_task_accuracy": 0.6666666666666666,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.1921207904815674
    },
    {
      "generation": 9,
      "population_size": 10,
      "avg_fitness": 0.6995199999999999,
      "best_fitness": 0.7077133333333332,
      "worst_fitness": 0.6696000000000001,
      "avg_raw_calibration": 0.76505,
      "avg_prediction_accuracy": 0.8011999999999999,
      "avg_task_accuracy": 0.66,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.097075939178467
    },
    {
      "generation": 10,
      "population_size": 10,
      "avg_fitness": 0.7551886666666666,
      "best_fitness": 0.7638666666666667,
      "worst_fitness": 0.7422066666666666,
      "avg_raw_calibration": 0.8603333333333332,
      "avg_prediction_accuracy": 0.8742033333333334,
      "avg_task_accuracy": 0.7333333333333333,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.0687060356140137
    },
    {
      "generation": 11,
      "population_size": 10,
      "avg_fitness": 0.633804,
      "best_fitness": 0.6419,
      "worst_fitness": 0.6234,
      "avg_raw_calibration": 0.7158333333333333,
      "avg_prediction_accuracy": 0.7763399999999999,
      "avg_task_accuracy": 0.5333333333333333,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.2087159156799316
    },
    {
      "generation": 12,
      "population_size": 10,
      "avg_fitness": 0.700314,
      "best_fitness": 0.7059,
      "worst_fitness": 0.6858000000000001,
      "avg_raw_calibration": 0.7719666666666666,
      "avg_prediction_accuracy": 0.8005233333333334,
      "avg_task_accuracy": 0.6666666666666666,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.058720827102661
    },
    {
      "generation": 13,
      "population_size": 10,
      "avg_fitness": 0.7163133333333334,
      "best_fitness": 0.7225333333333334,
      "worst_fitness": 0.7089333333333333,
      "avg_raw_calibration": 0.8180666666666666,
      "avg_prediction_accuracy": 0.8449666666666666,
      "avg_task_accuracy": 0.6666666666666666,
      "dominant_reasoning": "elimination",
      "dominant_memory": "recency",
      "elapsed_seconds": 3.0539321899414062
    },
    {
      "generation": 14,
      "population_size": 10,
      "avg_fitness": 0.7908826666666666,
      "best_fitness": 0.7966866666666667,
      "worst_fitness": 0.7779066666666666,
      "avg_raw_calibration": 0.8830166666666666,
      "avg_prediction_accuracy": 0.88036,
      "avg_task_accuracy": 0.8,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 2.847046136856079
    }
  ],
  "all_genomes": [
    {
      "genome_id": "63c01538",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": 0.02,
      "risk_tolerance": 0.58,
      "temperature": 0.94,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "703bc30a",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "ab57b63a",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.05,
      "risk_tolerance": 0.12,
      "temperature": 1.19,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "7ef0fb24",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.83,
      "temperature": 0.92,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "1089ac1a",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.23,
      "temperature": 0.8,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "e74fa1b3",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.02,
      "risk_tolerance": 0.54,
      "temperature": 0.67,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "bffd1c1b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.76,
      "temperature": 0.37,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "1c5d9d16",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.45,
      "temperature": 0.41,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "2c008add",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.88,
      "temperature": 1.12,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "cb4272bb",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.81,
      "temperature": 1.13,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "a5bd0b84",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.88,
      "temperature": 1.12,
      "generation": 1,
      "parent_ids": [
        "2c008add"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d254e72f",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.81,
      "temperature": 1.13,
      "generation": 1,
      "parent_ids": [
        "cb4272bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d097d380",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 1,
      "parent_ids": [
        "cb4272bb",
        "703bc30a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "94369b40",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.81,
      "temperature": 1.13,
      "generation": 1,
      "parent_ids": [
        "cb4272bb",
        "2c008add"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cb216b8d",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 11,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.93,
      "temperature": 1.23,
      "generation": 1,
      "parent_ids": [
        "2c008add",
        "703bc30a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c0628073",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.8,
      "temperature": 0.49,
      "generation": 1,
      "parent_ids": [
        "703bc30a",
        "2c008add"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "73f4e4af",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.81,
      "temperature": 1.13,
      "generation": 1,
      "parent_ids": [
        "703bc30a",
        "cb4272bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eaca0e17",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.81,
      "temperature": 1.13,
      "generation": 1,
      "parent_ids": [
        "cb4272bb",
        "2c008add"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a5437746",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.92,
      "temperature": 1.29,
      "generation": 1,
      "parent_ids": [
        "2c008add",
        "cb4272bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "90cf94b9",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.95,
      "temperature": 0.56,
      "generation": 1,
      "parent_ids": [
        "703bc30a",
        "2c008add"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b5b588da",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 2,
      "parent_ids": [
        "d097d380"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "982c76ee",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.8,
      "temperature": 0.49,
      "generation": 2,
      "parent_ids": [
        "c0628073"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d1d9d98e",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.81,
      "temperature": 0.56,
      "generation": 2,
      "parent_ids": [
        "94369b40",
        "d097d380"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a0915a7d",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.75,
      "temperature": 0.63,
      "generation": 2,
      "parent_ids": [
        "d097d380",
        "94369b40"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6be0d94e",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.81,
      "temperature": 0.49,
      "generation": 2,
      "parent_ids": [
        "94369b40",
        "c0628073"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a74cfb27",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.8,
      "temperature": 0.62,
      "generation": 2,
      "parent_ids": [
        "c0628073",
        "d097d380"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f40b75c9",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.8,
      "temperature": 1.13,
      "generation": 2,
      "parent_ids": [
        "c0628073",
        "94369b40"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f0cafc39",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 2,
      "parent_ids": [
        "94369b40",
        "d097d380"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "82651195",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.81,
      "temperature": 0.43,
      "generation": 2,
      "parent_ids": [
        "94369b40",
        "c0628073"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ed299348",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.82,
      "temperature": 0.47,
      "generation": 2,
      "parent_ids": [
        "d097d380",
        "c0628073"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "02bd05b0",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.82,
      "temperature": 0.47,
      "generation": 3,
      "parent_ids": [
        "ed299348"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "61d83da2",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 3,
      "parent_ids": [
        "b5b588da"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "50431530",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.69,
      "generation": 3,
      "parent_ids": [
        "d1d9d98e",
        "b5b588da"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bec68e49",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 3,
      "parent_ids": [
        "ed299348",
        "b5b588da"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b0db8616",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.81,
      "temperature": 0.56,
      "generation": 3,
      "parent_ids": [
        "d1d9d98e",
        "b5b588da"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "269174d6",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": 0.01,
      "risk_tolerance": 0.81,
      "temperature": 0.56,
      "generation": 3,
      "parent_ids": [
        "ed299348",
        "d1d9d98e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "60b95a23",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 3,
      "parent_ids": [
        "b5b588da",
        "d1d9d98e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8a2483f4",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 3,
      "parent_ids": [
        "d1d9d98e",
        "b5b588da"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4ce8e3e0",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.81,
      "temperature": 0.56,
      "generation": 3,
      "parent_ids": [
        "d1d9d98e",
        "b5b588da"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b45dfb31",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.85,
      "temperature": 0.47,
      "generation": 3,
      "parent_ids": [
        "b5b588da",
        "ed299348"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3b02d97d",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 4,
      "parent_ids": [
        "61d83da2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e7d47ca3",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.69,
      "generation": 4,
      "parent_ids": [
        "50431530"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "43c02aa5",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 9,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.85,
      "temperature": 0.47,
      "generation": 4,
      "parent_ids": [
        "50431530",
        "b45dfb31"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "057c958e",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 4,
      "parent_ids": [
        "50431530",
        "61d83da2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7b535585",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.85,
      "temperature": 0.56,
      "generation": 4,
      "parent_ids": [
        "b45dfb31",
        "61d83da2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b79a2674",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 9,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.85,
      "temperature": 0.69,
      "generation": 4,
      "parent_ids": [
        "b45dfb31",
        "50431530"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ffdf6a8d",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.82,
      "temperature": 0.86,
      "generation": 4,
      "parent_ids": [
        "61d83da2",
        "50431530"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b250d063",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.85,
      "temperature": 0.47,
      "generation": 4,
      "parent_ids": [
        "b45dfb31",
        "50431530"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ca6cf519",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.86,
      "temperature": 0.39,
      "generation": 4,
      "parent_ids": [
        "b45dfb31",
        "61d83da2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2ba22e89",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.8,
      "temperature": 0.56,
      "generation": 4,
      "parent_ids": [
        "61d83da2",
        "50431530"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a4819faf",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.8,
      "temperature": 0.56,
      "generation": 5,
      "parent_ids": [
        "2ba22e89"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2a39935d",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 5,
      "parent_ids": [
        "057c958e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "561d92c1",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.8,
      "temperature": 0.44,
      "generation": 5,
      "parent_ids": [
        "2ba22e89",
        "ca6cf519"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d9f2e16c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.9,
      "temperature": 0.38,
      "generation": 5,
      "parent_ids": [
        "ca6cf519",
        "057c958e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cae4c61c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.8,
      "temperature": 0.56,
      "generation": 5,
      "parent_ids": [
        "2ba22e89",
        "057c958e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7ebf9530",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.77,
      "temperature": 0.39,
      "generation": 5,
      "parent_ids": [
        "ca6cf519",
        "057c958e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4305e2eb",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.82,
      "temperature": 0.32,
      "generation": 5,
      "parent_ids": [
        "ca6cf519",
        "057c958e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dae1059b",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.93,
      "temperature": 0.56,
      "generation": 5,
      "parent_ids": [
        "057c958e",
        "2ba22e89"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c89cab91",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 5,
      "parent_ids": [
        "057c958e",
        "ca6cf519"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b1e35f9e",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.7,
      "temperature": 0.56,
      "generation": 5,
      "parent_ids": [
        "057c958e",
        "2ba22e89"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7f7c79d1",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 6,
      "parent_ids": [
        "2a39935d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a0b8bb84",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.9,
      "temperature": 0.38,
      "generation": 6,
      "parent_ids": [
        "d9f2e16c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b9f91fd9",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 6,
      "parent_ids": [
        "c89cab91",
        "2a39935d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b8bcf04c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 6,
      "parent_ids": [
        "2a39935d",
        "d9f2e16c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "56c59e28",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.9,
      "temperature": 0.56,
      "generation": 6,
      "parent_ids": [
        "c89cab91",
        "d9f2e16c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "659d1358",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 6,
      "parent_ids": [
        "2a39935d",
        "c89cab91"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4333b9b2",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "debate-self",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.01,
      "risk_tolerance": 1.0,
      "temperature": 0.38,
      "generation": 6,
      "parent_ids": [
        "d9f2e16c",
        "2a39935d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e0418ac3",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.9,
      "temperature": 0.38,
      "generation": 6,
      "parent_ids": [
        "c89cab91",
        "d9f2e16c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cff55418",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.81,
      "temperature": 0.56,
      "generation": 6,
      "parent_ids": [
        "2a39935d",
        "c89cab91"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ad64ae74",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.82,
      "temperature": 0.3,
      "generation": 6,
      "parent_ids": [
        "c89cab91",
        "d9f2e16c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "35bc891c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 7,
      "parent_ids": [
        "b8bcf04c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4a2fc67a",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 7,
      "parent_ids": [
        "7f7c79d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2b19c6d0",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.76,
      "temperature": 0.56,
      "generation": 7,
      "parent_ids": [
        "b8bcf04c",
        "cff55418"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8e82b3eb",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "analogical",
      "memory_window": 9,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 7,
      "parent_ids": [
        "7f7c79d1",
        "cff55418"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "be88e018",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.82,
      "temperature": 0.55,
      "generation": 7,
      "parent_ids": [
        "7f7c79d1",
        "b8bcf04c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "38abe6e6",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.81,
      "temperature": 0.56,
      "generation": 7,
      "parent_ids": [
        "b8bcf04c",
        "cff55418"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7add0558",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 7,
      "parent_ids": [
        "b8bcf04c",
        "cff55418"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ca40c2f0",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.78,
      "temperature": 0.56,
      "generation": 7,
      "parent_ids": [
        "7f7c79d1",
        "cff55418"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c582d6aa",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.49,
      "generation": 7,
      "parent_ids": [
        "b8bcf04c",
        "7f7c79d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "02a75c76",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.58,
      "generation": 7,
      "parent_ids": [
        "b8bcf04c",
        "7f7c79d1"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e10ebbf8",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 8,
      "parent_ids": [
        "7add0558"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "43d0686b",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 8,
      "parent_ids": [
        "35bc891c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a38d7b2b",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.79,
      "temperature": 0.59,
      "generation": 8,
      "parent_ids": [
        "7add0558",
        "02a75c76"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dccbf02d",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.58,
      "generation": 8,
      "parent_ids": [
        "35bc891c",
        "02a75c76"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "58b7d5e3",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.88,
      "temperature": 0.58,
      "generation": 8,
      "parent_ids": [
        "02a75c76",
        "35bc891c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6321655b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.7,
      "generation": 8,
      "parent_ids": [
        "35bc891c",
        "7add0558"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b9cf4996",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 13,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 8,
      "parent_ids": [
        "35bc891c",
        "7add0558"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2147740c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.82,
      "temperature": 0.58,
      "generation": 8,
      "parent_ids": [
        "02a75c76",
        "35bc891c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bd00364c",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "debate-self",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 8,
      "parent_ids": [
        "7add0558",
        "35bc891c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e6b70f12",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 8,
      "parent_ids": [
        "02a75c76",
        "7add0558"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0e02c8f6",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 9,
      "parent_ids": [
        "e10ebbf8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "24c16b9a",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.7,
      "generation": 9,
      "parent_ids": [
        "6321655b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "10b7ae74",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 9,
      "parent_ids": [
        "6321655b",
        "43d0686b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b052ddaa",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 9,
      "parent_ids": [
        "6321655b",
        "e10ebbf8"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b292ebeb",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.92,
      "temperature": 0.59,
      "generation": 9,
      "parent_ids": [
        "43d0686b",
        "6321655b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b8f017d3",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 9,
      "parent_ids": [
        "6321655b",
        "43d0686b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "694fb510",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.82,
      "temperature": 0.84,
      "generation": 9,
      "parent_ids": [
        "43d0686b",
        "6321655b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "091ff62f",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.86,
      "temperature": 0.76,
      "generation": 9,
      "parent_ids": [
        "e10ebbf8",
        "43d0686b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f0fb3123",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.7,
      "generation": 9,
      "parent_ids": [
        "6321655b",
        "43d0686b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6f8fefa2",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 9,
      "parent_ids": [
        "e10ebbf8",
        "43d0686b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "618af4a3",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 10,
      "parent_ids": [
        "6f8fefa2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6dd883bb",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 10,
      "parent_ids": [
        "b8f017d3"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "523f4530",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.7,
      "temperature": 0.44,
      "generation": 10,
      "parent_ids": [
        "b8f017d3",
        "6f8fefa2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "98fcb341",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.82,
      "temperature": 0.7,
      "generation": 10,
      "parent_ids": [
        "b8f017d3",
        "24c16b9a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f0caa046",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 10,
      "parent_ids": [
        "b8f017d3",
        "6f8fefa2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "697f0b11",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 13,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.82,
      "temperature": 0.53,
      "generation": 10,
      "parent_ids": [
        "24c16b9a",
        "b8f017d3"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "32da93dc",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.82,
      "temperature": 0.48,
      "generation": 10,
      "parent_ids": [
        "b8f017d3",
        "24c16b9a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6f3ac109",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.82,
      "temperature": 0.7,
      "generation": 10,
      "parent_ids": [
        "24c16b9a",
        "6f8fefa2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2a69c33b",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.61,
      "generation": 10,
      "parent_ids": [
        "6f8fefa2",
        "24c16b9a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2af3a560",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 10,
      "parent_ids": [
        "b8f017d3",
        "6f8fefa2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "91b3e51c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 11,
      "parent_ids": [
        "6dd883bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7447e611",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 11,
      "parent_ids": [
        "2af3a560"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "851dbc5f",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.82,
      "temperature": 0.7,
      "generation": 11,
      "parent_ids": [
        "98fcb341",
        "6dd883bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b683ee47",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.79,
      "temperature": 0.7,
      "generation": 11,
      "parent_ids": [
        "98fcb341",
        "2af3a560"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "34630672",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.82,
      "temperature": 0.59,
      "generation": 11,
      "parent_ids": [
        "98fcb341",
        "2af3a560"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b41a4772",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.73,
      "temperature": 0.7,
      "generation": 11,
      "parent_ids": [
        "2af3a560",
        "98fcb341"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1ef1775b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "relevance",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.84,
      "temperature": 0.6,
      "generation": 11,
      "parent_ids": [
        "98fcb341",
        "6dd883bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "71563e99",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.85,
      "temperature": 0.7,
      "generation": 11,
      "parent_ids": [
        "2af3a560",
        "98fcb341"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1a30445b",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.77,
      "temperature": 0.6,
      "generation": 11,
      "parent_ids": [
        "2af3a560",
        "6dd883bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7ba2245c",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 11,
      "parent_ids": [
        "2af3a560",
        "6dd883bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9dcd3d0b",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 12,
      "parent_ids": [
        "91b3e51c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0f4fe3a0",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 12,
      "parent_ids": [
        "7ba2245c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d439c130",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.73,
      "generation": 12,
      "parent_ids": [
        "7ba2245c",
        "7447e611"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bad24eb3",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.68,
      "temperature": 0.42,
      "generation": 12,
      "parent_ids": [
        "7447e611",
        "91b3e51c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "32f52a26",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.78,
      "generation": 12,
      "parent_ids": [
        "7447e611",
        "7ba2245c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "09353a83",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.67,
      "generation": 12,
      "parent_ids": [
        "91b3e51c",
        "7ba2245c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "64433b34",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 12,
      "parent_ids": [
        "7ba2245c",
        "7447e611"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d6228c6c",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 12,
      "parent_ids": [
        "91b3e51c",
        "7ba2245c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "24286263",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "first-principles",
      "memory_window": 11,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.93,
      "temperature": 0.59,
      "generation": 12,
      "parent_ids": [
        "7ba2245c",
        "7447e611"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fc74ee78",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.67,
      "generation": 12,
      "parent_ids": [
        "7ba2245c",
        "91b3e51c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5924c445",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 13,
      "parent_ids": [
        "9dcd3d0b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e2d2f3a0",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.78,
      "generation": 13,
      "parent_ids": [
        "32f52a26"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "226b4d5a",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 13,
      "parent_ids": [
        "32f52a26",
        "9dcd3d0b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6d01765d",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 13,
      "parent_ids": [
        "9dcd3d0b",
        "32f52a26"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d327e7c6",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 13,
      "parent_ids": [
        "9dcd3d0b",
        "09353a83"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0ffe8c9a",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 13,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.78,
      "generation": 13,
      "parent_ids": [
        "9dcd3d0b",
        "32f52a26"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e53d5013",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 13,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.67,
      "generation": 13,
      "parent_ids": [
        "09353a83",
        "9dcd3d0b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cd57727f",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.67,
      "generation": 13,
      "parent_ids": [
        "32f52a26",
        "09353a83"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e1479b2e",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.69,
      "temperature": 0.78,
      "generation": 13,
      "parent_ids": [
        "32f52a26",
        "09353a83"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3eb990d3",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.78,
      "generation": 13,
      "parent_ids": [
        "09353a83",
        "32f52a26"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a487bf60",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 14,
      "parent_ids": [
        "226b4d5a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "50128e4d",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.67,
      "generation": 14,
      "parent_ids": [
        "cd57727f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bd32e432",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.67,
      "generation": 14,
      "parent_ids": [
        "cd57727f",
        "226b4d5a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "310a6e26",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.13,
      "risk_tolerance": 0.82,
      "temperature": 0.71,
      "generation": 14,
      "parent_ids": [
        "226b4d5a",
        "cd57727f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "16868008",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.56,
      "generation": 14,
      "parent_ids": [
        "0ffe8c9a",
        "226b4d5a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a0574c10",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.6,
      "generation": 14,
      "parent_ids": [
        "0ffe8c9a",
        "226b4d5a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e193ee4d",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "elimination",
      "memory_window": 11,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.76,
      "temperature": 0.53,
      "generation": 14,
      "parent_ids": [
        "226b4d5a",
        "cd57727f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "134e2331",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.67,
      "generation": 14,
      "parent_ids": [
        "0ffe8c9a",
        "cd57727f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1f1554df",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "step-by-step",
      "memory_window": 12,
      "memory_weighting": "relevance",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.75,
      "temperature": 0.67,
      "generation": 14,
      "parent_ids": [
        "226b4d5a",
        "cd57727f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1b9779f6",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 13,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.82,
      "temperature": 0.78,
      "generation": 14,
      "parent_ids": [
        "0ffe8c9a",
        "226b4d5a"
      ],
      "fitness_history": []
    }
  ],
  "all_results": [
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6636,
      "fitness": 0.77816
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 0,
      "genome_id": "63c01538",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, you take their position",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "703bc30a",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9385000000000001
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9385000000000001
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.27749999999999986,
      "fitness": 0.1664999999999999
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9385000000000001
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.27749999999999986,
      "fitness": 0.1664999999999999
    },
    {
      "generation": 0,
      "genome_id": "ab57b63a",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,500",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "7ef0fb24",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 0,
      "genome_id": "1089ac1a",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 0,
      "genome_id": "e74fa1b3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, you take their position",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 0,
      "genome_id": "bffd1c1b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 0,
      "genome_id": "1c5d9d16",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 0,
      "genome_id": "2c008add",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 0,
      "genome_id": "cb4272bb",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8170599999999999
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 1,
      "genome_id": "a5bd0b84",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.81904
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 1,
      "genome_id": "d254e72f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9744,
      "fitness": 0.58464
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.81904
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 1,
      "genome_id": "d097d380",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.81904
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 1,
      "genome_id": "94369b40",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8170599999999999
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 1,
      "genome_id": "cb216b8d",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 1,
      "genome_id": "c0628073",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.81904
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 1,
      "genome_id": "73f4e4af",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.81976
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8570599999999999
    },
    {
      "generation": 1,
      "genome_id": "eaca0e17",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "47",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.81904
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 1,
      "genome_id": "a5437746",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8199400000000001
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 1,
      "genome_id": "90cf94b9",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 2,
      "genome_id": "b5b588da",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 2,
      "genome_id": "982c76ee",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 2,
      "genome_id": "d1d9d98e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 2,
      "genome_id": "a0915a7d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "6be0d94e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 2,
      "genome_id": "a74cfb27",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 2,
      "genome_id": "f40b75c9",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 2,
      "genome_id": "f0cafc39",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 2,
      "genome_id": "82651195",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8319,
      "fitness": 0.49914
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 2,
      "genome_id": "ed299348",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.5735399999999999
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8590399999999999
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "02bd05b0",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9744,
      "fitness": 0.58464
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 3,
      "genome_id": "61d83da2",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9744,
      "fitness": 0.58464
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 3,
      "genome_id": "50431530",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8570599999999999
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 3,
      "genome_id": "bec68e49",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 3,
      "genome_id": "b0db8616",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.5735399999999999
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "269174d6",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8713599999999999
    },
    {
      "generation": 3,
      "genome_id": "60b95a23",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 3,
      "genome_id": "8a2483f4",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 3,
      "genome_id": "4ce8e3e0",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9744,
      "fitness": 0.58464
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 3,
      "genome_id": "b45dfb31",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 4,
      "genome_id": "3b02d97d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 4,
      "genome_id": "e7d47ca3",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, you take their position",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 4,
      "genome_id": "43c02aa5",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 4,
      "genome_id": "057c958e",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 4,
      "genome_id": "7b535585",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "b79a2674",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 4,
      "genome_id": "ffdf6a8d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 4,
      "genome_id": "b250d063",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9446400000000001
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 4,
      "genome_id": "ca6cf519",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9744,
      "fitness": 0.58464
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 4,
      "genome_id": "2ba22e89",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 5,
      "genome_id": "a4819faf",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "2a39935d",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 5,
      "genome_id": "561d92c1",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9446400000000001
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.96464
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 5,
      "genome_id": "d9f2e16c",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9804,
      "fitness": 0.58824
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 5,
      "genome_id": "cae4c61c",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9446400000000001
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.96464
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 5,
      "genome_id": "7ebf9530",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.96464
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 5,
      "genome_id": "4305e2eb",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 5,
      "genome_id": "dae1059b",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9446400000000001
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.96464
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 5,
      "genome_id": "c89cab91",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9804,
      "fitness": 0.58824
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 5,
      "genome_id": "b1e35f9e",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 6,
      "genome_id": "7f7c79d1",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 6,
      "genome_id": "a0b8bb84",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 6,
      "genome_id": "b9f91fd9",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 6,
      "genome_id": "b8bcf04c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 6,
      "genome_id": "56c59e28",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8704,
      "fitness": 0.5222399999999999
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 6,
      "genome_id": "659d1358",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 6,
      "genome_id": "4333b9b2",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 6,
      "genome_id": "e0418ac3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 6,
      "genome_id": "cff55418",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 6,
      "genome_id": "ad64ae74",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.81514
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 7,
      "genome_id": "35bc891c",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "4a2fc67a",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.81976
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "2b19c6d0",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8178399999999999
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.96464
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 7,
      "genome_id": "8e82b3eb",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "be88e018",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "38abe6e6",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.86056
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.80986
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.5970599999999999
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 7,
      "genome_id": "7add0558",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.09750000000000003,
      "fitness": 0.05850000000000002
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8185
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9775,
      "fitness": 0.5865
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "ca40c2f0",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.81514
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.5423399999999999
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 7,
      "genome_id": "c582d6aa",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.81514
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 7,
      "genome_id": "02a75c76",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.5970599999999999
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "e10ebbf8",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "43d0686b",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "a38d7b2b",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "dccbf02d",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.88954
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "58b7d5e3",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "6321655b",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.5423399999999999
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "b9cf4996",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.5927399999999999
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.8695400000000001
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 8,
      "genome_id": "2147740c",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "bd00364c",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.87466
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.86056
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.89466
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 8,
      "genome_id": "e6b70f12",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.86056
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 9,
      "genome_id": "0e02c8f6",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 9,
      "genome_id": "24c16b9a",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.80504
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.8583399999999999
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7884,
      "fitness": 0.47303999999999996
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 9,
      "genome_id": "10b7ae74",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "47",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 9,
      "genome_id": "b052ddaa",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4958999999999999,
      "fitness": 0.2975399999999999
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.5423399999999999
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.2063399999999999
    },
    {
      "generation": 9,
      "genome_id": "b292ebeb",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 9,
      "genome_id": "b8f017d3",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 9,
      "genome_id": "694fb510",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.86056
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 9,
      "genome_id": "091ff62f",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 9,
      "genome_id": "f0fb3123",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 9,
      "genome_id": "6f8fefa2",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.81274
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 10,
      "genome_id": "618af4a3",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 10,
      "genome_id": "6dd883bb",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 10,
      "genome_id": "523f4530",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.75,
      "fitness": 0.83
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.8140000000000001
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3599999999999999,
      "fitness": 0.21599999999999991
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "98fcb341",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.81274
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 10,
      "genome_id": "f0caa046",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.8417600000000001
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.81616
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.87616
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4815999999999999,
      "fitness": 0.28895999999999994
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 10,
      "genome_id": "697f0b11",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.80504
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9324,
      "fitness": 0.5594399999999999
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9324,
      "fitness": 0.5594399999999999
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.82544
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 10,
      "genome_id": "32da93dc",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.8583399999999999
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.81274
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 10,
      "genome_id": "6f3ac109",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9271,
      "fitness": 0.55626
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.80986
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 10,
      "genome_id": "2a69c33b",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 10,
      "genome_id": "2af3a560",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "91b3e51c",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "7447e611",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.86266
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9936,
      "fitness": 0.59616
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.46176
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.89136
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.3225599999999999
    },
    {
      "generation": 11,
      "genome_id": "851dbc5f",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8976000000000001,
      "fitness": 0.89856
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, you take their position",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "b683ee47",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3599999999999999,
      "fitness": 0.21599999999999991
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "34630672",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.9060000000000001
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3599999999999999,
      "fitness": 0.21599999999999991
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.75,
      "fitness": 0.44999999999999996
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.8860000000000001
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "b41a4772",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.9060000000000001
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3599999999999999,
      "fitness": 0.21599999999999991
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.5459999999999999
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5099999999999999,
      "fitness": 0.30599999999999994
    },
    {
      "generation": 11,
      "genome_id": "1ef1775b",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.91,
      "fitness": 0.9060000000000001
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "71563e99",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8665
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "1a30445b",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8665
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "7ba2245c",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "9dcd3d0b",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 12,
      "genome_id": "0f4fe3a0",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 12,
      "genome_id": "d439c130",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8506400000000001
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.8583399999999999
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9324,
      "fitness": 0.5594399999999999
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.91064
    },
    {
      "generation": 12,
      "genome_id": "bad24eb3",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "32f52a26",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "09353a83",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "64433b34",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5238999999999999,
      "fitness": 0.31433999999999995
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.8623400000000001
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "You are now in second place, having passed the person who was previously in second place, and the person who was in first place is still ahead of you",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.9023400000000001
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.92234
    },
    {
      "generation": 12,
      "genome_id": "d6228c6c",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 12,
      "genome_id": "24286263",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "r14",
      "predicted_confidence": 0.8,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "First place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 12,
      "genome_id": "fc74ee78",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "5924c445",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "e2d2f3a0",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "226b4d5a",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "6d01765d",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "d327e7c6",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "0ffe8c9a",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "e53d5013",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "e12",
      "predicted_confidence": 0.4,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.3",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "cd57727f",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "r12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "e1479b2e",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "22",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8665
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "r12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The man's son",
      "ground_truth": "His son",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.8559999999999999
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "e09",
      "predicted_confidence": 0.8,
      "predicted_answer": "0.4",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 13,
      "genome_id": "3eb990d3",
      "task_id": "t15",
      "predicted_confidence": 0.8,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "a487bf60",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "50128e4d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.47850000000000004
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "bd32e432",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.80986
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.8698599999999999
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.8546600000000001
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8911,
      "fitness": 0.91466
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5510999999999999,
      "fitness": 0.33065999999999995
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.5970599999999999
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 14,
      "genome_id": "310a6e26",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "16868008",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "a0574c10",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.8583399999999999
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8506400000000001
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8506400000000001
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.80504
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "r02",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.87834
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "t11",
      "predicted_confidence": 0.8,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9324,
      "fitness": 0.5594399999999999
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "e193ee4d",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8706400000000001
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "e06",
      "predicted_confidence": 0.2,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.5985
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "134e2331",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200 million",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "1f1554df",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "r13",
      "predicted_confidence": 0.9,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "e03",
      "predicted_confidence": 0.8,
      "predicted_answer": "200",
      "ground_truth": "220",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8465
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "r02",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 minutes",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "t11",
      "predicted_confidence": 0.9,
      "predicted_answer": "1",
      "ground_truth": "1",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Oxygen",
      "ground_truth": "Silicon",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 14,
      "genome_id": "1b9779f6",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    }
  ],
  "test_results": {
    "n_tasks": 9,
    "n_agents": 10,
    "avg_raw_calibration": 0.8619444444444445,
    "avg_prediction_accuracy": 0.8698722222222222,
    "avg_task_accuracy": 0.8222222222222222,
    "best_fitness": 0.8161666666666667,
    "avg_fitness": 0.7779233333333333
  }
}