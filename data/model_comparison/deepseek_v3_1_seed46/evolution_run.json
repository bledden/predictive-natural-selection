{
  "model": "deepseek-ai/DeepSeek-V3.1",
  "slug": "deepseek_v3_1",
  "seed": 46,
  "elapsed_seconds": 127.74698686599731,
  "generation_stats": [
    {
      "generation": 0,
      "population_size": 10,
      "avg_fitness": 0.7436673333333333,
      "best_fitness": 0.7705,
      "worst_fitness": 0.7038933333333333,
      "avg_raw_calibration": 0.8158653333333333,
      "avg_prediction_accuracy": 0.81589,
      "avg_task_accuracy": 0.7733333333333333,
      "dominant_reasoning": "debate-self",
      "dominant_memory": "relevance",
      "elapsed_seconds": 8.554718017578125
    },
    {
      "generation": 1,
      "population_size": 10,
      "avg_fitness": 0.8341842666666667,
      "best_fitness": 0.88446,
      "worst_fitness": 0.7756693333333333,
      "avg_raw_calibration": 0.898816,
      "avg_prediction_accuracy": 0.889196,
      "avg_task_accuracy": 0.9,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "recency",
      "elapsed_seconds": 9.237727403640747
    },
    {
      "generation": 2,
      "population_size": 10,
      "avg_fitness": 0.7498115999999999,
      "best_fitness": 0.8217893333333333,
      "worst_fitness": 0.6823373333333334,
      "avg_raw_calibration": 0.8413666666666667,
      "avg_prediction_accuracy": 0.8416859999999999,
      "avg_task_accuracy": 0.76,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "recency",
      "elapsed_seconds": 8.632712125778198
    },
    {
      "generation": 3,
      "population_size": 10,
      "avg_fitness": 0.7796170666666666,
      "best_fitness": 0.8230466666666666,
      "worst_fitness": 0.702824,
      "avg_raw_calibration": 0.8553666666666667,
      "avg_prediction_accuracy": 0.8529173333333333,
      "avg_task_accuracy": 0.8066666666666666,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "recency",
      "elapsed_seconds": 7.351715087890625
    },
    {
      "generation": 4,
      "population_size": 10,
      "avg_fitness": 0.7874518666666667,
      "best_fitness": 0.8404333333333334,
      "worst_fitness": 0.7468266666666667,
      "avg_raw_calibration": 0.8588326666666666,
      "avg_prediction_accuracy": 0.858642,
      "avg_task_accuracy": 0.8266666666666667,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 8.672055959701538
    },
    {
      "generation": 5,
      "population_size": 10,
      "avg_fitness": 0.7373886666666667,
      "best_fitness": 0.77668,
      "worst_fitness": 0.7156973333333333,
      "avg_raw_calibration": 0.8024666666666667,
      "avg_prediction_accuracy": 0.7985366666666667,
      "avg_task_accuracy": 0.76,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 8.480130910873413
    },
    {
      "generation": 6,
      "population_size": 10,
      "avg_fitness": 0.7317652000000001,
      "best_fitness": 0.79432,
      "worst_fitness": 0.6811653333333333,
      "avg_raw_calibration": 0.8037406666666667,
      "avg_prediction_accuracy": 0.810942,
      "avg_task_accuracy": 0.74,
      "dominant_reasoning": "step-by-step",
      "dominant_memory": "recency",
      "elapsed_seconds": 8.983763933181763
    },
    {
      "generation": 7,
      "population_size": 10,
      "avg_fitness": 0.8058736,
      "best_fitness": 0.8732666666666666,
      "worst_fitness": 0.7697066666666667,
      "avg_raw_calibration": 0.891266,
      "avg_prediction_accuracy": 0.8884559999999999,
      "avg_task_accuracy": 0.84,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 8.30906891822815
    },
    {
      "generation": 8,
      "population_size": 10,
      "avg_fitness": 0.7428629333333333,
      "best_fitness": 0.7676200000000001,
      "worst_fitness": 0.6892026666666666,
      "avg_raw_calibration": 0.8219833333333333,
      "avg_prediction_accuracy": 0.825216,
      "avg_task_accuracy": 0.7466666666666667,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 8.757282733917236
    },
    {
      "generation": 9,
      "population_size": 10,
      "avg_fitness": 0.8157293333333333,
      "best_fitness": 0.8960533333333333,
      "worst_fitness": 0.7699666666666667,
      "avg_raw_calibration": 0.8852666666666666,
      "avg_prediction_accuracy": 0.86066,
      "avg_task_accuracy": 0.9,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 7.842713117599487
    },
    {
      "generation": 10,
      "population_size": 10,
      "avg_fitness": 0.8421926666666666,
      "best_fitness": 0.8722733333333333,
      "worst_fitness": 0.81604,
      "avg_raw_calibration": 0.9261499999999999,
      "avg_prediction_accuracy": 0.90921,
      "avg_task_accuracy": 0.9,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 7.213277816772461
    },
    {
      "generation": 11,
      "population_size": 10,
      "avg_fitness": 0.7773141333333333,
      "best_fitness": 0.8269,
      "worst_fitness": 0.7301226666666667,
      "avg_raw_calibration": 0.8421166666666666,
      "avg_prediction_accuracy": 0.8433013333333333,
      "avg_task_accuracy": 0.8333333333333334,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 7.920851945877075
    },
    {
      "generation": 12,
      "population_size": 10,
      "avg_fitness": 0.7381626666666666,
      "best_fitness": 0.7783666666666667,
      "worst_fitness": 0.6841,
      "avg_raw_calibration": 0.8131666666666666,
      "avg_prediction_accuracy": 0.81916,
      "avg_task_accuracy": 0.7333333333333333,
      "dominant_reasoning": "chain-of-thought",
      "dominant_memory": "relevance",
      "elapsed_seconds": 8.09102201461792
    },
    {
      "generation": 13,
      "population_size": 10,
      "avg_fitness": 0.7825245333333333,
      "best_fitness": 0.8273333333333334,
      "worst_fitness": 0.7430666666666667,
      "avg_raw_calibration": 0.85915,
      "avg_prediction_accuracy": 0.8579853333333334,
      "avg_task_accuracy": 0.8266666666666667,
      "dominant_reasoning": "analogical",
      "dominant_memory": "relevance",
      "elapsed_seconds": 7.759601831436157
    },
    {
      "generation": 14,
      "population_size": 10,
      "avg_fitness": 0.7403968,
      "best_fitness": 0.8054666666666667,
      "worst_fitness": 0.6831706666666667,
      "avg_raw_calibration": 0.7916,
      "avg_prediction_accuracy": 0.787328,
      "avg_task_accuracy": 0.8133333333333334,
      "dominant_reasoning": "analogical",
      "dominant_memory": "recency",
      "elapsed_seconds": 7.320537090301514
    }
  ],
  "all_genomes": [
    {
      "genome_id": "b1a877bf",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "debate-self",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.66,
      "temperature": 0.52,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "4de2b9f4",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.41,
      "temperature": 1.14,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "6c650ba0",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.52,
      "temperature": 1.15,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "5a6491d7",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.22,
      "temperature": 0.87,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "5b43d401",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.81,
      "temperature": 0.68,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "084841b2",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.62,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "be733146",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.82,
      "temperature": 1.08,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "e0b455d7",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.88,
      "temperature": 0.67,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "178b5938",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.85,
      "temperature": 0.66,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "0f456121",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 9,
      "memory_weighting": "recency",
      "confidence_bias": -0.12,
      "risk_tolerance": 0.35,
      "temperature": 0.84,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "1781f84e",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.62,
      "generation": 1,
      "parent_ids": [
        "084841b2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0273ca89",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.22,
      "temperature": 0.87,
      "generation": 1,
      "parent_ids": [
        "5a6491d7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "52afff5c",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.94,
      "generation": 1,
      "parent_ids": [
        "084841b2",
        "5a6491d7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f75de3f2",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.62,
      "generation": 1,
      "parent_ids": [
        "084841b2",
        "5a6491d7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8d65beb1",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.87,
      "generation": 1,
      "parent_ids": [
        "084841b2",
        "5a6491d7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c409ce79",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.62,
      "generation": 1,
      "parent_ids": [
        "5b43d401",
        "084841b2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "88b6e37f",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.68,
      "temperature": 0.68,
      "generation": 1,
      "parent_ids": [
        "5b43d401",
        "084841b2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c621ba6f",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.81,
      "temperature": 0.62,
      "generation": 1,
      "parent_ids": [
        "084841b2",
        "5b43d401"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5a354b1d",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.78,
      "temperature": 0.72,
      "generation": 1,
      "parent_ids": [
        "084841b2",
        "5b43d401"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bb2d0c5d",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.81,
      "temperature": 0.74,
      "generation": 1,
      "parent_ids": [
        "5b43d401",
        "5a6491d7"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fdbbcd07",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.81,
      "temperature": 0.62,
      "generation": 2,
      "parent_ids": [
        "c621ba6f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e3781ea4",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.94,
      "generation": 2,
      "parent_ids": [
        "52afff5c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c859e6dd",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.94,
      "generation": 2,
      "parent_ids": [
        "c409ce79",
        "52afff5c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "607f4842",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.72,
      "generation": 2,
      "parent_ids": [
        "c621ba6f",
        "52afff5c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d4c06d24",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.62,
      "generation": 2,
      "parent_ids": [
        "c621ba6f",
        "c409ce79"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "95b92cc3",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.6,
      "temperature": 0.62,
      "generation": 2,
      "parent_ids": [
        "52afff5c",
        "c409ce79"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6bc220bb",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.46,
      "temperature": 0.94,
      "generation": 2,
      "parent_ids": [
        "c409ce79",
        "52afff5c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3da1df6d",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.48,
      "temperature": 0.94,
      "generation": 2,
      "parent_ids": [
        "c409ce79",
        "52afff5c"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1ccfe6f8",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.81,
      "temperature": 0.62,
      "generation": 2,
      "parent_ids": [
        "c409ce79",
        "c621ba6f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7e487f5e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.74,
      "temperature": 0.62,
      "generation": 2,
      "parent_ids": [
        "52afff5c",
        "c409ce79"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ad4ce0e6",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.74,
      "temperature": 0.62,
      "generation": 3,
      "parent_ids": [
        "7e487f5e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1404f1ef",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.81,
      "temperature": 0.62,
      "generation": 3,
      "parent_ids": [
        "fdbbcd07"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "10dbc150",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.04,
      "risk_tolerance": 0.74,
      "temperature": 0.62,
      "generation": 3,
      "parent_ids": [
        "fdbbcd07",
        "7e487f5e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f422b602",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": 0.08,
      "risk_tolerance": 0.8,
      "temperature": 0.62,
      "generation": 3,
      "parent_ids": [
        "7e487f5e",
        "fdbbcd07"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "605311a6",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 4,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.74,
      "temperature": 0.62,
      "generation": 3,
      "parent_ids": [
        "7e487f5e",
        "fdbbcd07"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c2274ff8",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.46,
      "temperature": 0.94,
      "generation": 3,
      "parent_ids": [
        "6bc220bb",
        "7e487f5e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "85f7658d",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.58,
      "temperature": 0.94,
      "generation": 3,
      "parent_ids": [
        "7e487f5e",
        "6bc220bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ced3d877",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.79,
      "temperature": 0.53,
      "generation": 3,
      "parent_ids": [
        "fdbbcd07",
        "6bc220bb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f976ec2b",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.72,
      "temperature": 0.62,
      "generation": 3,
      "parent_ids": [
        "fdbbcd07",
        "7e487f5e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cf17c217",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.47,
      "temperature": 0.69,
      "generation": 3,
      "parent_ids": [
        "6bc220bb",
        "fdbbcd07"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f61a2e43",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": 0.08,
      "risk_tolerance": 0.8,
      "temperature": 0.62,
      "generation": 4,
      "parent_ids": [
        "f422b602"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b83daddd",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.58,
      "temperature": 0.94,
      "generation": 4,
      "parent_ids": [
        "85f7658d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "58c72981",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.8,
      "temperature": 0.94,
      "generation": 4,
      "parent_ids": [
        "85f7658d",
        "f422b602"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6768809c",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": 0.07,
      "risk_tolerance": 0.49,
      "temperature": 0.94,
      "generation": 4,
      "parent_ids": [
        "85f7658d",
        "f422b602"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f9dfe6c0",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.57,
      "generation": 4,
      "parent_ids": [
        "1404f1ef",
        "f422b602"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2e4eded4",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.08,
      "risk_tolerance": 0.7,
      "temperature": 0.94,
      "generation": 4,
      "parent_ids": [
        "f422b602",
        "85f7658d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6278b72f",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": 0.08,
      "risk_tolerance": 0.76,
      "temperature": 0.62,
      "generation": 4,
      "parent_ids": [
        "f422b602",
        "1404f1ef"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c8a789d7",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.81,
      "temperature": 0.58,
      "generation": 4,
      "parent_ids": [
        "85f7658d",
        "1404f1ef"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "09225aae",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.08,
      "risk_tolerance": 0.8,
      "temperature": 0.75,
      "generation": 4,
      "parent_ids": [
        "85f7658d",
        "f422b602"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "947c0698",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.8,
      "temperature": 0.62,
      "generation": 4,
      "parent_ids": [
        "85f7658d",
        "f422b602"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d2a95e81",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.57,
      "generation": 5,
      "parent_ids": [
        "f9dfe6c0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f01faa7d",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.8,
      "temperature": 0.94,
      "generation": 5,
      "parent_ids": [
        "58c72981"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dbc71fc7",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.08,
      "risk_tolerance": 0.8,
      "temperature": 0.94,
      "generation": 5,
      "parent_ids": [
        "58c72981",
        "6278b72f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1aec3eaf",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.8,
      "temperature": 0.94,
      "generation": 5,
      "parent_ids": [
        "f9dfe6c0",
        "58c72981"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3db0e88a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": 0.08,
      "risk_tolerance": 0.67,
      "temperature": 0.57,
      "generation": 5,
      "parent_ids": [
        "f9dfe6c0",
        "6278b72f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "94cadc16",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.76,
      "temperature": 0.62,
      "generation": 5,
      "parent_ids": [
        "6278b72f",
        "f9dfe6c0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "13703345",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.8,
      "temperature": 0.59,
      "generation": 5,
      "parent_ids": [
        "58c72981",
        "6278b72f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "055cc79c",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.69,
      "temperature": 0.57,
      "generation": 5,
      "parent_ids": [
        "58c72981",
        "f9dfe6c0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "910f0dad",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 5,
      "parent_ids": [
        "58c72981",
        "f9dfe6c0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9d3682c1",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.8,
      "temperature": 0.57,
      "generation": 5,
      "parent_ids": [
        "58c72981",
        "f9dfe6c0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d6a110ec",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.57,
      "generation": 6,
      "parent_ids": [
        "d2a95e81"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e235afb0",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 6,
      "parent_ids": [
        "910f0dad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bf2b20fe",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.48,
      "generation": 6,
      "parent_ids": [
        "910f0dad",
        "d2a95e81"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5c1d3d0a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.76,
      "temperature": 0.82,
      "generation": 6,
      "parent_ids": [
        "94cadc16",
        "910f0dad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2b1b9a60",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 6,
      "parent_ids": [
        "94cadc16",
        "910f0dad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e86a2018",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 6,
      "parent_ids": [
        "910f0dad",
        "94cadc16"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8db18933",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.73,
      "temperature": 0.57,
      "generation": 6,
      "parent_ids": [
        "910f0dad",
        "d2a95e81"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3c6efec5",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.75,
      "temperature": 0.62,
      "generation": 6,
      "parent_ids": [
        "d2a95e81",
        "94cadc16"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f35a79b0",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.62,
      "generation": 6,
      "parent_ids": [
        "94cadc16",
        "910f0dad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f5870a9f",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.57,
      "generation": 6,
      "parent_ids": [
        "910f0dad",
        "d2a95e81"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0ee13449",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.57,
      "generation": 7,
      "parent_ids": [
        "d6a110ec"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "21341dc4",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 7,
      "parent_ids": [
        "e86a2018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e8e52fde",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 1.09,
      "generation": 7,
      "parent_ids": [
        "d6a110ec",
        "e86a2018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f9cfb313",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 7,
      "parent_ids": [
        "e235afb0",
        "e86a2018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e30c4a1f",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.63,
      "generation": 7,
      "parent_ids": [
        "e235afb0",
        "d6a110ec"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a69b922c",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.0,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 7,
      "parent_ids": [
        "d6a110ec",
        "e86a2018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b2c1b5fd",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.02,
      "risk_tolerance": 0.52,
      "temperature": 0.5,
      "generation": 7,
      "parent_ids": [
        "d6a110ec",
        "e86a2018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "df78a387",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.67,
      "temperature": 0.94,
      "generation": 7,
      "parent_ids": [
        "e235afb0",
        "e86a2018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a1462d19",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.38,
      "generation": 7,
      "parent_ids": [
        "e235afb0",
        "d6a110ec"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5ca3a6fc",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 3,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.67,
      "temperature": 0.45,
      "generation": 7,
      "parent_ids": [
        "e235afb0",
        "d6a110ec"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "316b03a4",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.38,
      "generation": 8,
      "parent_ids": [
        "a1462d19"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "296e2745",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 1.09,
      "generation": 8,
      "parent_ids": [
        "e8e52fde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "70608722",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.38,
      "generation": 8,
      "parent_ids": [
        "e8e52fde",
        "a1462d19"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "12e87e02",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 8,
      "parent_ids": [
        "e8e52fde",
        "a1462d19"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6304433a",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.38,
      "generation": 8,
      "parent_ids": [
        "a1462d19",
        "e30c4a1f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ea62bb16",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.57,
      "generation": 8,
      "parent_ids": [
        "e30c4a1f",
        "e8e52fde"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "24c13723",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.67,
      "temperature": 0.62,
      "generation": 8,
      "parent_ids": [
        "e8e52fde",
        "e30c4a1f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c47af59e",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 8,
      "parent_ids": [
        "a1462d19",
        "e30c4a1f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3193be91",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.64,
      "temperature": 0.63,
      "generation": 8,
      "parent_ids": [
        "a1462d19",
        "e30c4a1f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6f551e54",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 1.09,
      "generation": 8,
      "parent_ids": [
        "e8e52fde",
        "e30c4a1f"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0a80bec4",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.38,
      "generation": 9,
      "parent_ids": [
        "316b03a4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6ae3bfe3",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 9,
      "parent_ids": [
        "12e87e02"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "caa98467",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.67,
      "temperature": 0.38,
      "generation": 9,
      "parent_ids": [
        "70608722",
        "12e87e02"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a525c689",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 9,
      "parent_ids": [
        "12e87e02",
        "316b03a4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f46852e7",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.48,
      "generation": 9,
      "parent_ids": [
        "12e87e02",
        "70608722"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "98849d76",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 3,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.55,
      "temperature": 0.23,
      "generation": 9,
      "parent_ids": [
        "12e87e02",
        "316b03a4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "03218b74",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 9,
      "parent_ids": [
        "12e87e02",
        "316b03a4"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4f27dc40",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 9,
      "parent_ids": [
        "70608722",
        "12e87e02"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "40141a7b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.67,
      "temperature": 0.44,
      "generation": 9,
      "parent_ids": [
        "316b03a4",
        "70608722"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "18c1f1c5",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 9,
      "parent_ids": [
        "12e87e02",
        "70608722"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "db373509",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 10,
      "parent_ids": [
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8272737b",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.67,
      "temperature": 0.44,
      "generation": 10,
      "parent_ids": [
        "40141a7b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "778c9cf6",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.39,
      "generation": 10,
      "parent_ids": [
        "40141a7b",
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "230dcf88",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 10,
      "parent_ids": [
        "caa98467",
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0f275902",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.11,
      "risk_tolerance": 0.67,
      "temperature": 0.44,
      "generation": 10,
      "parent_ids": [
        "caa98467",
        "40141a7b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ff645957",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 10,
      "parent_ids": [
        "caa98467",
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4f380bea",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.81,
      "temperature": 0.23,
      "generation": 10,
      "parent_ids": [
        "caa98467",
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "95ac1531",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 10,
      "parent_ids": [
        "40141a7b",
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2b9d167e",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.08,
      "risk_tolerance": 0.72,
      "temperature": 0.23,
      "generation": 10,
      "parent_ids": [
        "40141a7b",
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d2cee018",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 10,
      "parent_ids": [
        "40141a7b",
        "18c1f1c5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ae3e73aa",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 11,
      "parent_ids": [
        "d2cee018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bfa0b1bc",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 11,
      "parent_ids": [
        "95ac1531"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "882deddf",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 11,
      "parent_ids": [
        "d2cee018",
        "95ac1531"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "000e2261",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 11,
      "parent_ids": [
        "2b9d167e",
        "95ac1531"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "223056a6",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.78,
      "temperature": 0.36,
      "generation": 11,
      "parent_ids": [
        "2b9d167e",
        "95ac1531"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8cbc90e9",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 11,
      "parent_ids": [
        "d2cee018",
        "95ac1531"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a4ef86a8",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.36,
      "generation": 11,
      "parent_ids": [
        "95ac1531",
        "d2cee018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "83063f9a",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 11,
      "parent_ids": [
        "2b9d167e",
        "d2cee018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f5a35c3d",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.59,
      "temperature": 0.23,
      "generation": 11,
      "parent_ids": [
        "95ac1531",
        "d2cee018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "afb441da",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.52,
      "temperature": 0.41,
      "generation": 11,
      "parent_ids": [
        "2b9d167e",
        "d2cee018"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "632d568c",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 12,
      "parent_ids": [
        "8cbc90e9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e574f4b5",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 12,
      "parent_ids": [
        "ae3e73aa"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "36a7aaf0",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.01,
      "risk_tolerance": 0.67,
      "temperature": 0.24,
      "generation": 12,
      "parent_ids": [
        "000e2261",
        "8cbc90e9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "35fcb8ed",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 12,
      "parent_ids": [
        "ae3e73aa",
        "8cbc90e9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "05cd961e",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.66,
      "temperature": 0.21,
      "generation": 12,
      "parent_ids": [
        "ae3e73aa",
        "8cbc90e9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ab9aa791",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.21,
      "generation": 12,
      "parent_ids": [
        "ae3e73aa",
        "8cbc90e9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "70711c07",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.06,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 12,
      "parent_ids": [
        "8cbc90e9",
        "ae3e73aa"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a01f5662",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 12,
      "parent_ids": [
        "ae3e73aa",
        "8cbc90e9"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1da3867a",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.1,
      "generation": 12,
      "parent_ids": [
        "8cbc90e9",
        "000e2261"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "897f80da",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.23,
      "generation": 12,
      "parent_ids": [
        "8cbc90e9",
        "000e2261"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ad4b6fbb",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.66,
      "temperature": 0.21,
      "generation": 13,
      "parent_ids": [
        "05cd961e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0244d4f5",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.21,
      "generation": 13,
      "parent_ids": [
        "ab9aa791"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "373bcca2",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.66,
      "temperature": 0.21,
      "generation": 13,
      "parent_ids": [
        "ab9aa791",
        "05cd961e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0923bce0",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.72,
      "temperature": 0.26,
      "generation": 13,
      "parent_ids": [
        "05cd961e",
        "36a7aaf0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8fbadad7",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.21,
      "generation": 13,
      "parent_ids": [
        "ab9aa791",
        "05cd961e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f982422b",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.21,
      "generation": 13,
      "parent_ids": [
        "ab9aa791",
        "05cd961e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a6ee302d",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.67,
      "temperature": 0.18,
      "generation": 13,
      "parent_ids": [
        "ab9aa791",
        "36a7aaf0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b1f572a5",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.67,
      "temperature": 0.21,
      "generation": 13,
      "parent_ids": [
        "05cd961e",
        "36a7aaf0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eb706d56",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.6,
      "temperature": 0.19,
      "generation": 13,
      "parent_ids": [
        "05cd961e",
        "ab9aa791"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3cd42939",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.05,
      "risk_tolerance": 0.6,
      "temperature": 0.21,
      "generation": 13,
      "parent_ids": [
        "36a7aaf0",
        "ab9aa791"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "697108ec",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.66,
      "temperature": 0.21,
      "generation": 14,
      "parent_ids": [
        "373bcca2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ca7d7a24",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.72,
      "temperature": 0.26,
      "generation": 14,
      "parent_ids": [
        "0923bce0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ec0aa169",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.72,
      "temperature": 0.38,
      "generation": 14,
      "parent_ids": [
        "0244d4f5",
        "0923bce0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "eb345334",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.72,
      "temperature": 0.26,
      "generation": 14,
      "parent_ids": [
        "373bcca2",
        "0923bce0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fb34c758",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.6,
      "temperature": 0.26,
      "generation": 14,
      "parent_ids": [
        "0923bce0",
        "0244d4f5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "53f99934",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.74,
      "temperature": 0.21,
      "generation": 14,
      "parent_ids": [
        "0923bce0",
        "0244d4f5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "71da6f8d",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.09,
      "risk_tolerance": 0.78,
      "temperature": 0.26,
      "generation": 14,
      "parent_ids": [
        "373bcca2",
        "0923bce0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b89df783",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "analogical",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.66,
      "temperature": 0.35,
      "generation": 14,
      "parent_ids": [
        "373bcca2",
        "0244d4f5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b3d0c1d8",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.6,
      "temperature": 0.37,
      "generation": 14,
      "parent_ids": [
        "0244d4f5",
        "0923bce0"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0c90405a",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 1,
      "memory_weighting": "relevance",
      "confidence_bias": -0.07,
      "risk_tolerance": 0.71,
      "temperature": 0.21,
      "generation": 14,
      "parent_ids": [
        "0244d4f5",
        "373bcca2"
      ],
      "fitness_history": []
    }
  ],
  "all_results": [
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5375999999999999,
      "fitness": 0.7025599999999999
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.83946
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 0,
      "genome_id": "b1a877bf",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.77146
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "e08",
      "predicted_confidence": 0.3,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.63344
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8446400000000001
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.63344
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "t06",
      "predicted_confidence": 0.8,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "t12",
      "predicted_confidence": 0.75,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10,500",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.87354
    },
    {
      "generation": 0,
      "genome_id": "4de2b9f4",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.9022399999999999
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "t13",
      "predicted_confidence": 0.75,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.8023399999999999
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9424,
      "fitness": 0.5654399999999999
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "16 minutes",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 0,
      "genome_id": "6c650ba0",
      "task_id": "r07",
      "predicted_confidence": 0.75,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.8823399999999999
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "e08",
      "predicted_confidence": 0.85,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "e10",
      "predicted_confidence": 0.55,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7696,
      "fitness": 0.46175999999999995
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "t08",
      "predicted_confidence": 0.4,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.69066
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 0,
      "genome_id": "5a6491d7",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500,000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.9295399999999999
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "5b43d401",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.82384
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.8846400000000001
    },
    {
      "generation": 0,
      "genome_id": "084841b2",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.75184
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.85994
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.7918399999999999
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "t06",
      "predicted_confidence": 0.2,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4959,
      "fitness": 0.65754
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "r11",
      "predicted_confidence": 0.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.59514
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "r01",
      "predicted_confidence": 0.4,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.74394
    },
    {
      "generation": 0,
      "genome_id": "be733146",
      "task_id": "r07",
      "predicted_confidence": 0.2,
      "predicted_answer": "Yes",
      "ground_truth": "No",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8225
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.99,
      "fitness": 0.594
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.834
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "r01",
      "predicted_confidence": 0.05,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.30000000000000004
    },
    {
      "generation": 0,
      "genome_id": "e0b455d7",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.15360000000000018,
      "fitness": 0.0921600000000001
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.83946
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.88056
    },
    {
      "generation": 0,
      "genome_id": "178b5938",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.77146
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "400000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.81626
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.46176
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "t12",
      "predicted_confidence": 0.6,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7696000000000001,
      "fitness": 0.46176
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9856,
      "fitness": 0.83136
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.86266
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.88266
    },
    {
      "generation": 0,
      "genome_id": "0f456121",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "Yes",
      "ground_truth": "No",
      "is_correct": false,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.6
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.75184
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.9022399999999999
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.7186400000000001
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 1,
      "genome_id": "1781f84e",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "t12",
      "predicted_confidence": 0.65,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "e07",
      "predicted_confidence": 0.25,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.57656
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "0273ca89",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "r07",
      "predicted_confidence": 0.75,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.8823399999999999
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8319000000000001,
      "fitness": 0.87914
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.96464
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 1,
      "genome_id": "52afff5c",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8313599999999999
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.37546000000000007
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "e08",
      "predicted_confidence": 0.4,
      "predicted_answer": "500,000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "f75de3f2",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "1.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "Yes",
      "ground_truth": "No",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500,000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.7186400000000001
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 1,
      "genome_id": "8d65beb1",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.82384
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.80504
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "c409ce79",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.834
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "e08",
      "predicted_confidence": 0.4,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.51,
      "fitness": 0.6859999999999999
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.51,
      "fitness": 0.6859999999999999
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 1,
      "genome_id": "88b6e37f",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 1,
      "genome_id": "c621ba6f",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "e09",
      "predicted_confidence": 0.85,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "r07",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.91064
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 1,
      "genome_id": "5a354b1d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "bb2d0c5d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "e06",
      "predicted_confidence": 0.55,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7696,
      "fitness": 0.46175999999999995
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.5",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 2,
      "genome_id": "fdbbcd07",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.75184
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7083999999999999,
      "fitness": 0.4250399999999999
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5904000000000001,
      "fitness": 0.35424000000000005
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9046400000000001
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2079000000000002,
      "fitness": 0.12474000000000011
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9424,
      "fitness": 0.5654399999999999
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 2,
      "genome_id": "e3781ea4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "r09",
      "predicted_confidence": 0.85,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.48384
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5904000000000001,
      "fitness": 0.35424000000000005
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "e10",
      "predicted_confidence": 0.1,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9984,
      "fitness": 0.5990399999999999
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "r04",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.38704000000000005
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.7186400000000001
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 2,
      "genome_id": "c859e6dd",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 2,
      "genome_id": "607f4842",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 2,
      "genome_id": "d4c06d24",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.75184
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5904000000000001,
      "fitness": 0.35424000000000005
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6518999999999999,
      "fitness": 0.39113999999999993
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "t09",
      "predicted_confidence": 0.75,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 2,
      "genome_id": "95b92cc3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "e06",
      "predicted_confidence": 0.75,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "1.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8378399999999999
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 2,
      "genome_id": "6bc220bb",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "3.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "r04",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "t06",
      "predicted_confidence": 0.65,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.90954
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "e07",
      "predicted_confidence": 0.25,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.69434
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "3da1df6d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "t06",
      "predicted_confidence": 0.75,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "1ccfe6f8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.83914
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.97994
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "7e487f5e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "e08",
      "predicted_confidence": 0.85,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "r07",
      "predicted_confidence": 0.4,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.7839400000000001
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "r01",
      "predicted_confidence": 0.05,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.4562400000000001
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 3,
      "genome_id": "ad4ce0e6",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "r07",
      "predicted_confidence": 0.75,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.87856
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8709600000000001
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "e09",
      "predicted_confidence": 0.55,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7696,
      "fitness": 0.46175999999999995
    },
    {
      "generation": 3,
      "genome_id": "1404f1ef",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9151400000000001
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.85514
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8844,
      "fitness": 0.91064
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9390399999999999
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.17190000000000016,
      "fitness": 0.10314000000000009
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9639,
      "fitness": 0.87834
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.33864000000000005
    },
    {
      "generation": 3,
      "genome_id": "10dbc150",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "r01",
      "predicted_confidence": 0.5,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.79416
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "e09",
      "predicted_confidence": 0.35,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 3,
      "genome_id": "f422b602",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.83914
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.97994
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 3,
      "genome_id": "605311a6",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.85994
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "c2274ff8",
      "task_id": "t14",
      "predicted_confidence": 0.75,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.8578399999999999
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "r07",
      "predicted_confidence": 0.4,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.7839400000000001
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "r11",
      "predicted_confidence": 0.4,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 3,
      "genome_id": "85f7658d",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "r01",
      "predicted_confidence": 0.75,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.83856
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 3,
      "genome_id": "ced3d877",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.52586
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "r11",
      "predicted_confidence": 0.8,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 3,
      "genome_id": "f976ec2b",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "e06",
      "predicted_confidence": 0.75,
      "predicted_answer": "150",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.63344
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "e08",
      "predicted_confidence": 0.85,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5904000000000001,
      "fitness": 0.35424000000000005
    },
    {
      "generation": 3,
      "genome_id": "cf17c217",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8570599999999999
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "t14",
      "predicted_confidence": 0.8,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39160000000000017,
      "fitness": 0.23496000000000009
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7696,
      "fitness": 0.46175999999999995
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 4,
      "genome_id": "f61a2e43",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.83914
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "r04",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.5463399999999999
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "e08",
      "predicted_confidence": 0.8,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 4,
      "genome_id": "b83daddd",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 4,
      "genome_id": "58c72981",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "e07",
      "predicted_confidence": 0.25,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.7025600000000001
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "t12",
      "predicted_confidence": 0.75,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.19655999999999993
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "e02",
      "predicted_confidence": 0.95,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7790999999999999,
      "fitness": 0.46745999999999993
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "t08",
      "predicted_confidence": 0.3,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031,
      "fitness": 0.72186
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.28896
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "r07",
      "predicted_confidence": 0.2,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.62026
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9216,
      "fitness": 0.93296
    },
    {
      "generation": 4,
      "genome_id": "6768809c",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "t14",
      "predicted_confidence": 1.0,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "e08",
      "predicted_confidence": 0.8,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 4,
      "genome_id": "f9dfe6c0",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "r10",
      "predicted_confidence": 0.95,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "r01",
      "predicted_confidence": 0.7,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8709600000000001
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39160000000000017,
      "fitness": 0.23496000000000009
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "Yes",
      "ground_truth": "No",
      "is_correct": false,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.59616
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "r04",
      "predicted_confidence": 0.3,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.70936
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "e08",
      "predicted_confidence": 0.3,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 4,
      "genome_id": "2e4eded4",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.81776
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.43216
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 4,
      "genome_id": "6278b72f",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "e08",
      "predicted_confidence": 0.4,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 4,
      "genome_id": "c8a789d7",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8570599999999999
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "t12",
      "predicted_confidence": 0.75,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "t08",
      "predicted_confidence": 0.7,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "r04",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.83416
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 4,
      "genome_id": "09225aae",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.84
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "r01",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "r07",
      "predicted_confidence": 0.4,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.7839400000000001
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 4,
      "genome_id": "947c0698",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.52586
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "r07",
      "predicted_confidence": 0.85,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 5,
      "genome_id": "d2a95e81",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.44314
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "e08",
      "predicted_confidence": 0.25,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5644,
      "fitness": 0.71864
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.83914
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "r09",
      "predicted_confidence": 0.85,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "3.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "t03",
      "predicted_confidence": 0.95,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "t09",
      "predicted_confidence": 0.8,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.12473999999999998
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 5,
      "genome_id": "f01faa7d",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.43216
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39160000000000017,
      "fitness": 0.23496000000000009
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "e10",
      "predicted_confidence": 0.55,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 5,
      "genome_id": "dbc71fc7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.44314
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "t06",
      "predicted_confidence": 0.8,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "e08",
      "predicted_confidence": 0.8,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "r09",
      "predicted_confidence": 0.85,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 5,
      "genome_id": "1aec3eaf",
      "task_id": "r14",
      "predicted_confidence": 0.9,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9856,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8236,
      "fitness": 0.83416
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9996,
      "fitness": 0.91976
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "e09",
      "predicted_confidence": 0.5,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7296,
      "fitness": 0.79776
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 5,
      "genome_id": "3db0e88a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "e08",
      "predicted_confidence": 0.4,
      "predicted_answer": "500,000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 5,
      "genome_id": "94cadc16",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.4959,
      "fitness": 0.67754
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.83914
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.9199400000000001
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.31434
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "t09",
      "predicted_confidence": 0.75,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 5,
      "genome_id": "13703345",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "r04",
      "predicted_confidence": 0.2,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.48586
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 5,
      "genome_id": "055cc79c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "t06",
      "predicted_confidence": 0.8,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.52586
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "e09",
      "predicted_confidence": 0.35,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9216,
      "fitness": 0.55296
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9831,
      "fitness": 0.5898599999999999
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "t09",
      "predicted_confidence": 0.75,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 5,
      "genome_id": "910f0dad",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "r04",
      "predicted_confidence": 0.85,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.93784
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 5,
      "genome_id": "9d3682c1",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "e09",
      "predicted_confidence": 0.3,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 6,
      "genome_id": "d6a110ec",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "e07",
      "predicted_confidence": 0.55,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.7975000000000001,
      "prediction_accuracy": 0.7296,
      "fitness": 0.81776
    },
    {
      "generation": 6,
      "genome_id": "e235afb0",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "t04",
      "predicted_confidence": 0.9,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 6,
      "genome_id": "bf2b20fe",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.37546000000000007
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "t08",
      "predicted_confidence": 0.6,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.8274600000000001
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "e09",
      "predicted_confidence": 0.2,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9831,
      "fitness": 0.5898599999999999
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 6,
      "genome_id": "5c1d3d0a",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "t08",
      "predicted_confidence": 0.92,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "e09",
      "predicted_confidence": 0.55,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7696,
      "fitness": 0.46175999999999995
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 6,
      "genome_id": "2b1b9a60",
      "task_id": "r11",
      "predicted_confidence": 0.65,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.8797599999999999
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.97136
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "t09",
      "predicted_confidence": 0.6,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8236,
      "fitness": 0.87416
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "e09",
      "predicted_confidence": 0.55,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "t04",
      "predicted_confidence": 0.75,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.83976
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "r04",
      "predicted_confidence": 0.2,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.5365600000000001
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 6,
      "genome_id": "e86a2018",
      "task_id": "r11",
      "predicted_confidence": 0.2,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9676,
      "fitness": 0.58056
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "t04",
      "predicted_confidence": 0.8,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 6,
      "genome_id": "8db18933",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "r07",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "4.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "e04",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.83786
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 6,
      "genome_id": "3c6efec5",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.1351000000000001,
      "fitness": 0.08106000000000006
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 6,
      "genome_id": "f35a79b0",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "t08",
      "predicted_confidence": 0.75,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.89856
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "r09",
      "predicted_confidence": 0.85,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 6,
      "genome_id": "f5870a9f",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.81626
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "e07",
      "predicted_confidence": 0.25,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.57656
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 7,
      "genome_id": "0ee13449",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.7850599999999999
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.83976
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "t08",
      "predicted_confidence": 0.6,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8236,
      "fitness": 0.85416
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "t12",
      "predicted_confidence": 0.7,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8976,
      "fitness": 0.87856
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "r01",
      "predicted_confidence": 0.2,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.4965600000000001
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9216,
      "fitness": 0.55296
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 7,
      "genome_id": "21341dc4",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "400",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "t13",
      "predicted_confidence": 0.7,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.77786
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "t05",
      "predicted_confidence": 1.0,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8151,
      "fitness": 0.48906
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 7,
      "genome_id": "e8e52fde",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.84266
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "e04",
      "predicted_confidence": 0.65,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8631,
      "fitness": 0.83786
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.83976
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.88266
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.91706
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.97976
    },
    {
      "generation": 7,
      "genome_id": "f9cfb313",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "t12",
      "predicted_confidence": 0.8,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "r01",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.85626
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 7,
      "genome_id": "e30c4a1f",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.75,
      "fitness": 0.83
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "e02",
      "predicted_confidence": 0.99,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9999,
      "fitness": 0.83994
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "r01",
      "predicted_confidence": 0.75,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8625
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 7,
      "genome_id": "a69b922c",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.75,
      "fitness": 0.83
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8570599999999999
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8236,
      "fitness": 0.87416
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.89976
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9996,
      "fitness": 0.83976
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.92266
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9711,
      "fitness": 0.88266
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 7,
      "genome_id": "b2c1b5fd",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.83946
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "r01",
      "predicted_confidence": 0.2,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.4866600000000001
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "e10",
      "predicted_confidence": 0.45,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8236,
      "fitness": 0.49416
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 7,
      "genome_id": "df78a387",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 7,
      "genome_id": "a1462d19",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.75,
      "fitness": 0.83
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8825000000000001
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.834
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 7,
      "genome_id": "5ca3a6fc",
      "task_id": "e06",
      "predicted_confidence": 0.75,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 8,
      "genome_id": "316b03a4",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "e09",
      "predicted_confidence": 0.3,
      "predicted_answer": "3.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "7500",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "e06",
      "predicted_confidence": 0.45,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.7975,
      "prediction_accuracy": 0.8556,
      "fitness": 0.51336
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 8,
      "genome_id": "296e2745",
      "task_id": "e04",
      "predicted_confidence": 0.8,
      "predicted_answer": "10900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.87626
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8826600000000001
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "r04",
      "predicted_confidence": 0.7,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8578600000000001
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 8,
      "genome_id": "70608722",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7399,
      "fitness": 0.82394
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.87944
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.8823400000000001
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "12e87e02",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "e09",
      "predicted_confidence": 0.15,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.99,
      "fitness": 0.594
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9375,
      "fitness": 0.5625
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 8,
      "genome_id": "6304433a",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 8,
      "genome_id": "ea62bb16",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.7186400000000001
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8844,
      "fitness": 0.53064
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6518999999999999,
      "fitness": 0.39113999999999993
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2943999999999999,
      "fitness": 0.17663999999999994
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "t06",
      "predicted_confidence": 0.8,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6518999999999999,
      "fitness": 0.39113999999999993
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9446400000000001
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 8,
      "genome_id": "24c13723",
      "task_id": "e04",
      "predicted_confidence": 1.0,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8150999999999999,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "9000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "e06",
      "predicted_confidence": 0.5,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "r04",
      "predicted_confidence": 0.2,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.52666
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 8,
      "genome_id": "c47af59e",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8709600000000001
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 8,
      "genome_id": "3193be91",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "7000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "t09",
      "predicted_confidence": 0.75,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "r01",
      "predicted_confidence": 0.2,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.44586000000000003
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.81626
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "r14",
      "predicted_confidence": 0.95,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "r04",
      "predicted_confidence": 0.05,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 8,
      "genome_id": "6f551e54",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "e08",
      "predicted_confidence": 0.4,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8826600000000001
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "e07",
      "predicted_confidence": 0.25,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.3275999999999999,
      "fitness": 0.57656
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 9,
      "genome_id": "0a80bec4",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.81944
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "r01",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.8423400000000001
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.5863399999999999
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.89354
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 9,
      "genome_id": "6ae3bfe3",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9639,
      "fitness": 0.57834
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "t13",
      "predicted_confidence": 1.0,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8527399999999999
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.87354
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "6",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9639,
      "fitness": 0.57834
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 9,
      "genome_id": "caa98467",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9727399999999999
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4071,
      "fitness": 0.24426
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.7025600000000001
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 9,
      "genome_id": "a525c689",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.37546000000000007
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 9,
      "genome_id": "f46852e7",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4524,
      "fitness": 0.27144
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7599,
      "fitness": 0.45594
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.89944
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.85944
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3438999999999999,
      "fitness": 0.5863399999999999
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 9,
      "genome_id": "98849d76",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.9394399999999999
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.75,
      "fitness": 0.83
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7985
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.8059999999999999
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.8859999999999999
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.846
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.866
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9059999999999999
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8265
    },
    {
      "generation": 9,
      "genome_id": "03218b74",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "r07",
      "predicted_confidence": 0.1,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.37546000000000007
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 9,
      "genome_id": "4f27dc40",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8911,
      "fitness": 0.91466
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9676,
      "fitness": 0.84056
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.61496
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.88826
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 9,
      "genome_id": "40141a7b",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.94826
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 9,
      "genome_id": "18c1f1c5",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9626600000000001
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 10,
      "genome_id": "db373509",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9298599999999999
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.7025600000000001
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.4050599999999999
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8976,
      "fitness": 0.5385599999999999
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "8272737b",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 10,
      "genome_id": "778c9cf6",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "r01",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9039,
      "fitness": 0.8423400000000001
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.4959,
      "fitness": 0.67754
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "230dcf88",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.83354
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "t04",
      "predicted_confidence": 1.0,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.91944
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.88464
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.71674
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.5644000000000001,
      "fitness": 0.7186400000000001
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8327399999999999
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "e10",
      "predicted_confidence": 0.2,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9919,
      "fitness": 0.59514
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.5495399999999999
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 10,
      "genome_id": "0f275902",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9324,
      "fitness": 0.81944
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8709600000000001
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 10,
      "genome_id": "ff645957",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8826600000000001
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 10,
      "genome_id": "4f380bea",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.9065
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 10,
      "genome_id": "95ac1531",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9082600000000001
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.88986
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6636,
      "fitness": 0.7381599999999999
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8151,
      "fitness": 0.8690599999999999
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.83616
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9216000000000001,
      "fitness": 0.91296
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9516,
      "fitness": 0.57096
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.7025600000000001
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9698599999999999
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.90986
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 10,
      "genome_id": "2b9d167e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9831,
      "fitness": 0.84986
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 10,
      "genome_id": "d2cee018",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.83096
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "ae3e73aa",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "t06",
      "predicted_confidence": 0.9,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "e09",
      "predicted_confidence": 0.75,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.9065
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 11,
      "genome_id": "bfa0b1bc",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "t06",
      "predicted_confidence": 0.95,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 11,
      "genome_id": "882deddf",
      "task_id": "r11",
      "predicted_confidence": 0.9,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6279,
      "fitness": 0.75674
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.91354
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.83994
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9038999999999999,
      "fitness": 0.9223399999999999
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 11,
      "genome_id": "000e2261",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9065000000000001
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "5.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "223056a6",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "Second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8775,
      "fitness": 0.5265
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.9065
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 11,
      "genome_id": "8cbc90e9",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "t14",
      "predicted_confidence": 0.85,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 11,
      "genome_id": "a4ef86a8",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "t14",
      "predicted_confidence": 0.9,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "250",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 11,
      "genome_id": "83063f9a",
      "task_id": "r11",
      "predicted_confidence": 0.85,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "t06",
      "predicted_confidence": 0.85,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9225000000000001
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "r04",
      "predicted_confidence": 0.2,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.5065000000000001
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9099999999999999,
      "fitness": 0.9259999999999999
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 11,
      "genome_id": "f5a35c3d",
      "task_id": "r11",
      "predicted_confidence": 0.95,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.19000000000000017,
      "fitness": 0.1140000000000001
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.90056
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "t06",
      "predicted_confidence": 0.8,
      "predicted_answer": "Seychelles",
      "ground_truth": "Seychelles",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "t14",
      "predicted_confidence": 0.95,
      "predicted_answer": "Black",
      "ground_truth": "Black",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "1.2",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6751,
      "fitness": 0.40506000000000003
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.83946
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "e08",
      "predicted_confidence": 0.4,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6031,
      "fitness": 0.74186
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "t13",
      "predicted_confidence": 0.8,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.82826
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 11,
      "genome_id": "afb441da",
      "task_id": "r11",
      "predicted_confidence": 1.0,
      "predicted_answer": "second place",
      "ground_truth": "2",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.05910000000000004,
      "fitness": 0.035460000000000026
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4375,
      "fitness": 0.6425000000000001
    },
    {
      "generation": 12,
      "genome_id": "632d568c",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.94266
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    },
    {
      "generation": 12,
      "genome_id": "e574f4b5",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8479,
      "fitness": 0.50874
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomato",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8999400000000001
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9964,
      "fitness": 0.91784
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.90224
    },
    {
      "generation": 12,
      "genome_id": "36a7aaf0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9999,
      "fitness": 0.8799399999999999
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.9026600000000001
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 12,
      "genome_id": "35fcb8ed",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "r15",
      "predicted_confidence": 0.85,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 12,
      "genome_id": "05cd961e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "e06",
      "predicted_confidence": 0.65,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 12,
      "genome_id": "ab9aa791",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.89274
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6518999999999999,
      "fitness": 0.39113999999999993
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6518999999999999,
      "fitness": 0.39113999999999993
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.9246400000000001
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9559,
      "fitness": 0.93354
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9744,
      "fitness": 0.96464
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.89784
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.22553999999999993
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8704,
      "fitness": 0.9022399999999999
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "e07",
      "predicted_confidence": 0.6,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7884,
      "fitness": 0.85304
    },
    {
      "generation": 12,
      "genome_id": "70711c07",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9964,
      "fitness": 0.87784
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.876
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 12,
      "genome_id": "a01f5662",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "r01",
      "predicted_confidence": 0.8,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.8625
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "300",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.7265000000000001
    },
    {
      "generation": 12,
      "genome_id": "1da3867a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.51,
      "fitness": 0.306
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.896
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "r14",
      "predicted_confidence": 1.0,
      "predicted_answer": "the box labeled 'Both",
      "ground_truth": "Both",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.5465
    },
    {
      "generation": 12,
      "genome_id": "897f80da",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "t08",
      "predicted_confidence": 0.9,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "ad4b6fbb",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8775,
      "fitness": 0.9065
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "e09",
      "predicted_confidence": 0.4,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 13,
      "genome_id": "0244d4f5",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8513599999999999
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 13,
      "genome_id": "373bcca2",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.83514
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.73114
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.88954
    },
    {
      "generation": 13,
      "genome_id": "0923bce0",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "8fbadad7",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "e04",
      "predicted_confidence": 0.85,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "f982422b",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "e09",
      "predicted_confidence": 0.6,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.31109999999999993,
      "fitness": 0.18665999999999996
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "t04",
      "predicted_confidence": 0.85,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 13,
      "genome_id": "a6ee302d",
      "task_id": "e08",
      "predicted_confidence": 0.8,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "e09",
      "predicted_confidence": 0.65,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "t13",
      "predicted_confidence": 0.85,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.7585
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "b1f572a5",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6031000000000002,
      "fitness": 0.3618600000000001
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.8426600000000001
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.9162600000000001
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.9713599999999999
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7191,
      "fitness": 0.43145999999999995
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9271,
      "fitness": 0.8962600000000001
    },
    {
      "generation": 13,
      "genome_id": "eb706d56",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "e09",
      "predicted_confidence": 0.7,
      "predicted_answer": "2.0",
      "ground_truth": "0.1",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8985
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "a human brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.854
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8384999999999999
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "t04",
      "predicted_confidence": 0.95,
      "predicted_answer": "1648",
      "ground_truth": "1648",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.9339999999999999
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.8784999999999998
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9384999999999999
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.99,
      "fitness": 0.914
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5775000000000001,
      "fitness": 0.3465000000000001
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9025000000000001
    },
    {
      "generation": 13,
      "genome_id": "3cd42939",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7974999999999999,
      "fitness": 0.8584999999999999
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.5376000000000001,
      "fitness": 0.32256
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "r07",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9711000000000001,
      "fitness": 0.92266
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8976,
      "fitness": 0.9185599999999999
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 14,
      "genome_id": "697108ec",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "e07",
      "predicted_confidence": 0.2,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.20789999999999997,
      "fitness": 0.50474
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.88954
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.86544
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.83514
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.94824
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 14,
      "genome_id": "ca7d7a24",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.69434
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.90544
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.83514
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.91064
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 14,
      "genome_id": "ec0aa169",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5239,
      "fitness": 0.69434
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "r04",
      "predicted_confidence": 0.8,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.88954
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.83514
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "r07",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "t05",
      "predicted_confidence": 0.9,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 14,
      "genome_id": "eb345334",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3758999999999999,
      "fitness": 0.60554
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9351400000000001
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.83514
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.73114
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.91064
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 14,
      "genome_id": "fb34c758",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "e07",
      "predicted_confidence": 0.3,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6636,
      "fitness": 0.39815999999999996
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.93706
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "t08",
      "predicted_confidence": 1.0,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 14,
      "genome_id": "53f99934",
      "task_id": "e06",
      "predicted_confidence": 0.3,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.62426
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "e07",
      "predicted_confidence": 0.65,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.89514
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.83514
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9919,
      "fitness": 0.87514
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.73114
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9159,
      "fitness": 0.90954
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 14,
      "genome_id": "71da6f8d",
      "task_id": "e06",
      "predicted_confidence": 0.6,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7599,
      "fitness": 0.83594
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "e07",
      "predicted_confidence": 0.35,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.48160000000000003,
      "fitness": 0.66896
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "r04",
      "predicted_confidence": 0.3,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4071,
      "fitness": 0.58426
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.39159999999999995,
      "fitness": 0.23495999999999995
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "e10",
      "predicted_confidence": 0.3,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.91,
      "prediction_accuracy": 0.9471,
      "fitness": 0.56826
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "r01",
      "predicted_confidence": 0.85,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8709600000000001
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "t08",
      "predicted_confidence": 0.95,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.95136
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "e08",
      "predicted_confidence": 0.6,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.7791,
      "fitness": 0.84746
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "t05",
      "predicted_confidence": 0.85,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.8909600000000001
    },
    {
      "generation": 14,
      "genome_id": "b89df783",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "350",
      "ground_truth": "200",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "e07",
      "predicted_confidence": 0.4,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.6031,
      "fitness": 0.74186
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "t09",
      "predicted_confidence": 0.85,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.93946
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9676,
      "fitness": 0.92056
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.8961600000000001
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.89946
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212 degrees Fahrenheit",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.83946
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9991,
      "fitness": 0.8794599999999999
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.77146
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "t08",
      "predicted_confidence": 0.8,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9471,
      "fitness": 0.9282600000000001
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "e08",
      "predicted_confidence": 0.75,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9216,
      "fitness": 0.93296
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9161600000000001
    },
    {
      "generation": 14,
      "genome_id": "b3d0c1d8",
      "task_id": "e06",
      "predicted_confidence": 0.7,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8911,
      "fitness": 0.91466
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "e07",
      "predicted_confidence": 0.45,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.6975,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.74936
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.22560000000000013,
      "fitness": 0.13536000000000006
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "r04",
      "predicted_confidence": 0.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.34
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Tomatoes",
      "ground_truth": "Tomato",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.91096
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "e10",
      "predicted_confidence": 0.4,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.8911,
      "fitness": 0.53466
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.8913599999999999
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "r10",
      "predicted_confidence": 1.0,
      "predicted_answer": "3 socks",
      "ground_truth": "3",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.89706
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.93136
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "e02",
      "predicted_confidence": 1.0,
      "predicted_answer": "212\u00b0F",
      "ground_truth": "212",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.8370599999999999
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9951,
      "fitness": 0.87706
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "r07",
      "predicted_confidence": 0.5,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6750999999999999,
      "fitness": 0.74506
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "t08",
      "predicted_confidence": 0.85,
      "predicted_answer": "Q",
      "ground_truth": "Q",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9516,
      "fitness": 0.93096
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "e08",
      "predicted_confidence": 0.65,
      "predicted_answer": "500000",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.8236000000000001,
      "fitness": 0.87416
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "t05",
      "predicted_confidence": 0.95,
      "predicted_answer": "Silicon",
      "ground_truth": "Silicon",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9855999999999999,
      "fitness": 0.91136
    },
    {
      "generation": 14,
      "genome_id": "0c90405a",
      "task_id": "e06",
      "predicted_confidence": 0.4,
      "predicted_answer": "200",
      "ground_truth": "200",
      "is_correct": true,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.7106600000000001
    }
  ],
  "test_results": {
    "n_tasks": 9,
    "n_agents": 10,
    "avg_raw_calibration": 0.9201111111111111,
    "avg_prediction_accuracy": 0.913811111111111,
    "avg_task_accuracy": 0.8333333333333334,
    "best_fitness": 0.8462511111111112,
    "avg_fitness": 0.8062866666666666
  }
}