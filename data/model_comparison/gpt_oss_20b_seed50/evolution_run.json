{
  "model": "openai/gpt-oss-20b",
  "slug": "gpt_oss_20b",
  "seed": 50,
  "elapsed_seconds": 344.31554222106934,
  "generation_stats": [
    {
      "generation": 0,
      "population_size": 10,
      "avg_fitness": 0.7231484,
      "best_fitness": 0.7637853333333333,
      "worst_fitness": 0.6815626666666666,
      "avg_raw_calibration": 0.8270393333333333,
      "avg_prediction_accuracy": 0.8145806666666666,
      "avg_task_accuracy": 0.72,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 25.189220905303955
    },
    {
      "generation": 1,
      "population_size": 10,
      "avg_fitness": 0.739756,
      "best_fitness": 0.8292293333333334,
      "worst_fitness": 0.673696,
      "avg_raw_calibration": 0.8283973333333333,
      "avg_prediction_accuracy": 0.8169266666666667,
      "avg_task_accuracy": 0.7666666666666667,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 18.28313970565796
    },
    {
      "generation": 2,
      "population_size": 10,
      "avg_fitness": 0.7110842666666667,
      "best_fitness": 0.7778973333333333,
      "worst_fitness": 0.62766,
      "avg_raw_calibration": 0.790338,
      "avg_prediction_accuracy": 0.782696,
      "avg_task_accuracy": 0.7133333333333334,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 25.347332000732422
    },
    {
      "generation": 3,
      "population_size": 10,
      "avg_fitness": 0.7618332,
      "best_fitness": 0.8291186666666666,
      "worst_fitness": 0.6817399999999999,
      "avg_raw_calibration": 0.842962,
      "avg_prediction_accuracy": 0.8343886666666667,
      "avg_task_accuracy": 0.7866666666666666,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 21.522558212280273
    },
    {
      "generation": 4,
      "population_size": 10,
      "avg_fitness": 0.7301193333333333,
      "best_fitness": 0.7940053333333333,
      "worst_fitness": 0.6927453333333333,
      "avg_raw_calibration": 0.809288,
      "avg_prediction_accuracy": 0.80931,
      "avg_task_accuracy": 0.7666666666666667,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 18.961178064346313
    },
    {
      "generation": 5,
      "population_size": 10,
      "avg_fitness": 0.7919048,
      "best_fitness": 0.827916,
      "worst_fitness": 0.7242666666666666,
      "avg_raw_calibration": 0.8905493333333334,
      "avg_prediction_accuracy": 0.8798413333333334,
      "avg_task_accuracy": 0.8333333333333334,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 18.674850940704346
    },
    {
      "generation": 6,
      "population_size": 10,
      "avg_fitness": 0.7933412,
      "best_fitness": 0.867632,
      "worst_fitness": 0.6794,
      "avg_raw_calibration": 0.8627333333333334,
      "avg_prediction_accuracy": 0.8575686666666666,
      "avg_task_accuracy": 0.8333333333333334,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 22.42379903793335
    },
    {
      "generation": 7,
      "population_size": 10,
      "avg_fitness": 0.7698070666666667,
      "best_fitness": 0.8119666666666666,
      "worst_fitness": 0.7478666666666667,
      "avg_raw_calibration": 0.8664153333333333,
      "avg_prediction_accuracy": 0.853234,
      "avg_task_accuracy": 0.82,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 20.43253183364868
    },
    {
      "generation": 8,
      "population_size": 10,
      "avg_fitness": 0.7195517333333333,
      "best_fitness": 0.7598666666666667,
      "worst_fitness": 0.6648333333333334,
      "avg_raw_calibration": 0.8163293333333334,
      "avg_prediction_accuracy": 0.7916973333333334,
      "avg_task_accuracy": 0.74,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 23.956139087677002
    },
    {
      "generation": 9,
      "population_size": 10,
      "avg_fitness": 0.7269089333333334,
      "best_fitness": 0.7803,
      "worst_fitness": 0.6550666666666667,
      "avg_raw_calibration": 0.8035813333333334,
      "avg_prediction_accuracy": 0.7832926666666666,
      "avg_task_accuracy": 0.7466666666666667,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "relevance",
      "elapsed_seconds": 23.210487127304077
    },
    {
      "generation": 10,
      "population_size": 10,
      "avg_fitness": 0.7926969333333334,
      "best_fitness": 0.86414,
      "worst_fitness": 0.7243693333333333,
      "avg_raw_calibration": 0.8627073333333334,
      "avg_prediction_accuracy": 0.8582726666666667,
      "avg_task_accuracy": 0.84,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "recency",
      "elapsed_seconds": 21.741596937179565
    },
    {
      "generation": 11,
      "population_size": 10,
      "avg_fitness": 0.6656484,
      "best_fitness": 0.7317506666666667,
      "worst_fitness": 0.5886613333333333,
      "avg_raw_calibration": 0.763618,
      "avg_prediction_accuracy": 0.733414,
      "avg_task_accuracy": 0.6533333333333333,
      "dominant_reasoning": "first-principles",
      "dominant_memory": "success-rate",
      "elapsed_seconds": 25.589086055755615
    },
    {
      "generation": 12,
      "population_size": 10,
      "avg_fitness": 0.6860853333333333,
      "best_fitness": 0.733724,
      "worst_fitness": 0.6191226666666666,
      "avg_raw_calibration": 0.79418,
      "avg_prediction_accuracy": 0.76592,
      "avg_task_accuracy": 0.6866666666666666,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 25.40940284729004
    },
    {
      "generation": 13,
      "population_size": 10,
      "avg_fitness": 0.717866,
      "best_fitness": 0.81582,
      "worst_fitness": 0.6454666666666667,
      "avg_raw_calibration": 0.8253233333333334,
      "avg_prediction_accuracy": 0.8004433333333334,
      "avg_task_accuracy": 0.74,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 23.69852304458618
    },
    {
      "generation": 14,
      "population_size": 10,
      "avg_fitness": 0.7391768,
      "best_fitness": 0.8635746666666667,
      "worst_fitness": 0.6474666666666666,
      "avg_raw_calibration": 0.8443753333333334,
      "avg_prediction_accuracy": 0.8219613333333333,
      "avg_task_accuracy": 0.7733333333333333,
      "dominant_reasoning": "elimination",
      "dominant_memory": "relevance",
      "elapsed_seconds": 17.26675796508789
    }
  ],
  "all_genomes": [
    {
      "genome_id": "5749f4a6",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "9217d381",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.73,
      "temperature": 0.86,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "e0eaf7a5",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.68,
      "temperature": 1.01,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "89e79cef",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.8,
      "temperature": 0.87,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "e79de51c",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 3,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.14,
      "temperature": 1.09,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "438bedcd",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 1,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.26,
      "temperature": 0.76,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "2334a6ba",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": 0.14,
      "risk_tolerance": 0.79,
      "temperature": 1.07,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "b6713f7e",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": -0.03,
      "risk_tolerance": 0.54,
      "temperature": 1.04,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "42e98736",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.05,
      "risk_tolerance": 0.3,
      "temperature": 0.79,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "e4d54607",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "first-principles",
      "memory_window": 1,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.52,
      "temperature": 0.83,
      "generation": 0,
      "parent_ids": [],
      "fitness_history": []
    },
    {
      "genome_id": "678e1e67",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 1,
      "parent_ids": [
        "5749f4a6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "bc73e105",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.68,
      "temperature": 1.01,
      "generation": 1,
      "parent_ids": [
        "e0eaf7a5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "83e94041",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.15,
      "risk_tolerance": 0.73,
      "temperature": 0.82,
      "generation": 1,
      "parent_ids": [
        "e0eaf7a5",
        "9217d381"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0026737e",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.73,
      "temperature": 1.01,
      "generation": 1,
      "parent_ids": [
        "9217d381",
        "e0eaf7a5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8a7ada68",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "analogical",
      "memory_window": 2,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.04,
      "temperature": 0.86,
      "generation": 1,
      "parent_ids": [
        "5749f4a6",
        "9217d381"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "49d49754",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.71,
      "generation": 1,
      "parent_ids": [
        "9217d381",
        "5749f4a6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b27e703f",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": 0.06,
      "risk_tolerance": 0.79,
      "temperature": 1.01,
      "generation": 1,
      "parent_ids": [
        "e0eaf7a5",
        "9217d381"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "82bc9aeb",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.68,
      "temperature": 0.48,
      "generation": 1,
      "parent_ids": [
        "5749f4a6",
        "e0eaf7a5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cf5b8cfc",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.68,
      "temperature": 0.33,
      "generation": 1,
      "parent_ids": [
        "e0eaf7a5",
        "5749f4a6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3dfae36a",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 1,
      "parent_ids": [
        "e0eaf7a5",
        "5749f4a6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ff107785",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.71,
      "generation": 2,
      "parent_ids": [
        "49d49754"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fc895144",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 2,
      "parent_ids": [
        "678e1e67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9326ca20",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 2,
      "parent_ids": [
        "678e1e67",
        "82bc9aeb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ce340567",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 2,
      "parent_ids": [
        "82bc9aeb",
        "678e1e67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "637fdb4b",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.19,
      "temperature": 0.52,
      "generation": 2,
      "parent_ids": [
        "82bc9aeb",
        "678e1e67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5b8a5add",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.73,
      "temperature": 0.71,
      "generation": 2,
      "parent_ids": [
        "49d49754",
        "82bc9aeb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "55d7c9b4",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.13,
      "temperature": 0.48,
      "generation": 2,
      "parent_ids": [
        "82bc9aeb",
        "678e1e67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a77d6ce8",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "first-principles",
      "memory_window": 2,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.59,
      "temperature": 0.62,
      "generation": 2,
      "parent_ids": [
        "82bc9aeb",
        "49d49754"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "45f44b77",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.05,
      "temperature": 0.63,
      "generation": 2,
      "parent_ids": [
        "678e1e67",
        "49d49754"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ad37289b",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 4,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.48,
      "generation": 2,
      "parent_ids": [
        "82bc9aeb",
        "678e1e67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9837c6af",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.19,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "637fdb4b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "741ddd56",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "9326ca20"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ba442588",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "debate-self",
      "memory_window": 5,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.19,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "9326ca20",
        "637fdb4b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c990b0a0",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.67,
      "generation": 3,
      "parent_ids": [
        "637fdb4b",
        "fc895144"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "dbc3ba35",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "fc895144",
        "9326ca20"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6f4ddaab",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.19,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "637fdb4b",
        "fc895144"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "23c51f34",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "elimination",
      "memory_window": 3,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "9326ca20",
        "fc895144"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e7b34a25",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.61,
      "generation": 3,
      "parent_ids": [
        "fc895144",
        "637fdb4b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "76929ba6",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.28,
      "temperature": 0.35,
      "generation": 3,
      "parent_ids": [
        "637fdb4b",
        "fc895144"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7ad3ee0e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 5,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.19,
      "temperature": 0.52,
      "generation": 3,
      "parent_ids": [
        "637fdb4b",
        "9326ca20"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1939a23f",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.61,
      "generation": 4,
      "parent_ids": [
        "e7b34a25"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "95e88539",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.19,
      "temperature": 0.52,
      "generation": 4,
      "parent_ids": [
        "9837c6af"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5301f9db",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.06,
      "temperature": 0.61,
      "generation": 4,
      "parent_ids": [
        "e7b34a25",
        "741ddd56"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e648efa5",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 4,
      "parent_ids": [
        "741ddd56",
        "e7b34a25"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b87eac67",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.15,
      "temperature": 0.62,
      "generation": 4,
      "parent_ids": [
        "e7b34a25",
        "9837c6af"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a3da1ad6",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "success-rate",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.19,
      "temperature": 0.61,
      "generation": 4,
      "parent_ids": [
        "9837c6af",
        "e7b34a25"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "250f18d1",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 4,
      "parent_ids": [
        "741ddd56",
        "e7b34a25"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5d694295",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.61,
      "generation": 4,
      "parent_ids": [
        "9837c6af",
        "e7b34a25"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8cac8f7a",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.19,
      "temperature": 0.42,
      "generation": 4,
      "parent_ids": [
        "9837c6af",
        "741ddd56"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fba84c3e",
      "system_prompt": "You argue with yourself, considering multiple viewpoints before deciding.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.19,
      "temperature": 0.52,
      "generation": 4,
      "parent_ids": [
        "e7b34a25",
        "9837c6af"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "663dcbda",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.52,
      "generation": 5,
      "parent_ids": [
        "e648efa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "77f5d05e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.15,
      "temperature": 0.62,
      "generation": 5,
      "parent_ids": [
        "b87eac67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "adfe0d77",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.15,
      "temperature": 0.62,
      "generation": 5,
      "parent_ids": [
        "5d694295",
        "b87eac67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f5bd77eb",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.61,
      "generation": 5,
      "parent_ids": [
        "b87eac67",
        "5d694295"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3ece4c8e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.15,
      "temperature": 0.52,
      "generation": 5,
      "parent_ids": [
        "b87eac67",
        "e648efa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "60b37c14",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.13,
      "temperature": 0.39,
      "generation": 5,
      "parent_ids": [
        "b87eac67",
        "e648efa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8efbbf97",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.53,
      "generation": 5,
      "parent_ids": [
        "5d694295",
        "e648efa5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "455e0783",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "analogical",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.61,
      "generation": 5,
      "parent_ids": [
        "b87eac67",
        "5d694295"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b28161c0",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": -0.14,
      "risk_tolerance": 0.07,
      "temperature": 0.48,
      "generation": 5,
      "parent_ids": [
        "e648efa5",
        "b87eac67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f0324fba",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.24,
      "temperature": 0.61,
      "generation": 5,
      "parent_ids": [
        "5d694295",
        "b87eac67"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "665cdbaf",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.53,
      "generation": 6,
      "parent_ids": [
        "8efbbf97"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b366d0f2",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.15,
      "temperature": 0.62,
      "generation": 6,
      "parent_ids": [
        "adfe0d77"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "87ad45ad",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 6,
      "parent_ids": [
        "adfe0d77",
        "455e0783"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "446465e4",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "analogical",
      "memory_window": 7,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.14,
      "temperature": 0.53,
      "generation": 6,
      "parent_ids": [
        "8efbbf97",
        "455e0783"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "149dc8b0",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.24,
      "temperature": 0.61,
      "generation": 6,
      "parent_ids": [
        "8efbbf97",
        "455e0783"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "64b3f3cc",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.18,
      "temperature": 0.53,
      "generation": 6,
      "parent_ids": [
        "8efbbf97",
        "455e0783"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c0358b66",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.08,
      "temperature": 0.7,
      "generation": 6,
      "parent_ids": [
        "adfe0d77",
        "8efbbf97"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "15b2c44f",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.53,
      "generation": 6,
      "parent_ids": [
        "adfe0d77",
        "8efbbf97"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "15618e73",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.53,
      "generation": 6,
      "parent_ids": [
        "8efbbf97",
        "455e0783"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0b55f84d",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "analogical",
      "memory_window": 7,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.15,
      "temperature": 0.61,
      "generation": 6,
      "parent_ids": [
        "adfe0d77",
        "455e0783"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a79c0fb7",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.53,
      "generation": 7,
      "parent_ids": [
        "15618e73"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "3cbd94fe",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.08,
      "temperature": 0.7,
      "generation": 7,
      "parent_ids": [
        "c0358b66"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a2dbc4a0",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.7,
      "generation": 7,
      "parent_ids": [
        "c0358b66",
        "87ad45ad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0ac0227d",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 7,
      "parent_ids": [
        "c0358b66",
        "87ad45ad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "85523e83",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 7,
      "parent_ids": [
        "87ad45ad",
        "c0358b66"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "69c0c631",
      "system_prompt": "You think by analogy \u2014 relate new problems to ones you know.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.0,
      "temperature": 0.5,
      "generation": 7,
      "parent_ids": [
        "c0358b66",
        "87ad45ad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "954421c7",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.08,
      "temperature": 0.7,
      "generation": 7,
      "parent_ids": [
        "c0358b66",
        "15618e73"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1cc634a8",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.43,
      "generation": 7,
      "parent_ids": [
        "87ad45ad",
        "15618e73"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cbef6464",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.08,
      "temperature": 0.53,
      "generation": 7,
      "parent_ids": [
        "15618e73",
        "c0358b66"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9606cfae",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.87,
      "generation": 7,
      "parent_ids": [
        "c0358b66",
        "87ad45ad"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "986fe0a5",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 8,
      "parent_ids": [
        "0ac0227d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "06c2e7c7",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 8,
      "parent_ids": [
        "85523e83"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "99c34a5b",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.77,
      "generation": 8,
      "parent_ids": [
        "0ac0227d",
        "9606cfae"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "62b62d55",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 8,
      "parent_ids": [
        "0ac0227d",
        "9606cfae"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "de0f6775",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.87,
      "generation": 8,
      "parent_ids": [
        "9606cfae",
        "0ac0227d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b125fee6",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.62,
      "generation": 8,
      "parent_ids": [
        "0ac0227d",
        "85523e83"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "14baf1ae",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.14,
      "risk_tolerance": 0.06,
      "temperature": 0.34,
      "generation": 8,
      "parent_ids": [
        "0ac0227d",
        "85523e83"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f06cbe94",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.87,
      "generation": 8,
      "parent_ids": [
        "9606cfae",
        "85523e83"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7ce40f01",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.79,
      "generation": 8,
      "parent_ids": [
        "0ac0227d",
        "9606cfae"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "34dd2bc2",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 8,
      "parent_ids": [
        "85523e83",
        "0ac0227d"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "d076bea5",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.77,
      "generation": 9,
      "parent_ids": [
        "99c34a5b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8b9f8a44",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.79,
      "generation": 9,
      "parent_ids": [
        "7ce40f01"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2aa35abd",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.79,
      "generation": 9,
      "parent_ids": [
        "99c34a5b",
        "7ce40f01"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cba1a494",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.16,
      "temperature": 0.79,
      "generation": 9,
      "parent_ids": [
        "62b62d55",
        "7ce40f01"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8b62743c",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.77,
      "generation": 9,
      "parent_ids": [
        "7ce40f01",
        "99c34a5b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "93d19c06",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.77,
      "generation": 9,
      "parent_ids": [
        "99c34a5b",
        "7ce40f01"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "db92bd86",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 9,
      "parent_ids": [
        "62b62d55",
        "7ce40f01"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6e0eb750",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.77,
      "generation": 9,
      "parent_ids": [
        "99c34a5b",
        "62b62d55"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a3189d5d",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "chain-of-thought",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.24,
      "temperature": 0.77,
      "generation": 9,
      "parent_ids": [
        "99c34a5b",
        "62b62d55"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "735932ee",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 9,
      "parent_ids": [
        "62b62d55",
        "99c34a5b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2e7db13e",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.13,
      "temperature": 0.77,
      "generation": 10,
      "parent_ids": [
        "d076bea5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "96674f8f",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 10,
      "parent_ids": [
        "db92bd86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6f4e571e",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.13,
      "temperature": 0.77,
      "generation": 10,
      "parent_ids": [
        "6e0eb750",
        "db92bd86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "19f2b543",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 10,
      "parent_ids": [
        "db92bd86",
        "6e0eb750"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e92eb6eb",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 10,
      "parent_ids": [
        "d076bea5",
        "db92bd86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "af735b13",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 10,
      "parent_ids": [
        "d076bea5",
        "db92bd86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "f2c326e2",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.92,
      "generation": 10,
      "parent_ids": [
        "db92bd86",
        "6e0eb750"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "719117df",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.28,
      "temperature": 0.3,
      "generation": 10,
      "parent_ids": [
        "d076bea5",
        "db92bd86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "ec6f22d0",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.16,
      "temperature": 0.77,
      "generation": 10,
      "parent_ids": [
        "db92bd86",
        "d076bea5"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "664c5ada",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.77,
      "generation": 10,
      "parent_ids": [
        "6e0eb750",
        "db92bd86"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "93e59087",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.92,
      "generation": 11,
      "parent_ids": [
        "f2c326e2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "e016dd6b",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 11,
      "parent_ids": [
        "e92eb6eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "97990ebc",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.09,
      "risk_tolerance": 0.24,
      "temperature": 0.92,
      "generation": 11,
      "parent_ids": [
        "664c5ada",
        "f2c326e2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "567b2042",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.37,
      "temperature": 0.77,
      "generation": 11,
      "parent_ids": [
        "664c5ada",
        "e92eb6eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "706e66e2",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.44,
      "generation": 11,
      "parent_ids": [
        "f2c326e2",
        "e92eb6eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "396111c6",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.89,
      "generation": 11,
      "parent_ids": [
        "664c5ada",
        "f2c326e2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "5ac144eb",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.14,
      "risk_tolerance": 0.24,
      "temperature": 0.77,
      "generation": 11,
      "parent_ids": [
        "f2c326e2",
        "664c5ada"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "37d832cc",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.24,
      "temperature": 0.74,
      "generation": 11,
      "parent_ids": [
        "f2c326e2",
        "664c5ada"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2ab3fd8b",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.26,
      "temperature": 0.92,
      "generation": 11,
      "parent_ids": [
        "f2c326e2",
        "664c5ada"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6543faa9",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.92,
      "generation": 11,
      "parent_ids": [
        "f2c326e2",
        "e92eb6eb"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "6bbc1d03",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.89,
      "generation": 12,
      "parent_ids": [
        "396111c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "52082a46",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 6,
      "memory_weighting": "recency",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.16,
      "temperature": 0.44,
      "generation": 12,
      "parent_ids": [
        "e016dd6b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "4b185f7d",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 7,
      "memory_weighting": "relevance",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.24,
      "temperature": 0.89,
      "generation": 12,
      "parent_ids": [
        "e016dd6b",
        "396111c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "fca55a71",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.38,
      "temperature": 0.92,
      "generation": 12,
      "parent_ids": [
        "93e59087",
        "396111c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "79d301a5",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.13,
      "temperature": 0.92,
      "generation": 12,
      "parent_ids": [
        "396111c6",
        "93e59087"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "44bbc448",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 9,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.92,
      "generation": 12,
      "parent_ids": [
        "396111c6",
        "93e59087"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2a4fe94b",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.72,
      "generation": 12,
      "parent_ids": [
        "396111c6",
        "93e59087"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0704b0b8",
      "system_prompt": "You break every problem down to its fundamental principles.",
      "reasoning_style": "elimination",
      "memory_window": 9,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.92,
      "generation": 12,
      "parent_ids": [
        "396111c6",
        "93e59087"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "669b0f05",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "debate-self",
      "memory_window": 10,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.16,
      "temperature": 0.35,
      "generation": 12,
      "parent_ids": [
        "e016dd6b",
        "93e59087"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a0a73c80",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 10,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.24,
      "temperature": 0.71,
      "generation": 12,
      "parent_ids": [
        "e016dd6b",
        "396111c6"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "08893c42",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.89,
      "generation": 13,
      "parent_ids": [
        "6bbc1d03"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "9e220ad6",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.38,
      "temperature": 0.92,
      "generation": 13,
      "parent_ids": [
        "fca55a71"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8ae8aa4a",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.72,
      "generation": 13,
      "parent_ids": [
        "fca55a71",
        "2a4fe94b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "821d6d6c",
      "system_prompt": "You reason by eliminating wrong answers first.",
      "reasoning_style": "debate-self",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.38,
      "temperature": 0.72,
      "generation": 13,
      "parent_ids": [
        "fca55a71",
        "2a4fe94b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "70c99196",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "first-principles",
      "memory_window": 10,
      "memory_weighting": "relevance",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.72,
      "generation": 13,
      "parent_ids": [
        "2a4fe94b",
        "fca55a71"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2014c0bf",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "step-by-step",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.11,
      "risk_tolerance": 0.38,
      "temperature": 0.72,
      "generation": 13,
      "parent_ids": [
        "fca55a71",
        "2a4fe94b"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8adb2549",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.89,
      "generation": 13,
      "parent_ids": [
        "6bbc1d03",
        "fca55a71"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "249fc1f2",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.24,
      "temperature": 0.92,
      "generation": 13,
      "parent_ids": [
        "fca55a71",
        "6bbc1d03"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "0f9cf3c5",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "success-rate",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.86,
      "generation": 13,
      "parent_ids": [
        "2a4fe94b",
        "fca55a71"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7d892c1e",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.72,
      "generation": 13,
      "parent_ids": [
        "2a4fe94b",
        "6bbc1d03"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "18184a8a",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "7d892c1e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "38b07cae",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.13,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "8ae8aa4a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "1cc87d42",
      "system_prompt": "You are a bold, intuitive reasoner who trusts your first instinct.",
      "reasoning_style": "analogical",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.1,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "7d892c1e",
        "8ae8aa4a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "c965500f",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.24,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "7d892c1e",
        "8ae8aa4a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "7b65c1b7",
      "system_prompt": "You are a devil's advocate who stress-tests your own reasoning.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.15,
      "risk_tolerance": 0.18,
      "temperature": 0.52,
      "generation": 14,
      "parent_ids": [
        "249fc1f2",
        "7d892c1e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "2520d9b3",
      "system_prompt": "You think probabilistically, always estimating likelihoods.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.99,
      "generation": 14,
      "parent_ids": [
        "7d892c1e",
        "249fc1f2"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "8fd6deaf",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "8ae8aa4a",
        "7d892c1e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "cee48807",
      "system_prompt": "You are a pattern-matcher who looks for structural similarity.",
      "reasoning_style": "debate-self",
      "memory_window": 9,
      "memory_weighting": "relevance",
      "confidence_bias": 0.13,
      "risk_tolerance": 0.24,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "8ae8aa4a",
        "7d892c1e"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "b183109a",
      "system_prompt": "You are a calibrated predictor who honestly assesses uncertainty.",
      "reasoning_style": "elimination",
      "memory_window": 8,
      "memory_weighting": "relevance",
      "confidence_bias": 0.12,
      "risk_tolerance": 0.13,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "7d892c1e",
        "8ae8aa4a"
      ],
      "fitness_history": []
    },
    {
      "genome_id": "a07a476c",
      "system_prompt": "You are a careful, methodical thinker who checks each step.",
      "reasoning_style": "first-principles",
      "memory_window": 8,
      "memory_weighting": "recency",
      "confidence_bias": 0.1,
      "risk_tolerance": 0.24,
      "temperature": 0.72,
      "generation": 14,
      "parent_ids": [
        "7d892c1e",
        "8ae8aa4a"
      ],
      "fitness_history": []
    }
  ],
  "all_results": [
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "5749f4a6",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.8638399999999999
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the banana/plantain is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.07840000000000003,
      "fitness": 0.04704000000000001
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.82384
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "9217d381",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.90954
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "t12",
      "predicted_confidence": 0.85,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4959,
      "fitness": 0.29753999999999997
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.69424
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain \u2013 roughly 86\u202fbillion neurons in a human versus around 530\u202fmillion in a typical dog",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "7 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.90464
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River (Chang Jiang) in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 0,
      "genome_id": "e0eaf7a5",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "89e79cef",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei observed Saturn\u2019s rings in 1607, though he did not understand what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain \u2013 roughly 80\u201190\u202fbillion neurons in humans versus a few hundred million (\u2248\u202f0.5\u202fbillion) in dogs",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "e79de51c",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3599999999999999,
      "fitness": 0.21599999999999991
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.3599999999999999,
      "fitness": 0.21599999999999991
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4,000,000 miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.4375,
      "fitness": 0.2625
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9856,
      "fitness": 0.9113600000000001
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 0,
      "genome_id": "438bedcd",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.90224
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The banana is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain \u2013 roughly 86\u202fbillion versus a few billion",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.8822400000000001
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "2334a6ba",
      "task_id": "r01",
      "predicted_confidence": 0.9,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.81146
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the banana fruit) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.24309999999999998,
      "fitness": 0.14586
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.8790399999999998
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.77146
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.85616
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.5511000000000001,
      "fitness": 0.33066000000000006
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7791,
      "fitness": 0.46746
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9790399999999999
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "e04",
      "predicted_confidence": 0.92,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 0.9879,
      "fitness": 0.91274
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7191,
      "fitness": 0.79146
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9361600000000001
    },
    {
      "generation": 0,
      "genome_id": "b6713f7e",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5\u00a0cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9984,
      "fitness": 0.89904
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.7975000000000001,
      "fitness": 0.8585
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones (adult human)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (in China)",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "42e98736",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the banana is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "t07",
      "predicted_confidence": 0.99,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm (approximately)",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China (approximately 6,300\u202fkm) is the longest river entirely within one country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 0,
      "genome_id": "e4d54607",
      "task_id": "r01",
      "predicted_confidence": 0.95,
      "predicted_answer": "5\u202fcents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones (in a typical adult human)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "678e1e67",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.88544
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "e05",
      "predicted_confidence": 0.75,
      "predicted_answer": "19,000 airports\u200b",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain, with about 86 billion neurons, has far more neurons than a dog's brain, which has roughly 300-500 million",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "e01",
      "predicted_confidence": 1.0,
      "predicted_answer": "About 206 bones in an adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.84824
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 1,
      "genome_id": "bc73e105",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9711,
      "fitness": 0.90266
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "9 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9646399999999999
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.8646399999999999
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9246399999999999
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.7064999999999999
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6975,
      "fitness": 0.4185
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones in the adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.836
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 1,
      "genome_id": "83e94041",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4.2",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River, flowing entirely within China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "r03",
      "predicted_confidence": 0.97,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19\u202f000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "t13",
      "predicted_confidence": 0.98,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "0026737e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 (approximately)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "8a7ada68",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "t10",
      "predicted_confidence": 0.85,
      "predicted_answer": "The Yangtze River in China is the longest river that remains entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "e01",
      "predicted_confidence": 1.0,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "49d49754",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9984,
      "fitness": 0.91904
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6863999999999999,
      "fitness": 0.41183999999999993
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "4\u202f000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "b27e703f",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Finland",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.91064
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "t10",
      "predicted_confidence": 0.85,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.90954
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 1,
      "genome_id": "82bc9aeb",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.97976
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "cf5b8cfc",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "26,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 1,
      "genome_id": "3dfae36a",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "e10",
      "predicted_confidence": 0.6,
      "predicted_answer": "3,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.64,
      "prediction_accuracy": 0.46710000000000007,
      "fitness": 0.28026
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "21",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "r03",
      "predicted_confidence": 0.95,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 2,
      "genome_id": "ff107785",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 2,
      "genome_id": "fc895144",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 2,
      "genome_id": "9326ca20",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.96,
      "fitness": 0.9159999999999999
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.894
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9327399999999999
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9879,
      "fitness": 0.8727399999999998
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, at about 6,300\u202fkm long, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.764
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.99,
      "fitness": 0.8739999999999999
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.504
    },
    {
      "generation": 2,
      "genome_id": "ce340567",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 2,
      "genome_id": "637fdb4b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "About 19\u202f000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "t15",
      "predicted_confidence": 0.98,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China, approximately 6,300\u202fkm long, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 2,
      "genome_id": "5b8a5add",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.9295399999999999
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands (about 17,508 islands)",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.69424
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, which is about 6,300\u202fkm long and lies entirely within Chinese territory",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 2,
      "genome_id": "55d7c9b4",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.4224,
      "fitness": 0.25344
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.8640000000000001
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.8640000000000001
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.8640000000000001
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 2,
      "genome_id": "a77d6ce8",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 2,
      "genome_id": "45f44b77",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "t07",
      "predicted_confidence": 0.85,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn's rings, although he did not understand their true nature",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.9397599999999999
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "7,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 2,
      "genome_id": "ad37289b",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.9295399999999999
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "9837c6af",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.97994
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 (approximately)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "741ddd56",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.9778399999999999
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones in an adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "ba442588",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "19",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9271,
      "fitness": 0.93626
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.97976
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "810,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "c990b0a0",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9964,
      "fitness": 0.95784
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4,000 miles (4 million miles)",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "dbc3ba35",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "6f4ddaab",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.95994
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9559,
      "fitness": 0.9535399999999999
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "23c51f34",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 3,
      "genome_id": "e7b34a25",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "e07",
      "predicted_confidence": 0.95,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8465
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "76929ba6",
      "task_id": "t15",
      "predicted_confidence": 0.99,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.9295399999999999
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country, at about 3,915\u202fkm long",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.90824
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones in the adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 3,
      "genome_id": "7ad3ee0e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands of any country",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "1939a23f",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8065
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "1,180,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "95e88539",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 40,000 airports",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "5301f9db",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "e648efa5",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "e01",
      "predicted_confidence": 0.98,
      "predicted_answer": "206 (approximately)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.84464
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "b87eac67",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.90954
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.6864000000000001,
      "fitness": 0.41184000000000004
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana fruit is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "a3da1ad6",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9265000000000001
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "e01",
      "predicted_confidence": 0.9,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "18,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, about 6,300\u202fkm long, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "250f18d1",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 10,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "t02",
      "predicted_confidence": 0.95,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "5d694295",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 (the typical number of bones in an adult human skeleton)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.85914
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (the banana fruit) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 4,
      "genome_id": "8cac8f7a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.89834
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "Approximately 5,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.71424
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana/plantain fruit) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 4,
      "genome_id": "fba84c3e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "663dcbda",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones (the typical count in an adult human body)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "e10",
      "predicted_confidence": 0.65,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.5774999999999999,
      "prediction_accuracy": 0.7399,
      "fitness": 0.44394
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.94544
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 5,
      "genome_id": "77f5d05e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 bones in the adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "adfe0d77",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 (adult human skeleton)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "f5bd77eb",
      "task_id": "t07",
      "predicted_confidence": 0.99,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.9359999999999999
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "3ece4c8e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not understand what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.8064,
      "fitness": 0.84384
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9424,
      "fitness": 0.92544
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 5,
      "genome_id": "60b37c14",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.69424
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 bones in the adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9991,
      "fitness": 0.95946
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "8efbbf97",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings with a telescope, though he did not understand what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "approximately 3,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5\u00a0cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "455e0783",
      "task_id": "t07",
      "predicted_confidence": 0.98,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.83834
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.5644,
      "fitness": 0.33864
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.73424
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.92824
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8082400000000001
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.8844000000000001,
      "fitness": 0.8906400000000001
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.88824
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.8682399999999999
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 0.9804,
      "fitness": 0.96824
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.52224
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.95834
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.8665
    },
    {
      "generation": 5,
      "genome_id": "b28161c0",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.9183399999999999
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 5,
      "genome_id": "f0324fba",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "e05",
      "predicted_confidence": 0.95,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (plantain) is the fruit most produced worldwide by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "665cdbaf",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000 satellites in orbit around Earth (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9785
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "b366d0f2",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first observed Saturn\u2019s rings in 1610 but did not recognize them as rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "87ad45ad",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.15359999999999996,
      "fitness": 0.09215999999999998
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8533600000000001
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "r05",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9936,
      "fitness": 0.97616
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "446465e4",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first person to observe Saturn's rings, though he did not know what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9936,
      "fitness": 0.9561600000000001
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "149dc8b0",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not understand their true nature",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports (approximately)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "90,000",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "64b3f3cc",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "c0358b66",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8665
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "15b2c44f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, which is about 6,300\u202fkm long",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.9405600000000001
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei, who first noted Saturn\u2019s \u201cears\u201d or \u201chandles\u201d in 1610, did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "15618e73",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 6,
      "genome_id": "0b55f84d",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana is the most produced fruit worldwide by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "e10",
      "predicted_confidence": 0.82,
      "predicted_answer": "3,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3276000000000001,
      "prediction_accuracy": 0.11640000000000006,
      "fitness": 0.06984000000000003
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "a79c0fb7",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The banana is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "11,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "t13",
      "predicted_confidence": 0.92,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9936,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "3cbd94fe",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana/plantain) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "e01",
      "predicted_confidence": 0.98,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons\u2014approximately 86\u202fbillion\u2014than a dog\u2019s brain, which contains only about 530\u202fmillion neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "a2dbc4a0",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "0ac0227d",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "85523e83",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (plantain) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "69c0c631",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The banana is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "e01",
      "predicted_confidence": 0.98,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3276000000000001,
      "fitness": 0.19656000000000007
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "954421c7",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "1cc634a8",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (bananas)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 bones in the adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "5,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "cbef6464",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "e01",
      "predicted_confidence": 0.9,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 7,
      "genome_id": "9606cfae",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "986fe0a5",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "06c2e7c7",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000 airports in the United States (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "e10",
      "predicted_confidence": 0.85,
      "predicted_answer": "7,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "t15",
      "predicted_confidence": 0.98,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "99c34a5b",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "4000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "62b62d55",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "20,000 airports",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4.2 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9785
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "t03",
      "predicted_confidence": 0.98,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "de0f6775",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "e05",
      "predicted_confidence": 0.93,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9951,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9784999999999999
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "b125fee6",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9743999999999999,
      "fitness": 0.9446399999999999
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.90224
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (the banana, including plantains, is the most produced fruit in the world by weight.)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "14baf1ae",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, at about 6,300\u202fkm, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9375,
      "fitness": 0.9425
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "f06cbe94",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "7ce40f01",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 8,
      "genome_id": "34dd2bc2",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "30,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (he first observed Saturn's rings in 1610, though he didn't recognize them as rings)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "r05",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "r07",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "t13",
      "predicted_confidence": 0.99,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "r15",
      "predicted_confidence": 0.9,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters (nearest 500)",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "t15",
      "predicted_confidence": 0.85,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "d076bea5",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "t10",
      "predicted_confidence": 0.99,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "9,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "8b9f8a44",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, measuring about 6,300\u202fkm, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "8,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8665
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "2aa35abd",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "18,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11\u202f000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "cba1a494",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9974999999999999,
      "fitness": 0.9584999999999999
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "3,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "t12",
      "predicted_confidence": 0.92,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.15359999999999996,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn's rings, though he did not understand what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.96,
      "fitness": 0.956
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "8b62743c",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not yet know what they were",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "t15",
      "predicted_confidence": 0.93,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.1350999999999999,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "93d19c06",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9590399999999999
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "db92bd86",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (plantain and dessert banana varieties) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8578600000000001
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10,900 m",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands, with over 17,000 islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "6e0eb750",
      "task_id": "r13",
      "predicted_confidence": 0.98,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, at about 6,300\u202fkm long, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "Approximately 10,900\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia has the largest number of islands in the world, with over 17,000 islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "a3189d5d",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.8640000000000001
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden has the most islands, with roughly 267,570 islands",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 9,
      "genome_id": "735932ee",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "e08",
      "predicted_confidence": 0.8,
      "predicted_answer": "960,000 golf balls\uff08nearest 10,000\uff09",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.09749999999999992,
      "fitness": 0.05849999999999995
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9465000000000001
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "t09",
      "predicted_confidence": 0.97,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.05910000000000004,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "t07",
      "predicted_confidence": 0.99,
      "predicted_answer": "Galileo\u202fGalilei (he first noticed the ring\u2011like structure of Saturn in 1610 but did not yet understand its true nature)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "2e7db13e",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9790399999999999
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China, at about 6,300\u202fkm, is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Aristarchus of Samos (c.\u202f310\u2013230\u202fBCE) was the first to describe Saturn\u2019s \u201chalo,\u201d noting the ring\u2011like appearance without understanding its nature",
      "ground_truth": "Galileo",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "96674f8f",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9639,
      "fitness": 0.93834
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "r04",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (he observed Saturn\u2019s rings in 1610, describing them as \u201chandles\u201d but did not understand their nature)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "Approximately 3,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.2604000000000001,
      "fitness": 0.15624000000000005
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "6f4e571e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not recognize them as rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,000 satellites (approximately)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "19f2b543",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "e05",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "r15",
      "predicted_confidence": 1.0,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "e92eb6eb",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.84874
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "Approximately 6,000 satellites are currently orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "af735b13",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "9",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "f2c326e2",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9585000000000001
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8665
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2775000000000001,
      "fitness": 0.16650000000000004
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "719117df",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "r15",
      "predicted_confidence": 0.99,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "ec6f22d0",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.97976
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 10,
      "genome_id": "664c5ada",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000 airports in the United States",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "t07",
      "predicted_confidence": 0.97,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he did not realize they were rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "There are about 206 bones in a typical adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 11,
      "genome_id": "93e59087",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 11,
      "genome_id": "e016dd6b",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3759000000000001,
      "fitness": 0.22554000000000007
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yangtze River (China)",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo\u202fGalilei, who first noted the \u201cmysterious double\u2011topped\u201d appearance of Saturn in 1610 (though he did not recognise them as rings)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 0.9999,
      "fitness": 0.93994
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit worldwide by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.01990000000000003,
      "fitness": 0.011940000000000017
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8319,
      "fitness": 0.87914
    },
    {
      "generation": 11,
      "genome_id": "97990ebc",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6519,
      "fitness": 0.39114
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "t07",
      "predicted_confidence": 0.9,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 11,
      "genome_id": "567b2042",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 11,
      "genome_id": "706e66e2",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "t09",
      "predicted_confidence": 0.98,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.03960000000000008,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "t07",
      "predicted_confidence": 0.97,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 11,
      "genome_id": "396111c6",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "e05",
      "predicted_confidence": 0.75,
      "predicted_answer": "19,000 airports (nearest 1,000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9879,
      "fitness": 0.9527399999999999
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia is the country with the most islands, boasting roughly 17,500 islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8704000000000001,
      "fitness": 0.8822400000000001
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 (the typical adult human skeleton has 206 bones)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "e08",
      "predicted_confidence": 0.7,
      "predicted_answer": "960,000 golf balls",
      "ground_truth": "500000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.2944000000000001,
      "fitness": 0.17664000000000005
    },
    {
      "generation": 11,
      "genome_id": "5ac144eb",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5904,
      "fitness": 0.35424
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "e05",
      "predicted_confidence": 0.95,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.8865000000000001
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei (the first to note Saturn's rings with a telescope, though he did not understand their nature)",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9975,
      "fitness": 0.9785
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones (the typical adult human skeleton)",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8775000000000001,
      "fitness": 0.9065
    },
    {
      "generation": 11,
      "genome_id": "37d832cc",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "e07",
      "predicted_confidence": 0.95,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 11,
      "genome_id": "2ab3fd8b",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.2775000000000001,
      "prediction_accuracy": 0.03960000000000008,
      "fitness": 0.02376000000000005
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first to observe Saturn\u2019s rings, though he mistook them for \u201chandles\u201d or \u201carms\u201d rather than a ringed planet",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 11,
      "genome_id": "6543faa9",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "r15",
      "predicted_confidence": 0.97,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas are the world's most produced fruit by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "6bbc1d03",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "r15",
      "predicted_confidence": 0.98,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "52082a46",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (the banana fruit) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.8687400000000001
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9919,
      "fitness": 0.9551400000000001
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9984,
      "fitness": 0.9790399999999999
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "t01",
      "predicted_confidence": 0.98,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "4b185f7d",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "t09",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "fca55a71",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "t09",
      "predicted_confidence": 0.99,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.01990000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "e05",
      "predicted_confidence": 0.75,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9856,
      "fitness": 0.95136
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "79d301a5",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "6,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River (in China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "44bbc448",
      "task_id": "e10",
      "predicted_confidence": 0.75,
      "predicted_answer": "3\u202f000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.4375,
      "prediction_accuracy": 0.22560000000000002,
      "fitness": 0.13536
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River, which is about 6,300 km long and lies entirely within China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.99,
      "fitness": 0.974
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "2a4fe94b",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites (nearest thousand)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "r15",
      "predicted_confidence": 0.95,
      "predicted_answer": "9 minutes",
      "ground_truth": "9",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "r01",
      "predicted_confidence": 0.99,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "t03",
      "predicted_confidence": 0.99,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "14,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "0704b0b8",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "5,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "669b0f05",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "r04",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "r15",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "9",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana fruit is the most produced fruit in the world by weight)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "r01",
      "predicted_confidence": 1.0,
      "predicted_answer": "5 cents",
      "ground_truth": "5",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.9
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6279,
      "fitness": 0.37674
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "t03",
      "predicted_confidence": 1.0,
      "predicted_answer": "Canberra",
      "ground_truth": "Canberra",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "13,000",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.17189999999999994,
      "fitness": 0.10313999999999997
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9919,
      "fitness": 0.97514
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 12,
      "genome_id": "a0a73c80",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "8,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "r09",
      "predicted_confidence": 0.95,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6,000 satellites orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "08893c42",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (plantain) is the fruit that is produced most in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "3,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "9e220ad6",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "The human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana (including plantain) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "6000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "8ae8aa4a",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia has the most islands of any country",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "t13",
      "predicted_confidence": 0.99,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "t12",
      "predicted_confidence": 0.93,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.1350999999999999,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "e07",
      "predicted_confidence": 0.85,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.97976
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "821d6d6c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Indonesia (about 17,500 islands)",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "t10",
      "predicted_confidence": 0.95,
      "predicted_answer": "The Yangtze River (China) is the longest river that lies entirely within a single country, at about 6,300\u202fkm (3,917\u202fmi)",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "e04",
      "predicted_confidence": 0.96,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9984,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "t13",
      "predicted_confidence": 0.9,
      "predicted_answer": "A human brain has far more neurons\u2014about 86\u202fbillion\u2014compared with a dog's brain, which contains roughly 500\u202fmillion neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000 satellites orbiting Earth",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "70c99196",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "e12",
      "predicted_confidence": 0.6,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.84,
      "prediction_accuracy": 0.9158999999999999,
      "fitness": 0.9295399999999999
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (the banana is the fruit with the highest total production by weight worldwide)",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "r03",
      "predicted_confidence": 0.98,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "5",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "7,000 satellites (nearest 1,000)",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3439000000000001,
      "fitness": 0.20634000000000005
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8479,
      "fitness": 0.88874
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "2014c0bf",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "t10",
      "predicted_confidence": 0.9,
      "predicted_answer": "The Yangtze River in China is the longest river that lies entirely within a single country",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "t13",
      "predicted_confidence": 0.97,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "The banana is the fruit most produced in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "e10",
      "predicted_confidence": 0.8,
      "predicted_answer": "9,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.1350999999999999,
      "fitness": 0.08105999999999992
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "r08",
      "predicted_confidence": 0.97,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9991,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "8adb2549",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has far more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Bananas\u202fare the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "e10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.8733600000000001
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8556,
      "fitness": 0.89336
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "r08",
      "predicted_confidence": 0.98,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "249fc1f2",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "t15",
      "predicted_confidence": 0.95,
      "predicted_answer": "Indonesia",
      "ground_truth": "Sweden",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.8640000000000001
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "11000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "The human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "r03",
      "predicted_confidence": 0.99,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "12,000",
      "ground_truth": "10000",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.3600000000000001,
      "fitness": 0.21600000000000005
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.84,
      "fitness": 0.884
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "r08",
      "predicted_confidence": 0.99,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "0f9cf3c5",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "t15",
      "predicted_confidence": 0.9,
      "predicted_answer": "Sweden",
      "ground_truth": "Sweden",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "t10",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Yangtze",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8778600000000001
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "t13",
      "predicted_confidence": 0.95,
      "predicted_answer": "A human brain has more neurons than a dog's brain",
      "ground_truth": "Human",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "r03",
      "predicted_confidence": 1.0,
      "predicted_answer": "47 days",
      "ground_truth": "47",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8799999999999999
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "e10",
      "predicted_confidence": 0.7,
      "predicted_answer": "10,000 satellites",
      "ground_truth": "10000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "e08",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "500000",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.89786
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "r08",
      "predicted_confidence": 1.0,
      "predicted_answer": "391",
      "ground_truth": "391",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 13,
      "genome_id": "7d892c1e",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "r04",
      "predicted_confidence": 0.9,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1978",
      "ground_truth": "1978",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "e05",
      "predicted_confidence": 0.7,
      "predicted_answer": "20,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.94266
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "e12",
      "predicted_confidence": 0.65,
      "predicted_answer": "18",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.8775000000000001,
      "prediction_accuracy": 0.9516,
      "fitness": 0.95096
    },
    {
      "generation": 14,
      "genome_id": "18184a8a",
      "task_id": "e07",
      "predicted_confidence": 0.9,
      "predicted_answer": "4 million miles",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11000",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "The banana (plantain) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 14,
      "genome_id": "38b07cae",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "t09",
      "predicted_confidence": 0.9,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Bananas (including plantains) are the most produced fruit worldwide by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.95706
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo\u202fGalilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 14,
      "genome_id": "1cc87d42",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.9951,
      "fitness": 0.9770599999999999
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "e04",
      "predicted_confidence": 0.9,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "14,000 airports (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 14,
      "genome_id": "c965500f",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "7",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "t02",
      "predicted_confidence": 0.98,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206 bones in the adult human body",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "r13",
      "predicted_confidence": 0.95,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana (including plantains) is the most produced fruit in the world by weight",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "r05",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "e05",
      "predicted_confidence": 0.75,
      "predicted_answer": "19,000 airports in the United States (nearest 1000)",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.5774999999999999,
      "fitness": 0.3464999999999999
    },
    {
      "generation": 14,
      "genome_id": "7b65c1b7",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9775,
      "fitness": 0.9665
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "e04",
      "predicted_confidence": 0.98,
      "predicted_answer": "10,900 meters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "r04",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei was the first person to observe Saturn's rings, though he did not recognize them as rings",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 14,
      "genome_id": "2520d9b3",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "e01",
      "predicted_confidence": 0.99,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "t09",
      "predicted_confidence": 1.0,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.0,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "r05",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "e05",
      "predicted_confidence": 0.85,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9775,
      "prediction_accuracy": 0.9996,
      "fitness": 0.95976
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "e12",
      "predicted_confidence": 0.7,
      "predicted_answer": "19",
      "ground_truth": "18",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9711,
      "fitness": 0.96266
    },
    {
      "generation": 14,
      "genome_id": "8fd6deaf",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "5 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.51,
      "prediction_accuracy": 0.31110000000000004,
      "fitness": 0.18666000000000002
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "e04",
      "predicted_confidence": 0.99,
      "predicted_answer": "Approximately 11,000\u202fmeters",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "t01",
      "predicted_confidence": 0.99,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "r07",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "05:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "e05",
      "predicted_confidence": 0.9,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.99,
      "prediction_accuracy": 1.0,
      "fitness": 0.96
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "t07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.8631,
      "fitness": 0.8578600000000001
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 14,
      "genome_id": "cee48807",
      "task_id": "e07",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6031,
      "fitness": 0.36185999999999996
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "10,900\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "t02",
      "predicted_confidence": 0.99,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "r13",
      "predicted_confidence": 1.0,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "t12",
      "predicted_confidence": 0.9,
      "predicted_answer": "banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.18999999999999995,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "r05",
      "predicted_confidence": 0.98,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9996,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "r07",
      "predicted_confidence": 1.0,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "r09",
      "predicted_confidence": 1.0,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "e05",
      "predicted_confidence": 0.75,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.9375,
      "prediction_accuracy": 0.9831,
      "fitness": 0.9498599999999999
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.6155999999999999,
      "fitness": 0.36935999999999997
    },
    {
      "generation": 14,
      "genome_id": "b183109a",
      "task_id": "e07",
      "predicted_confidence": 0.7,
      "predicted_answer": "4",
      "ground_truth": "4",
      "is_correct": true,
      "raw_calibration": 0.9099999999999999,
      "prediction_accuracy": 0.9676,
      "fitness": 0.96056
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "e04",
      "predicted_confidence": 0.95,
      "predicted_answer": "11,000\u202fm",
      "ground_truth": "11000",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.92
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "r04",
      "predicted_confidence": 0.95,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "t02",
      "predicted_confidence": 1.0,
      "predicted_answer": "Mercury",
      "ground_truth": "Mercury",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "e01",
      "predicted_confidence": 0.95,
      "predicted_answer": "206",
      "ground_truth": "206",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.86
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "r13",
      "predicted_confidence": 0.99,
      "predicted_answer": "Yes",
      "ground_truth": "Yes",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.98
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "t09",
      "predicted_confidence": 0.95,
      "predicted_answer": "1977",
      "ground_truth": "1978",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "t12",
      "predicted_confidence": 0.95,
      "predicted_answer": "Banana",
      "ground_truth": "Tomato",
      "is_correct": false,
      "raw_calibration": 0.09750000000000003,
      "prediction_accuracy": 0.0,
      "fitness": 0.0
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "t01",
      "predicted_confidence": 1.0,
      "predicted_answer": "Au",
      "ground_truth": "Au",
      "is_correct": true,
      "raw_calibration": 1.0,
      "prediction_accuracy": 1.0,
      "fitness": 0.8200000000000001
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "r05",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "r07",
      "predicted_confidence": 0.99,
      "predicted_answer": "No",
      "ground_truth": "No",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "r09",
      "predicted_confidence": 0.99,
      "predicted_answer": "5:00 PM",
      "ground_truth": "5:00 PM",
      "is_correct": true,
      "raw_calibration": 0.9999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "e05",
      "predicted_confidence": 0.8,
      "predicted_answer": "19,000",
      "ground_truth": "19000",
      "is_correct": true,
      "raw_calibration": 0.96,
      "prediction_accuracy": 0.99,
      "fitness": 0.954
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "t07",
      "predicted_confidence": 0.95,
      "predicted_answer": "Galileo Galilei",
      "ground_truth": "Galileo",
      "is_correct": true,
      "raw_calibration": 0.9974999999999999,
      "prediction_accuracy": 1.0,
      "fitness": 0.94
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "e12",
      "predicted_confidence": 0.5,
      "predicted_answer": "",
      "ground_truth": "18",
      "is_correct": false,
      "raw_calibration": 0.75,
      "prediction_accuracy": 0.64,
      "fitness": 0.384
    },
    {
      "generation": 14,
      "genome_id": "a07a476c",
      "task_id": "e07",
      "predicted_confidence": 0.8,
      "predicted_answer": "6 million miles",
      "ground_truth": "4",
      "is_correct": false,
      "raw_calibration": 0.3599999999999999,
      "prediction_accuracy": 0.18999999999999995,
      "fitness": 0.11399999999999996
    }
  ],
  "test_results": {
    "n_tasks": 9,
    "n_agents": 10,
    "avg_raw_calibration": 0.8186088888888889,
    "avg_prediction_accuracy": 0.7865077777777778,
    "avg_task_accuracy": 0.6555555555555556,
    "best_fitness": 0.6942222222222223,
    "avg_fitness": 0.6767935555555555
  }
}